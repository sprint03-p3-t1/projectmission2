{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdd892f7-ac2f-45f6-a074-2434702b16db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¶œë ¥ ë””ë ‰í„°ë¦¬ê°€ ì—†ì–´ ìƒì„±í–ˆìŠµë‹ˆë‹¤: ../data/processed/datapreprocessingbjs(pdfplumber)/text_single_file\n",
      "--- ë³€í™˜ ì¤‘: ëŒ€ì „ëŒ€í•™êµ_ëŒ€ì „ëŒ€í•™êµ 2024í•™ë…„ë„ ë‹¤ì¸µì  ìœµí•© í•™ìŠµê²½í—˜ í”Œë«í¼(MILE) ì „.pdf ([<Page:1>, <Page:2>, <Page:3>, <Page:4>, <Page:5>, <Page:6>, <Page:7>, <Page:8>, <Page:9>, <Page:10>, <Page:11>, <Page:12>, <Page:13>, <Page:14>, <Page:15>, <Page:16>, <Page:17>, <Page:18>, <Page:19>, <Page:20>, <Page:21>, <Page:22>, <Page:23>, <Page:24>, <Page:25>, <Page:26>, <Page:27>, <Page:28>, <Page:29>, <Page:30>, <Page:31>, <Page:32>, <Page:33>, <Page:34>, <Page:35>, <Page:36>, <Page:37>, <Page:38>, <Page:39>, <Page:40>, <Page:41>, <Page:42>, <Page:43>, <Page:44>, <Page:45>, <Page:46>, <Page:47>, <Page:48>, <Page:49>, <Page:50>, <Page:51>, <Page:52>, <Page:53>, <Page:54>, <Page:55>, <Page:56>, <Page:57>, <Page:58>, <Page:59>, <Page:60>, <Page:61>, <Page:62>, <Page:63>, <Page:64>, <Page:65>, <Page:66>, <Page:67>, <Page:68>, <Page:69>, <Page:70>, <Page:71>, <Page:72>, <Page:73>, <Page:74>, <Page:75>] í˜ì´ì§€) ---\n",
      "âœ… ì„±ê³µ: ëŒ€ì „ëŒ€í•™êµ_ëŒ€ì „ëŒ€í•™êµ 2024í•™ë…„ë„ ë‹¤ì¸µì  ìœµí•© í•™ìŠµê²½í—˜ í”Œë«í¼(MILE) ì „.pdf -> ../data/processed/datapreprocessingbjs(pdfplumber)/text_single_file/ëŒ€ì „ëŒ€í•™êµ_ëŒ€ì „ëŒ€í•™êµ 2024í•™ë…„ë„ ë‹¤ì¸µì  ìœµí•© í•™ìŠµê²½í—˜ í”Œë«í¼(MILE) ì „.txt\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "def convert_pdf_single_file(input_path, output_directory):\n",
    "    \"\"\"\n",
    "    pdfplumberë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì¼ PDF íŒŒì¼ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): ë³€í™˜í•  PDF íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ.\n",
    "        output_directory (str): ë³€í™˜ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í„°ë¦¬.\n",
    "    \"\"\"\n",
    "    # ì¶œë ¥ ë””ë ‰í„°ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "        print(f\"ì¶œë ¥ ë””ë ‰í„°ë¦¬ê°€ ì—†ì–´ ìƒì„±í–ˆìŠµë‹ˆë‹¤: {output_directory}\")\n",
    "\n",
    "    # íŒŒì¼ëª… ì„¤ì • (í™•ì¥ìë¥¼ .txtë¡œ ë³€ê²½)\n",
    "    file_name = os.path.basename(input_path)\n",
    "    base_name, _ = os.path.splitext(file_name)\n",
    "    output_path = os.path.join(output_directory, f'{base_name}.txt')\n",
    "\n",
    "    full_text = \"\"\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(input_path) as pdf:\n",
    "            print(f\"--- ë³€í™˜ ì¤‘: {file_name} ({pdf.pages} í˜ì´ì§€) ---\")\n",
    "            for page in pdf.pages:\n",
    "                # í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ. í˜ì´ì§€ ë²ˆí˜¸ë¥¼ í¬í•¨í•˜ì—¬ êµ¬ì¡°í™” ê°€ëŠ¥\n",
    "                extracted_text = page.extract_text()\n",
    "                if extracted_text:\n",
    "                    full_text += extracted_text + \"\\n\"  # í˜ì´ì§€ ê°„ ì¤„ ë°”ê¿ˆ ì¶”ê°€\n",
    "\n",
    "        # ì¶”ì¶œëœ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(full_text)\n",
    "\n",
    "        print(f\"âœ… ì„±ê³µ: {file_name} -> {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ ({file_name}): {e}\")\n",
    "\n",
    "# --- ì‹¤í–‰ ì˜ˆì‹œ ---\n",
    "# ë³€í™˜í•  PDF íŒŒì¼ ê²½ë¡œ\n",
    "input_file = '../data/raw/files/ëŒ€ì „ëŒ€í•™êµ_ëŒ€ì „ëŒ€í•™êµ 2024í•™ë…„ë„ ë‹¤ì¸µì  ìœµí•© í•™ìŠµê²½í—˜ í”Œë«í¼(MILE) ì „.pdf'\n",
    "# í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í„°ë¦¬\n",
    "output_dir = '../data/processed/datapreprocessingbjs(pdfplumber)/text_single_file'\n",
    "\n",
    "# í•¨ìˆ˜ í˜¸ì¶œ\n",
    "convert_pdf_single_file(input_file, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32edbeda-d350-4d66-bffe-e9822a7006c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¶œë ¥ ë””ë ‰í„°ë¦¬ê°€ ì—†ì–´ ìƒì„±í–ˆìŠµë‹ˆë‹¤: ../data/processed/datapreprocessingbjs(pdfplumber)/text_all_files\n",
      "\n",
      "--- PDF íŒŒì¼ ì¼ê´„ ë³€í™˜ ì‹œì‘ (ì´ 5ê°œ) ---\n",
      "âœ… ì„±ê³µ: ê¸°ì´ˆê³¼í•™ì—°êµ¬ì›_2025ë…„ë„ ì¤‘ì´ì˜¨ê°€ì†ê¸°ìš© ê·¹ì €ì˜¨ì‹œìŠ¤í…œ ìš´ì „ ìš©ì—­.pdf -> ../data/processed/datapreprocessingbjs(pdfplumber)/text_all_files/ê¸°ì´ˆê³¼í•™ì—°êµ¬ì›_2025ë…„ë„ ì¤‘ì´ì˜¨ê°€ì†ê¸°ìš© ê·¹ì €ì˜¨ì‹œìŠ¤í…œ ìš´ì „ ìš©ì—­.txt\n",
      "âœ… ì„±ê³µ: ê³ ë ¤ëŒ€í•™êµ_ì°¨ì„¸ëŒ€ í¬í„¸Â·í•™ì‚¬ ì •ë³´ì‹œìŠ¤í…œ êµ¬ì¶•ì‚¬ì—….pdf -> ../data/processed/datapreprocessingbjs(pdfplumber)/text_all_files/ê³ ë ¤ëŒ€í•™êµ_ì°¨ì„¸ëŒ€ í¬í„¸Â·í•™ì‚¬ ì •ë³´ì‹œìŠ¤í…œ êµ¬ì¶•ì‚¬ì—….txt\n",
      "âœ… ì„±ê³µ: ì„œìš¸íŠ¹ë³„ì‹œ_2024ë…„ ì§€ë„ì •ë³´ í”Œë«í¼ ë° ì „ë¬¸í™œìš© ì—°ê³„ ì‹œìŠ¤í…œ ê³ ë„í™” ìš©.pdf -> ../data/processed/datapreprocessingbjs(pdfplumber)/text_all_files/ì„œìš¸íŠ¹ë³„ì‹œ_2024ë…„ ì§€ë„ì •ë³´ í”Œë«í¼ ë° ì „ë¬¸í™œìš© ì—°ê³„ ì‹œìŠ¤í…œ ê³ ë„í™” ìš©.txt\n",
      "âœ… ì„±ê³µ: ì„œìš¸ì‹œë¦½ëŒ€í•™êµ_[ì‚¬ì „ê³µê°œ] í•™ì—…ì„±ì·¨ë„ ë‹¤ì°¨ì› ì¢…ë‹¨ë¶„ì„ í†µí•©ì‹œìŠ¤í…œ 1ì°¨.pdf -> ../data/processed/datapreprocessingbjs(pdfplumber)/text_all_files/ì„œìš¸ì‹œë¦½ëŒ€í•™êµ_[ì‚¬ì „ê³µê°œ] í•™ì—…ì„±ì·¨ë„ ë‹¤ì°¨ì› ì¢…ë‹¨ë¶„ì„ í†µí•©ì‹œìŠ¤í…œ 1ì°¨.txt\n",
      "âœ… ì„±ê³µ: ëŒ€ì „ëŒ€í•™êµ_ëŒ€ì „ëŒ€í•™êµ 2024í•™ë…„ë„ ë‹¤ì¸µì  ìœµí•© í•™ìŠµê²½í—˜ í”Œë«í¼(MILE) ì „.pdf -> ../data/processed/datapreprocessingbjs(pdfplumber)/text_all_files/ëŒ€ì „ëŒ€í•™êµ_ëŒ€ì „ëŒ€í•™êµ 2024í•™ë…„ë„ ë‹¤ì¸µì  ìœµí•© í•™ìŠµê²½í—˜ í”Œë«í¼(MILE) ì „.txt\n",
      "\n",
      "--- ë³€í™˜ ì‘ì—… ì™„ë£Œ ---\n",
      "ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ëœ íŒŒì¼ ìˆ˜: 5\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "def convert_all_pdfs_in_folder(input_directory, output_directory):\n",
    "    \"\"\"\n",
    "    ì§€ì •ëœ ë””ë ‰í„°ë¦¬ ë‚´ì˜ ëª¨ë“  PDF íŒŒì¼ì„ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "        input_directory (str): PDF íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í„°ë¦¬ ê²½ë¡œ.\n",
    "        output_directory (str): ë³€í™˜ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í„°ë¦¬.\n",
    "    \"\"\"\n",
    "    # ì¶œë ¥ ë””ë ‰í„°ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "        print(f\"ì¶œë ¥ ë””ë ‰í„°ë¦¬ê°€ ì—†ì–´ ìƒì„±í–ˆìŠµë‹ˆë‹¤: {output_directory}\")\n",
    "\n",
    "    # ì…ë ¥ ë””ë ‰í„°ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼ì„ ìˆœíšŒí•˜ë©° PDF íŒŒì¼ë§Œ ì°¾ê¸°\n",
    "    pdf_files = [f for f in os.listdir(input_directory) if f.lower().endswith(\".pdf\")]\n",
    "    \n",
    "    total_files = len(pdf_files)\n",
    "    success_count = 0\n",
    "    failed_files = []\n",
    "\n",
    "    print(f\"\\n--- PDF íŒŒì¼ ì¼ê´„ ë³€í™˜ ì‹œì‘ (ì´ {total_files}ê°œ) ---\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        input_path = os.path.join(input_directory, pdf_file)\n",
    "        base_name, _ = os.path.splitext(pdf_file)\n",
    "        output_path = os.path.join(output_directory, f'{base_name}.txt')\n",
    "        \n",
    "        full_text = \"\"\n",
    "\n",
    "        try:\n",
    "            with pdfplumber.open(input_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    extracted_text = page.extract_text()\n",
    "                    if extracted_text:\n",
    "                        full_text += extracted_text + \"\\n\"\n",
    "\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(full_text)\n",
    "\n",
    "            print(f\"âœ… ì„±ê³µ: {pdf_file} -> {os.path.relpath(output_path, start='.')}\")\n",
    "            success_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ ({pdf_file}): {e}\")\n",
    "            failed_files.append(pdf_file)\n",
    "\n",
    "    print(\"\\n--- ë³€í™˜ ì‘ì—… ì™„ë£Œ ---\")\n",
    "    print(f\"ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ëœ íŒŒì¼ ìˆ˜: {success_count}\")\n",
    "    if failed_files:\n",
    "        print(f\"ì˜¤ë¥˜ê°€ ë°œìƒí•œ íŒŒì¼ ëª©ë¡: {len(failed_files)}ê°œ\")\n",
    "        for f in failed_files:\n",
    "            print(f\"- {f}\")\n",
    "\n",
    "# --- ì‹¤í–‰ ì˜ˆì‹œ ---\n",
    "# PDF íŒŒì¼ë“¤ì´ ìˆëŠ” ì…ë ¥ í´ë” ê²½ë¡œ\n",
    "input_folder = '../data/raw/files'\n",
    "# ë³€í™˜ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì €ì¥í•  ì¶œë ¥ ë””ë ‰í„°ë¦¬\n",
    "output_folder = '../data/processed/datapreprocessingbjs(pdfplumber)/text_all_files'\n",
    "\n",
    "# í•¨ìˆ˜ í˜¸ì¶œ\n",
    "convert_all_pdfs_in_folder(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "207af9db-6d94-4184-b991-136e9028a807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF íŒŒì¼ íŒŒì‹± ì‹œì‘: ../data/raw/files/ëŒ€ì „ëŒ€í•™êµ_ëŒ€ì „ëŒ€í•™êµ 2024í•™ë…„ë„ ë‹¤ì¸µì  ìœµí•© í•™ìŠµê²½í—˜ í”Œë«í¼(MILE) ì „.pdf\n",
      "ì´ 75 í˜ì´ì§€ ë°œê²¬\n",
      "íŒŒì‹± ì™„ë£Œ - ë¬¸ë‹¨: 75, í‘œ: 51\n",
      "ì²­í‚¹ ì‘ì—… ì‹œì‘...\n",
      "ì²­í‚¹ ì™„ë£Œ - ì´ 254ê°œ ì²­í¬ ìƒì„±\n",
      "\n",
      "=== ì²­í¬ ë¶„ì„ ê²°ê³¼ ===\n",
      "íƒ€ì…ë³„ ë¶„í¬:\n",
      "  table: 51\n",
      "  paragraph: 203\n",
      "\n",
      "ì„¸ë¶€ íƒ€ì…ë³„ ë¶„í¬:\n",
      "  organization: 1\n",
      "  general: 30\n",
      "  budget: 4\n",
      "  schedule: 16\n",
      "  split_general: 202\n",
      "  header: 1\n",
      "\n",
      "ìš°ì„ ìˆœìœ„ë³„ ë¶„í¬:\n",
      "  ìš°ì„ ìˆœìœ„ 10: 16ê°œ\n",
      "  ìš°ì„ ìˆœìœ„ 8: 4ê°œ\n",
      "  ìš°ì„ ìˆœìœ„ 6: 2ê°œ\n",
      "  ìš°ì„ ìˆœìœ„ 5: 202ê°œ\n",
      "  ìš°ì„ ìˆœìœ„ 4: 30ê°œ\n",
      "\n",
      "=== ìƒ˜í”Œ ì²­í¬ ===\n",
      "ì²« ë²ˆì§¸ í‘œ ì²­í¬:\n",
      "  íƒ€ì…: table-organization\n",
      "  ë‚´ìš©: ë¬¸ì˜: ì…ì°°, ì†Œì† / ë‹´ë‹¹ì: ì´ë¬´íŒ€ ì´ë³‘ì„, ì „í™”: 042-280-2163. ë¬¸ì˜: ì‚¬ì—…, ì†Œì† / ë‹´ë‹¹ì: ëŒ€í•™êµìœ¡í˜ì‹ ì› êµìˆ˜í•™ìŠµê°œë°œì„¼í„°\n",
      "í™ë‚˜ë˜, ì „í™”: 042-280-4036...\n",
      "  ìš°ì„ ìˆœìœ„: 6\n",
      "\n",
      "ì²« ë²ˆì§¸ ë¬¸ë‹¨ ì²­í¬:\n",
      "  íƒ€ì…: paragraph-split_general\n",
      "  ë‚´ìš©: (ì¬ê³µê³ )ëŒ€ì „ëŒ€í•™êµ                    ë‹¤ì¸µì          ìœµí•©      í•™ìŠµê²½í—˜                  \n",
      "                                                                                  \n",
      "                                            ...\n",
      "  ë¬¸ë‹¨ ìˆ˜: 1\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def convert_table_to_natural_language(table_rows: List[List[str]]) -> str:\n",
    "    \"\"\"\n",
    "    pdfplumberì—ì„œ ì¶”ì¶œí•œ í‘œ ë°ì´í„°ë¥¼ ìì—°ì–´ í˜•íƒœë¡œ ë³€í™˜.\n",
    "    NoneType ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ëŠ” ë¡œì§ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if not table_rows or len(table_rows) < 2:\n",
    "        return \"\"\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ ê°€ì • (NoneType ë°©ì§€)\n",
    "    headers = [h if h is not None else \"\" for h in table_rows[0]]\n",
    "    content_rows = table_rows[1:]\n",
    "    \n",
    "    natural_text = []\n",
    "    \n",
    "    # 'ì›”' ë˜ëŠ” 'ì¼ì •'ì´ í¬í•¨ëœ í—¤ë”ê°€ ìˆì„ ê²½ìš° ì¼ì •í‘œë¡œ ê°„ì£¼\n",
    "    if any('ì›”' in h for h in headers) or any('ì¼ì •' in h for h in headers):\n",
    "        natural_text.append(\"ë‹¤ìŒì€ ì¶”ì§„ì¼ì •í‘œì…ë‹ˆë‹¤:\")\n",
    "        for row in content_rows:\n",
    "            row = [cell if cell is not None else \"\" for cell in row]\n",
    "            if len(row) > 1 and row[0].strip():\n",
    "                task = row[0].strip()\n",
    "                schedule_info = []\n",
    "                for i, cell in enumerate(row[1:], 1):\n",
    "                    if cell.strip() and i < len(headers):\n",
    "                        header = headers[i]\n",
    "                        schedule_info.append(f\"{header}ì— {task}\")\n",
    "                if schedule_info:\n",
    "                    natural_text.append(\". \".join(schedule_info))\n",
    "    else:\n",
    "        # ì¼ë°˜ í‘œ ì²˜ë¦¬\n",
    "        for row in content_rows:\n",
    "            row = [cell if cell is not None else \"\" for cell in row]\n",
    "            if len(row) >= len(headers):\n",
    "                row_text = []\n",
    "                for i, (header, cell) in enumerate(zip(headers, row)):\n",
    "                    if cell and cell.strip():\n",
    "                        row_text.append(f\"{header}: {cell.strip()}\")\n",
    "                if row_text:\n",
    "                    natural_text.append(\", \".join(row_text))\n",
    "    \n",
    "    return \". \".join(natural_text)\n",
    "\n",
    "def extract_text_and_tables(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    pdfplumberë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ì™€ í‘œë¥¼ ì¶”ì¶œí•˜ê³  êµ¬ì¡°í™”í•©ë‹ˆë‹¤.\n",
    "    NoneType ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ëŠ” ë¡œì§ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(f\"PDF íŒŒì¼ íŒŒì‹± ì‹œì‘: {file_path}\")\n",
    "    \n",
    "    parsed_data = {\n",
    "        'sections': [],\n",
    "        'tables': [],\n",
    "        'metadata': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            parsed_data['metadata']['total_pages'] = len(pdf.pages)\n",
    "            print(f\"ì´ {len(pdf.pages)} í˜ì´ì§€ ë°œê²¬\")\n",
    "\n",
    "            for page_idx, page in enumerate(pdf.pages):\n",
    "                # í˜ì´ì§€ ë‚´ ëª¨ë“  í‘œ ì¶”ì¶œ\n",
    "                page_tables = page.extract_tables()\n",
    "                \n",
    "                # í‘œ ë°ì´í„°ëŠ” ë³„ë„ë¡œ ì €ì¥\n",
    "                for i, table in enumerate(page_tables):\n",
    "                    table_data = {\n",
    "                        'id': f\"table_{page_idx}_{i}\",\n",
    "                        'type': 'table',\n",
    "                        'rows': table,\n",
    "                        'raw_content': convert_table_to_natural_language(table),\n",
    "                        'metadata': { 'page': page_idx + 1 }\n",
    "                    }\n",
    "                    if table_data['raw_content']:\n",
    "                        parsed_data['tables'].append(table_data)\n",
    "\n",
    "                # í˜ì´ì§€ ë‚´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (NoneType ë°©ì§€)\n",
    "                page_text_with_layout = page.extract_text(x_tolerance=2, y_tolerance=2, layout=True)\n",
    "                if not page_text_with_layout:\n",
    "                    continue\n",
    "\n",
    "                # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
    "                paragraphs = [para for para in page_text_with_layout.split('\\n\\n') if para.strip()]\n",
    "                for i, para in enumerate(paragraphs):\n",
    "                    parsed_data['sections'].append({\n",
    "                        'id': f\"para_{page_idx}_{i}\",\n",
    "                        'type': 'paragraph',\n",
    "                        'content': para.strip(),\n",
    "                        'metadata': { 'page': page_idx + 1 }\n",
    "                    })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"PDF íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    \n",
    "    parsed_data['metadata']['total_paragraphs'] = len(parsed_data['sections'])\n",
    "    parsed_data['metadata']['total_tables'] = len(parsed_data['tables'])\n",
    "    print(f\"íŒŒì‹± ì™„ë£Œ - ë¬¸ë‹¨: {len(parsed_data['sections'])}, í‘œ: {len(parsed_data['tables'])}\")\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "### **ì²­í¬ ìƒì„± ë° ë¶„ì„ í•¨ìˆ˜**\n",
    "\n",
    "def create_structured_chunks(parsed_data: Dict[str, Any], max_chunk_size: int = 1000) -> List[Dict[str, Any]]:\n",
    "    chunks = []\n",
    "    \n",
    "    print(\"ì²­í‚¹ ì‘ì—… ì‹œì‘...\")\n",
    "    \n",
    "    for table in parsed_data['tables']:\n",
    "        table_chunk = create_table_chunk(table)\n",
    "        if table_chunk:\n",
    "            chunks.append(table_chunk)\n",
    "    \n",
    "    paragraph_chunks = create_paragraph_chunks(parsed_data['sections'], max_chunk_size)\n",
    "    chunks.extend(paragraph_chunks)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk['chunk_id'] = f\"chunk_{i:04d}\"\n",
    "    \n",
    "    print(f\"ì²­í‚¹ ì™„ë£Œ - ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±\")\n",
    "    return chunks\n",
    "\n",
    "def create_table_chunk(table_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not table_data.get('raw_content') or not table_data['raw_content'].strip():\n",
    "        return None\n",
    "    \n",
    "    table_type = identify_table_type(table_data)\n",
    "    \n",
    "    chunk = {\n",
    "        'type': 'table',\n",
    "        'subtype': table_type,\n",
    "        'content': table_data['raw_content'],\n",
    "        'original_rows': len(table_data.get('rows', [])),\n",
    "        'metadata': {\n",
    "            'table_id': table_data.get('id'),\n",
    "            'source': 'pdf_table_extraction',\n",
    "            'priority': get_table_priority(table_type)\n",
    "        }\n",
    "    }\n",
    "    return chunk\n",
    "\n",
    "def identify_table_type(table_data: Dict[str, Any]) -> str:\n",
    "    content = table_data.get('raw_content', '').lower()\n",
    "    rows = table_data.get('rows', [])\n",
    "    \n",
    "    if rows and rows[0]:\n",
    "        header = ' '.join(cell for cell in rows[0] if cell).lower()\n",
    "        if any(keyword in header for keyword in ['ì›”', 'ì¼ì •', 'ì¶”ì§„']):\n",
    "            return 'schedule'\n",
    "        if any(keyword in header for keyword in ['ì˜ˆì‚°', 'ë¹„ìš©', 'ê¸ˆì•¡', 'ì›']):\n",
    "            return 'budget'\n",
    "        if any(keyword in header for keyword in ['ë‹´ë‹¹', 'ì—­í• ', 'ì¡°ì§']):\n",
    "            return 'organization'\n",
    "    \n",
    "    if any(keyword in content for keyword in ['ì¼ì •', 'ì›”', 'ì¶”ì§„']):\n",
    "        return 'schedule'\n",
    "    elif any(keyword in content for keyword in ['ì˜ˆì‚°', 'ë¹„ìš©', 'ì²œì›']):\n",
    "        return 'budget'\n",
    "    else:\n",
    "        return 'general'\n",
    "\n",
    "def get_table_priority(table_type: str) -> int:\n",
    "    priority_map = {\n",
    "        'schedule': 10,\n",
    "        'budget': 8,\n",
    "        'organization': 6,\n",
    "        'general': 4\n",
    "    }\n",
    "    return priority_map.get(table_type, 4)\n",
    "\n",
    "def create_paragraph_chunks(sections: List[Dict[str, Any]], max_chunk_size: int) -> List[Dict[str, Any]]:\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for section in sections:\n",
    "        content = section.get('content', '')\n",
    "        if not content.strip():\n",
    "            continue\n",
    "        \n",
    "        estimated_tokens = len(content) * 0.7\n",
    "        \n",
    "        if current_size + estimated_tokens <= max_chunk_size:\n",
    "            current_chunk.append(content)\n",
    "            current_size += estimated_tokens\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(create_paragraph_chunk(current_chunk))\n",
    "            \n",
    "            if estimated_tokens > max_chunk_size:\n",
    "                split_chunks = split_large_paragraph(content, max_chunk_size)\n",
    "                chunks.extend(split_chunks)\n",
    "                current_chunk = []\n",
    "                current_size = 0\n",
    "            else:\n",
    "                current_chunk = [content]\n",
    "                current_size = estimated_tokens\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(create_paragraph_chunk(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def create_paragraph_chunk(content_list: List[str]) -> Dict[str, Any]:\n",
    "    combined_content = ' '.join(content_list)\n",
    "    paragraph_type = identify_paragraph_type(combined_content)\n",
    "    \n",
    "    chunk = {\n",
    "        'type': 'paragraph',\n",
    "        'subtype': paragraph_type,\n",
    "        'content': combined_content,\n",
    "        'paragraph_count': len(content_list),\n",
    "        'metadata': {\n",
    "            'source': 'pdf_paragraph_extraction',\n",
    "            'priority': get_paragraph_priority(paragraph_type)\n",
    "        }\n",
    "    }\n",
    "    return chunk\n",
    "\n",
    "def identify_paragraph_type(content: str) -> str:\n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    if any(keyword in content_lower for keyword in ['ì œ', 'ì¥', 'ì ˆ', 'í•­']):\n",
    "        return 'header'\n",
    "    if any(keyword in content_lower for keyword in ['ëª©ì ', 'ê°œìš”', 'ë°°ê²½']):\n",
    "        return 'overview'\n",
    "    if any(keyword in content_lower for keyword in ['ê²°ë¡ ', 'ìš”ì•½', 'ì¢…í•©']):\n",
    "        return 'conclusion'\n",
    "    if any(keyword in content_lower for keyword in ['ë°©ë²•', 'ì ˆì°¨', 'ê³¼ì •']):\n",
    "        return 'methodology'\n",
    "    \n",
    "    return 'general'\n",
    "\n",
    "def get_paragraph_priority(paragraph_type: str) -> int:\n",
    "    priority_map = {\n",
    "        'overview': 9,\n",
    "        'conclusion': 8,\n",
    "        'methodology': 7,\n",
    "        'header': 6,\n",
    "        'general': 5\n",
    "    }\n",
    "    return priority_map.get(paragraph_type, 5)\n",
    "\n",
    "def split_large_paragraph(content: str, max_chunk_size: int) -> List[Dict[str, Any]]:\n",
    "    chunks = []\n",
    "    sentences = re.split(r'[.!?]\\s+', content)\n",
    "    \n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        estimated_tokens = len(sentence) * 0.7\n",
    "        \n",
    "        if current_size + estimated_tokens <= max_chunk_size:\n",
    "            current_chunk.append(sentence)\n",
    "            current_size += estimated_tokens\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunk_content = '. '.join(current_chunk) + '.'\n",
    "                chunks.append({\n",
    "                    'type': 'paragraph',\n",
    "                    'subtype': 'split_general',\n",
    "                    'content': chunk_content,\n",
    "                    'paragraph_count': 1,\n",
    "                    'metadata': {\n",
    "                        'source': 'paragraph_split',\n",
    "                        'priority': 5\n",
    "                    }\n",
    "                })\n",
    "            current_chunk = [sentence]\n",
    "            current_size = estimated_tokens\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunk_content = '. '.join(current_chunk) + '.'\n",
    "        chunks.append({\n",
    "            'type': 'paragraph',\n",
    "            'subtype': 'split_general',\n",
    "            'content': chunk_content,\n",
    "            'paragraph_count': 1,\n",
    "            'metadata': {\n",
    "                'source': 'paragraph_split',\n",
    "                'priority': 5\n",
    "            }\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "def analyze_chunks(chunks: List[Dict[str, Any]]) -> None:\n",
    "    print(\"\\n=== ì²­í¬ ë¶„ì„ ê²°ê³¼ ===\")\n",
    "    \n",
    "    type_counts = {}\n",
    "    subtype_counts = {}\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        chunk_type = chunk['type']\n",
    "        chunk_subtype = chunk['subtype']\n",
    "        \n",
    "        type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1\n",
    "        subtype_counts[chunk_subtype] = subtype_counts.get(chunk_subtype, 0) + 1\n",
    "    \n",
    "    print(\"íƒ€ì…ë³„ ë¶„í¬:\")\n",
    "    for chunk_type, count in type_counts.items():\n",
    "        print(f\"  {chunk_type}: {count}\")\n",
    "    \n",
    "    print(\"\\nì„¸ë¶€ íƒ€ì…ë³„ ë¶„í¬:\")\n",
    "    for subtype, count in subtype_counts.items():\n",
    "        print(f\"  {subtype}: {count}\")\n",
    "    \n",
    "    priority_counts = {}\n",
    "    for chunk in chunks:\n",
    "        priority = chunk['metadata']['priority']\n",
    "        priority_counts[priority] = priority_counts.get(priority, 0) + 1\n",
    "    \n",
    "    print(\"\\nìš°ì„ ìˆœìœ„ë³„ ë¶„í¬:\")\n",
    "    for priority in sorted(priority_counts.keys(), reverse=True):\n",
    "        print(f\"  ìš°ì„ ìˆœìœ„ {priority}: {priority_counts[priority]}ê°œ\")\n",
    "\n",
    "def main_pdf_processing():\n",
    "    pdf_file_path = '../data/raw/files/ëŒ€ì „ëŒ€í•™êµ_ëŒ€ì „ëŒ€í•™êµ 2024í•™ë…„ë„ ë‹¤ì¸µì  ìœµí•© í•™ìŠµê²½í—˜ í”Œë«í¼(MILE) ì „.pdf'\n",
    "    \n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        print(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_file_path}\")\n",
    "        return\n",
    "        \n",
    "    parsed_pdf_data = extract_text_and_tables(pdf_file_path)\n",
    "    pdf_chunks = create_structured_chunks(parsed_pdf_data)\n",
    "    \n",
    "    analyze_chunks(pdf_chunks)\n",
    "    \n",
    "    print(\"\\n=== ìƒ˜í”Œ ì²­í¬ ===\")\n",
    "    table_chunks = [c for c in pdf_chunks if c.get('type') == 'table']\n",
    "    if table_chunks:\n",
    "        print(\"ì²« ë²ˆì§¸ í‘œ ì²­í¬:\")\n",
    "        sample_table = table_chunks[0]\n",
    "        print(f\"  íƒ€ì…: {sample_table['type']}-{sample_table['subtype']}\")\n",
    "        print(f\"  ë‚´ìš©: {sample_table['content'][:200]}...\")\n",
    "        print(f\"  ìš°ì„ ìˆœìœ„: {sample_table['metadata']['priority']}\")\n",
    "    \n",
    "    paragraph_chunks = [c for c in pdf_chunks if c['type'] == 'paragraph']\n",
    "    if paragraph_chunks:\n",
    "        print(\"\\nì²« ë²ˆì§¸ ë¬¸ë‹¨ ì²­í¬:\")\n",
    "        sample_paragraph = paragraph_chunks[0]\n",
    "        print(f\"  íƒ€ì…: {sample_paragraph['type']}-{sample_paragraph['subtype']}\")\n",
    "        print(f\"  ë‚´ìš©: {sample_paragraph['content'][:200]}...\")\n",
    "        print(f\"  ë¬¸ë‹¨ ìˆ˜: {sample_paragraph['paragraph_count']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_pdf_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "153da80a-3090-45b8-96aa-fbe17b357ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ì´ 5ê°œ PDF íŒŒì¼ ì¼ê´„ ì²˜ë¦¬ ì‹œì‘ ---\n",
      "\n",
      "[1/5] íŒŒì¼ ì²˜ë¦¬ ì¤‘: ê¸°ì´ˆê³¼í•™ì—°êµ¬ì›_2025ë…„ë„ ì¤‘ì´ì˜¨ê°€ì†ê¸°ìš© ê·¹ì €ì˜¨ì‹œìŠ¤í…œ ìš´ì „ ìš©ì—­.pdf\n",
      "PDF íŒŒì¼ íŒŒì‹± ì‹œì‘: ../data/raw/files/ê¸°ì´ˆê³¼í•™ì—°êµ¬ì›_2025ë…„ë„ ì¤‘ì´ì˜¨ê°€ì†ê¸°ìš© ê·¹ì €ì˜¨ì‹œìŠ¤í…œ ìš´ì „ ìš©ì—­.pdf\n",
      "ì´ 49 í˜ì´ì§€ ë°œê²¬\n",
      "íŒŒì‹± ì™„ë£Œ - ë¬¸ë‹¨: 49, í‘œ: 52\n",
      "ì²­í‚¹ ì‘ì—… ì‹œì‘...\n",
      "ì²­í‚¹ ì™„ë£Œ - ì´ 192ê°œ ì²­í¬ ìƒì„±\n",
      "âœ… ê¸°ì´ˆê³¼í•™ì—°êµ¬ì›_2025ë…„ë„ ì¤‘ì´ì˜¨ê°€ì†ê¸°ìš© ê·¹ì €ì˜¨ì‹œìŠ¤í…œ ìš´ì „ ìš©ì—­.pdfì—ì„œ 192ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ.\n",
      "\n",
      "[2/5] íŒŒì¼ ì²˜ë¦¬ ì¤‘: ê³ ë ¤ëŒ€í•™êµ_ì°¨ì„¸ëŒ€ í¬í„¸Â·í•™ì‚¬ ì •ë³´ì‹œìŠ¤í…œ êµ¬ì¶•ì‚¬ì—….pdf\n",
      "PDF íŒŒì¼ íŒŒì‹± ì‹œì‘: ../data/raw/files/ê³ ë ¤ëŒ€í•™êµ_ì°¨ì„¸ëŒ€ í¬í„¸Â·í•™ì‚¬ ì •ë³´ì‹œìŠ¤í…œ êµ¬ì¶•ì‚¬ì—….pdf\n",
      "ì´ 297 í˜ì´ì§€ ë°œê²¬\n",
      "íŒŒì‹± ì™„ë£Œ - ë¬¸ë‹¨: 297, í‘œ: 377\n",
      "ì²­í‚¹ ì‘ì—… ì‹œì‘...\n",
      "ì²­í‚¹ ì™„ë£Œ - ì´ 808ê°œ ì²­í¬ ìƒì„±\n",
      "âœ… ê³ ë ¤ëŒ€í•™êµ_ì°¨ì„¸ëŒ€ í¬í„¸Â·í•™ì‚¬ ì •ë³´ì‹œìŠ¤í…œ êµ¬ì¶•ì‚¬ì—….pdfì—ì„œ 808ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ.\n",
      "\n",
      "[3/5] íŒŒì¼ ì²˜ë¦¬ ì¤‘: ì„œìš¸íŠ¹ë³„ì‹œ_2024ë…„ ì§€ë„ì •ë³´ í”Œë«í¼ ë° ì „ë¬¸í™œìš© ì—°ê³„ ì‹œìŠ¤í…œ ê³ ë„í™” ìš©.pdf\n",
      "PDF íŒŒì¼ íŒŒì‹± ì‹œì‘: ../data/raw/files/ì„œìš¸íŠ¹ë³„ì‹œ_2024ë…„ ì§€ë„ì •ë³´ í”Œë«í¼ ë° ì „ë¬¸í™œìš© ì—°ê³„ ì‹œìŠ¤í…œ ê³ ë„í™” ìš©.pdf\n",
      "ì´ 75 í˜ì´ì§€ ë°œê²¬\n",
      "íŒŒì‹± ì™„ë£Œ - ë¬¸ë‹¨: 75, í‘œ: 151\n",
      "ì²­í‚¹ ì‘ì—… ì‹œì‘...\n",
      "ì²­í‚¹ ì™„ë£Œ - ì´ 405ê°œ ì²­í¬ ìƒì„±\n",
      "âœ… ì„œìš¸íŠ¹ë³„ì‹œ_2024ë…„ ì§€ë„ì •ë³´ í”Œë«í¼ ë° ì „ë¬¸í™œìš© ì—°ê³„ ì‹œìŠ¤í…œ ê³ ë„í™” ìš©.pdfì—ì„œ 405ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ.\n",
      "\n",
      "[4/5] íŒŒì¼ ì²˜ë¦¬ ì¤‘: ì„œìš¸ì‹œë¦½ëŒ€í•™êµ_[ì‚¬ì „ê³µê°œ] í•™ì—…ì„±ì·¨ë„ ë‹¤ì°¨ì› ì¢…ë‹¨ë¶„ì„ í†µí•©ì‹œìŠ¤í…œ 1ì°¨.pdf\n",
      "PDF íŒŒì¼ íŒŒì‹± ì‹œì‘: ../data/raw/files/ì„œìš¸ì‹œë¦½ëŒ€í•™êµ_[ì‚¬ì „ê³µê°œ] í•™ì—…ì„±ì·¨ë„ ë‹¤ì°¨ì› ì¢…ë‹¨ë¶„ì„ í†µí•©ì‹œìŠ¤í…œ 1ì°¨.pdf\n",
      "ì´ 149 í˜ì´ì§€ ë°œê²¬\n",
      "íŒŒì‹± ì™„ë£Œ - ë¬¸ë‹¨: 149, í‘œ: 163\n",
      "ì²­í‚¹ ì‘ì—… ì‹œì‘...\n",
      "ì²­í‚¹ ì™„ë£Œ - ì´ 563ê°œ ì²­í¬ ìƒì„±\n",
      "âœ… ì„œìš¸ì‹œë¦½ëŒ€í•™êµ_[ì‚¬ì „ê³µê°œ] í•™ì—…ì„±ì·¨ë„ ë‹¤ì°¨ì› ì¢…ë‹¨ë¶„ì„ í†µí•©ì‹œìŠ¤í…œ 1ì°¨.pdfì—ì„œ 563ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ.\n",
      "\n",
      "[5/5] íŒŒì¼ ì²˜ë¦¬ ì¤‘: ëŒ€ì „ëŒ€í•™êµ_ëŒ€ì „ëŒ€í•™êµ 2024í•™ë…„ë„ ë‹¤ì¸µì  ìœµí•© í•™ìŠµê²½í—˜ í”Œë«í¼(MILE) ì „.pdf\n",
      "PDF íŒŒì¼ íŒŒì‹± ì‹œì‘: ../data/raw/files/ëŒ€ì „ëŒ€í•™êµ_ëŒ€ì „ëŒ€í•™êµ 2024í•™ë…„ë„ ë‹¤ì¸µì  ìœµí•© í•™ìŠµê²½í—˜ í”Œë«í¼(MILE) ì „.pdf\n",
      "ì´ 75 í˜ì´ì§€ ë°œê²¬\n",
      "íŒŒì‹± ì™„ë£Œ - ë¬¸ë‹¨: 75, í‘œ: 51\n",
      "ì²­í‚¹ ì‘ì—… ì‹œì‘...\n",
      "ì²­í‚¹ ì™„ë£Œ - ì´ 254ê°œ ì²­í¬ ìƒì„±\n",
      "âœ… ëŒ€ì „ëŒ€í•™êµ_ëŒ€ì „ëŒ€í•™êµ 2024í•™ë…„ë„ ë‹¤ì¸µì  ìœµí•© í•™ìŠµê²½í—˜ í”Œë«í¼(MILE) ì „.pdfì—ì„œ 254ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ.\n",
      "âœ… ëª¨ë“  ì²­í¬ ë°ì´í„°ë¥¼ ../data/processed/datapreprocessingbjs(pdfplumber)/all_pdf_chunks.jsonì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "--- ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ë° JSON ì €ì¥ ì™„ë£Œ ---\n",
      "ì´ 2222ê°œ ì²­í¬ê°€ ìƒì„±ë˜ì–´ ../data/processed/datapreprocessingbjs(pdfplumber)/all_pdf_chunks.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ìµœì¢… ì²­í¬ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜\n",
    "def save_chunks_to_json(chunks_data: List[Dict], output_file_path: str):\n",
    "    \"\"\"\n",
    "    ëª¨ë“  ì²­í¬ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"âœ… ëª¨ë“  ì²­í¬ ë°ì´í„°ë¥¼ {output_file_path}ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ JSON íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "\n",
    "# ìµœì¢… ì‹¤í–‰ í•¨ìˆ˜: ëª¨ë“  PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  JSONìœ¼ë¡œ ì €ì¥\n",
    "def process_all_pdfs_and_save_json(input_dir, output_dir, json_file_name=\"all_pdf_chunks.json\"):\n",
    "    \"\"\"\n",
    "    ì§€ì •ëœ ì…ë ¥ ë””ë ‰í„°ë¦¬ì˜ ëª¨ë“  PDF íŒŒì¼ì„ íŒŒì‹±í•˜ê³  ì²­í‚¹í•œ í›„,\n",
    "    ê²°ê³¼ë¥¼ ë‹¨ì¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ìƒì„±\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"ì¶œë ¥ ë””ë ‰í„°ë¦¬ê°€ ì—†ì–´ ìƒì„±í–ˆìŠµë‹ˆë‹¤: {output_dir}\")\n",
    "\n",
    "    all_chunks = []\n",
    "    file_list = [f for f in os.listdir(input_dir) if f.lower().endswith('.pdf')]\n",
    "    total_files = len(file_list)\n",
    "    \n",
    "    print(f\"\\n--- ì´ {total_files}ê°œ PDF íŒŒì¼ ì¼ê´„ ì²˜ë¦¬ ì‹œì‘ ---\")\n",
    "\n",
    "    for i, file_name in enumerate(file_list):\n",
    "        input_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        print(f\"\\n[{i+1}/{total_files}] íŒŒì¼ ì²˜ë¦¬ ì¤‘: {file_name}\")\n",
    "        \n",
    "        # PDF íŒŒì¼ íŒŒì‹± (ì˜¤ë¥˜ ë°©ì§€ ë¡œì§ ì ìš©)\n",
    "        parsed_data = extract_text_and_tables(input_path)\n",
    "        \n",
    "        # íŒŒì‹±ëœ ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë³€í™˜\n",
    "        if parsed_data:\n",
    "            chunks_for_file = create_structured_chunks(parsed_data)\n",
    "            \n",
    "            # ê° ì²­í¬ì— íŒŒì¼ëª… ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "            for chunk in chunks_for_file:\n",
    "                chunk['metadata']['source_file'] = file_name\n",
    "            \n",
    "            all_chunks.extend(chunks_for_file)\n",
    "            print(f\"âœ… {file_name}ì—ì„œ {len(chunks_for_file)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ.\")\n",
    "    \n",
    "    final_output_path = os.path.join(output_dir, json_file_name)\n",
    "    save_chunks_to_json(all_chunks, final_output_path)\n",
    "    print(f\"\\n--- ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ë° JSON ì €ì¥ ì™„ë£Œ ---\")\n",
    "    print(f\"ì´ {len(all_chunks)}ê°œ ì²­í¬ê°€ ìƒì„±ë˜ì–´ {final_output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# --- ì‹¤í–‰ ì˜ˆì‹œ ---\n",
    "input_folder = '../data/raw/files'\n",
    "output_folder = '../data/processed/datapreprocessingbjs(pdfplumber)'\n",
    "\n",
    "process_all_pdfs_and_save_json(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cfbe713b-b863-494e-818d-bb766718c999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ../data/processed/datapreprocessingbjs(pdfplumber)/all_pdf_chunks.jsonì—ì„œ 2222ê°œì˜ ì²­í¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_chunks_from_json(file_path):\n",
    "    \"\"\"JSON íŒŒì¼ì—ì„œ ì²­í¬ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"âœ… {file_path}ì—ì„œ {len(data)}ê°œì˜ ì²­í¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: JSON ë””ì½”ë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "        return None\n",
    "\n",
    "# JSON íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° ë°ì´í„° ë¡œë“œ\n",
    "json_file_path = '../data/processed/datapreprocessingbjs(pdfplumber)/all_pdf_chunks.json'\n",
    "chunks = load_chunks_from_json(json_file_path)\n",
    "\n",
    "if chunks is None:\n",
    "    # ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ì‹œ, ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f64e1ad9-e007-44a7-8f18-c5ee6d999b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì„ë² ë”© ëª¨ë¸ 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "âœ¨ ìƒˆë¡œìš´ ì»¬ë ‰ì…˜ 'pdf_rag_collection_snunlp'ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# chunks ë¦¬ìŠ¤íŠ¸ì˜ ê° ì²­í¬ë¥¼ ìˆœíšŒí•˜ë©° ì²­í¬ ID ê³ ìœ í™”\n",
    "for chunk in chunks:\n",
    "    # ë©”íƒ€ë°ì´í„°ì—ì„œ ì›ë³¸ íŒŒì¼ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°, ì—†ìœ¼ë©´ \"unknown\" ì‚¬ìš©\n",
    "    file_name = chunk['metadata'].get(\"source_file\", \"unknown\")\n",
    "    \n",
    "    # ê¸°ì¡´ chunk_idì— íŒŒì¼ ì´ë¦„ì„ ë¶™ì—¬ì„œ ê³ ìœ  ID ìƒì„±\n",
    "    chunk['chunk_id'] = f\"{file_name}_{chunk['chunk_id']}\"\n",
    "\n",
    "# ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "client = chromadb.Client()\n",
    "\n",
    "# PersistentClient ì ìš©\n",
    "db_path = \"../data/processed/datapreprocessingbjs(pdfplumber)/chromaDB\"  # ğŸ”¹ DB ì €ì¥ ê²½ë¡œ\n",
    "client = chromadb.PersistentClient(path=db_path)  # ğŸ”¹ ë©”ëª¨ë¦¬ ëŒ€ì‹  ë””ìŠ¤í¬ ê¸°ë°˜ DB ì‚¬ìš©\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ì§€ì •\n",
    "model_name = \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\"\n",
    "try:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(f\"âœ… ì„ë² ë”© ëª¨ë¸ '{model_name}'ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "except OSError as e:\n",
    "    print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    exit()\n",
    "\n",
    "def get_or_create_collection(collection_name):\n",
    "    \"\"\"ì»¬ë ‰ì…˜ì„ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "        print(f\"âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}'ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\")\n",
    "        return collection\n",
    "    except Exception:\n",
    "        collection = client.create_collection(name=collection_name)\n",
    "        print(f\"âœ¨ ìƒˆë¡œìš´ ì»¬ë ‰ì…˜ '{collection_name}'ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        return collection\n",
    "\n",
    "# ChromaDB ì»¬ë ‰ì…˜ ì„¤ì •\n",
    "collection_name = \"pdf_rag_collection_snunlp\"\n",
    "collection = get_or_create_collection(collection_name)\n",
    "\n",
    "# ê¸°ì¡´ ë°ì´í„°ê°€ ìˆì„ ê²½ìš° ì‚­ì œ (í…ŒìŠ¤íŠ¸ìš©)\n",
    "if collection.count() > 0:\n",
    "    print(f\"âš ï¸ ê¸°ì¡´ ë°ì´í„° {collection.count()}ê°œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.\")\n",
    "    client.delete_collection(collection_name)\n",
    "    collection = client.create_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "66068fec-7aa6-4786-8587-0fc6b7f5b629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì²­í¬ ì„ë² ë”© ë° ChromaDB ì €ì¥ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ChromaDB ì €ì¥ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:18<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´ 2222ê°œì˜ ì²­í¬ê°€ ChromaDBì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def embed_and_save_to_chroma(chunks, collection, model):\n",
    "    \"\"\"ì²­í¬ ë°ì´í„°ë¥¼ ì„ë² ë”©í•˜ì—¬ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(\"ğŸš€ ì²­í¬ ì„ë² ë”© ë° ChromaDB ì €ì¥ ì‹œì‘...\")\n",
    "    \n",
    "    # ë°ì´í„°ë¥¼ ChromaDBì— ë§ê²Œ ë³€í™˜\n",
    "    documents = [chunk['content'] for chunk in chunks]\n",
    "    metadatas = [chunk['metadata'] for chunk in chunks]\n",
    "    ids = [chunk['chunk_id'] for chunk in chunks]\n",
    "    \n",
    "    # 50ê°œì”© ì¼ê´„ ì²˜ë¦¬\n",
    "    batch_size = 50\n",
    "    total_chunks = len(documents)\n",
    "    \n",
    "    for i in tqdm(range(0, total_chunks, batch_size), desc=\"ChromaDB ì €ì¥ ì¤‘\"):\n",
    "        batch_documents = documents[i:i + batch_size]\n",
    "        batch_metadatas = metadatas[i:i + batch_size]\n",
    "        batch_ids = ids[i:i + batch_size]\n",
    "        \n",
    "        # ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ë²¡í„° ìƒì„±\n",
    "        embeddings = model.encode(batch_documents, show_progress_bar=False).tolist()\n",
    "        \n",
    "        # ì„ë² ë”©ê³¼ í•¨ê»˜ ë°ì´í„° ì €ì¥\n",
    "        collection.add(\n",
    "            embeddings=embeddings,\n",
    "            documents=batch_documents,\n",
    "            metadatas=batch_metadatas,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "    \n",
    "    print(f\"âœ… ì´ {collection.count()}ê°œì˜ ì²­í¬ê°€ ChromaDBì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ìµœì¢… ì‹¤í–‰ í•¨ìˆ˜ í˜¸ì¶œ\n",
    "embed_and_save_to_chroma(chunks, collection, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1aaacf54-4e21-4398-a3f4-03a6c2ed59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromaDB ì»¬ë ‰ì…˜ ë¡œë“œ\n",
    "collection = client.get_collection(name=collection_name)\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì €ì¥ ì‹œ ì‚¬ìš©í•œ ëª¨ë¸ê³¼ ë™ì¼í•´ì•¼ í•¨)\n",
    "model = SentenceTransformer(\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "985fe44b-d1f2-4c45-b61d-239e76223aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI API í‚¤ ì„¤ì • (ë˜ëŠ” ë‹¤ë¥¸ LLM API í‚¤)\n",
    "openai_api_key ='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'\n",
    "client_llm = OpenAI(api_key=openai_api_key)\n",
    "print(\"âœ… OpenAI LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "16b20cf0-92ab-4da3-8019-c82bb4510662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_augmented_generation(query: str):\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— RAG ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        query (str): ì‚¬ìš©ìì˜ ì§ˆë¬¸\n",
    "    \n",
    "    Returns:\n",
    "        str: LLMì´ ìƒì„±í•œ ë‹µë³€\n",
    "    \"\"\"\n",
    "    print(\"1. ê²€ìƒ‰(Retrieval) ì‹œì‘...\")\n",
    "    # 1.1. ì§ˆë¬¸ì„ ì„ë² ë”©\n",
    "    query_embedding = model.encode([query]).tolist()\n",
    "    \n",
    "    # 1.2. ChromaDBì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰\n",
    "    # n_resultsë¥¼ ëŠ˜ë ¤ ë” ë§ì€ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=10,  # ìƒìœ„ 10ê°œ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜´\n",
    "    )\n",
    "    \n",
    "    # 1.3. ê²€ìƒ‰ëœ ë¬¸ì„œ(ì²­í¬) ì¶”ì¶œ\n",
    "    retrieved_chunks = results['documents'][0]\n",
    "    retrieved_metas = results['metadatas'][0]\n",
    "    print(f\"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(retrieved_chunks)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬\")\n",
    "    \n",
    "    print(\"2. ë³´ê°•(Augmentation) ì‹œì‘...\")\n",
    "    # 2.1. ê²€ìƒ‰ëœ ì²­í¬ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨\n",
    "    context = \"\"\n",
    "    for doc, meta in zip(retrieved_chunks, retrieved_metas):\n",
    "        file_name = meta.get(\"source_file\", \"unknown\")\n",
    "        context += f\"[ì¶œì²˜: {file_name}]\\n{doc}\\n\\n\"\n",
    "    \n",
    "    # 2.2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "    system_prompt = f\"\"\"\n",
    "    ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤.\n",
    "    ë¬¸ì„œ ë‚´ìš©ì„ ìš”ì•½í•˜ê³ , ë°˜ë“œì‹œ ë¬¸ì„œì—ì„œ ì œê³µí•˜ëŠ” ì •ë³´ë§Œ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "    ë¬¸ì„œì— ê´€ë ¨ëœ ë‚´ìš©ì´ í™•ì‹¤íˆ ì—†ëŠ” ê²½ìš°ì—ë§Œ 'ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n",
    "\n",
    "    [ê²€ìƒ‰ëœ ë¬¸ì„œ]\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"3. ìƒì„±(Generation) ì‹œì‘...\")\n",
    "    # 3.1. LLMì— ì§ˆë¬¸ ì „ë‹¬ ë° ë‹µë³€ ìƒì„±\n",
    "    response = client_llm.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",  # ë˜ëŠ” \"gpt-3.5-turbo\" ë“±\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=700\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e008f3d3-3a5f-4bb4-846b-964c2a8b69f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ê²€ìƒ‰(Retrieval) ì‹œì‘...\n",
      "âœ… ê²€ìƒ‰ ì™„ë£Œ: 10ê°œì˜ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬\n",
      "2. ë³´ê°•(Augmentation) ì‹œì‘...\n",
      "3. ìƒì„±(Generation) ì‹œì‘...\n",
      "\n",
      "--- ì§ˆë¬¸ ---\n",
      "ëŒ€ì „ëŒ€í•™êµì˜ ë‹¤ì¸µì  ìœµí•© í•™ìŠµê²½í—˜ í”Œë«í¼ì— ëŒ€í•´ ìì„¸íˆ ì•Œë ¤ì¤˜.\n",
      "\n",
      "--- ë‹µë³€ ---\n",
      "ëŒ€ì „ëŒ€í•™êµì˜ ë‹¤ì¸µì  ìœµí•© í•™ìŠµê²½í—˜ í”Œë«í¼(MILE)ì€ ìœµí•© êµìœ¡ í˜ì‹ ê³¼ í•™ìƒ ë§ì¶¤í˜• í•™ìŠµ ì§€ì›ì„ ëª©í‘œë¡œ êµ¬ì¶•ë˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì£¼ìš” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. ì‚¬ì—… ëª©í‘œ  \n",
      "- ìœµí•© êµìœ¡ í•™ìŠµ ì„±ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ì í•˜ê³  í”¼ë“œë°±ì„ ê°•í™”  \n",
      "- ê²½ê³„ ì—†ëŠ” í•™ìƒ ë§ì¶¤í˜• í•™ìŠµê²½ë¡œ ì œê³µì„ í†µí•œ ìœµí•© êµìœ¡ í™•ì‚°  \n",
      "- ìœ ì—°í•œ í•™ì‚¬ ìš´ì˜ê³¼ ìœµí•© êµìœ¡ í”„ë¡œê·¸ë¨ ì§€ì›  \n",
      "\n",
      "2. ì‹œìŠ¤í…œ ë° ì‚¬ì—… ë²”ìœ„  \n",
      "- â€˜ë‚˜ë…¸ ë§ˆì´í¬ë¡œë””ê·¸ë¦¬ ì „ì‚°ì‹œìŠ¤í…œâ€™ê³¼ â€˜ì ì‘í˜• êµìˆ˜Â·í•™ìŠµ ì‹œìŠ¤í…œâ€™ê³¼ ì—°ê³„  \n",
      "- êµìˆ˜ ë° ìœµí•©êµìœ¡ ë‹´ë‹¹ ì½”ë””ë„¤ì´í„°ì™€ í•™ìƒì´ í™œìš©í•  ìˆ˜ ìˆëŠ” ìœµí•© êµìœ¡ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹ ê·œ êµ¬ì¶•  \n",
      "- ë‚˜ë…¸Â·ë§ˆì´í¬ë¡œë””ê·¸ë¦¬ ì¶œë ¥ í™”ë©´ ì¡°ì • ë° êµìˆ˜ ìœµí•©êµìœ¡ ì—­ëŸ‰ ì§„ë‹¨ ê¸°ëŠ¥ í¬í•¨  \n",
      "- í•™ìƒ ì¤‘ì‹¬ êµìœ¡ ê¸°ë°˜ì¸ â€˜ACT êµê³¼ì¸ì¦ í™•ëŒ€â€™ë¥¼ ëŒ€ë¹„í•œ í•™ì‚¬ ê°•ì¢Œê°œì„¤ ì—°ê³„ ë³´ì™„ ë° ì¶”ê°€ ê°œì„   \n",
      "\n",
      "3. í•„ìš”ì„± ë° ìš´ì˜ ë°©í–¥  \n",
      "- í•™ìƒë“¤ì´ ììœ¨ì ìœ¼ë¡œ í•™ìŠµ ê²½ë¡œë¥¼ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ ê°œë°©ì  ìœµí•© êµìœ¡ ìš´ì˜ê³¼ ì„±ê³¼ ê´€ë¦¬ ì²´ê³„ ë§ˆë ¨  \n",
      "- ë¬´ê²½ê³„ êµìœ¡ê³¼ì • ì°¸ì—¬ í•™ìƒ ì§€ì› ë° ìœµí•© êµìœ¡ í™œì„±í™”ë¥¼ ìœ„í•œ êµìˆ˜-í•™ìƒ í™œìš© í”Œë«í¼ êµ¬ì¶•  \n",
      "- ì „ì£¼ê¸°ì  í•™ìƒ ì§€ì›ì„ ìœ„í•œ ì œë°˜ í™˜ê²½ ì¡°ì„±  \n",
      "\n",
      "ìš”ì•½í•˜ë©´, MILE í”Œë«í¼ì€ ëŒ€ì „ëŒ€í•™êµê°€ ìœµí•© êµìœ¡ì„ ì²´ê³„ì ì´ê³  ì‹¤ì‹œê°„ìœ¼ë¡œ ì§€ì›í•˜ë©° í•™ìƒ ê°œê°œì¸ ë§ì¶¤í˜• í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ë‹¤ì¸µì  í†µí•© í•™ìŠµê´€ë¦¬ ì‹œìŠ¤í…œìœ¼ë¡œ, ê¸°ì¡´ì˜ ë‚˜ë…¸Â·ë§ˆì´í¬ë¡œë””ê·¸ë¦¬ ì‹œìŠ¤í…œê³¼ ì ì‘í˜• í•™ìŠµ ì‹œìŠ¤í…œì„ ì—°ê³„ ë³´ì™„í•˜ì—¬ ìœµí•© êµìœ¡ì˜ í˜ì‹ ê³¼ í™•ì‚°ì„ ë„ëª¨í•˜ëŠ” ì‚¬ì—…ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# --- ì‚¬ìš© ì˜ˆì‹œ ---\n",
    "user_query = \"ëŒ€ì „ëŒ€í•™êµì˜ ë‹¤ì¸µì  ìœµí•© í•™ìŠµê²½í—˜ í”Œë«í¼ì— ëŒ€í•´ ìì„¸íˆ ì•Œë ¤ì¤˜.\"\n",
    "answer = retrieval_augmented_generation(user_query)\n",
    "\n",
    "print(\"\\n--- ì§ˆë¬¸ ---\")\n",
    "print(user_query)\n",
    "print(\"\\n--- ë‹µë³€ ---\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "788ef98e-645b-4c04-a65a-f2e7b4b0dcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4901.6904296875 KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_path = \"../data/processed/datapreprocessingbjs(pdfplumber)/all_pdf_chunks.json\"\n",
    "print(os.path.getsize(file_path) / 1024, \"KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "447f0c48-fe4d-47cd-af04-b86d8d330235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "\n",
      "    {\n",
      "\n",
      "        \"type\": \"table\",\n",
      "\n",
      "        \"subtype\": \"general\",\n",
      "\n",
      "        \"content\": \"ë¬¸ì„œë²ˆí˜¸: ê°œì •ë²ˆí˜¸, -: 0. ë¬¸ì„œë²ˆí˜¸: ë°œ í–‰ ì¼, -: 2024. 10. 30\",\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(5):  # ì²˜ìŒ 5ì¤„ë§Œ\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5a9b1361-01b8-4d89-8eb1-4a66d4ccc9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ê°œìˆ˜: 2222\n",
      "data íƒ€ì…: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"ì´ ê°œìˆ˜:\", len(data))\n",
    "print(\"data íƒ€ì…:\", type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "557e4910-f743-4cc3-8ac0-a03218914046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²« ë²ˆì§¸ í•­ëª©: dict_keys(['type', 'subtype', 'content', 'original_rows', 'metadata', 'chunk_id'])\n"
     ]
    }
   ],
   "source": [
    "if isinstance(data, list):\n",
    "    print(\"ì²« ë²ˆì§¸ í•­ëª©:\", data[0].keys())\n",
    "elif isinstance(data, dict):\n",
    "    print(\"keys:\", list(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "816afd37-ceb1-46ee-89dd-7d6bd1953720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Chunk 1 ===\n",
      "type: table\n",
      "subtype: general\n",
      "content: ë¬¸ì„œë²ˆí˜¸: ê°œì •ë²ˆí˜¸, -: 0. ë¬¸ì„œë²ˆí˜¸: ë°œ í–‰ ì¼, -: 2024. 10. 30\n",
      "original_rows: 3\n",
      "metadata: {'table_id': 'table_1_0', 'source': 'pdf_table_extraction', 'priority': 4, 'source_file': 'ê¸°ì´ˆê³¼í•™ì—°êµ¬ì›_2025ë…„ë„ ì¤‘ì´ì˜¨ê°€ì†ê¸°ìš© ê·¹ì €ì˜¨ì‹œìŠ¤í…œ ìš´ì „ ìš©ì—­.pdf'}\n",
      "chunk_id: chunk_0000\n",
      "\n",
      "=== Chunk 2 ===\n",
      "type: table\n",
      "subtype: general\n",
      "content: ë¬¸ì„œìƒíƒœ ì‹ë³„ (ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì— ì²´í¬): â–¡ ë¬¸ì„œìƒíƒœ-1 : ìµœì¢…ì„¤ê³„, í•´ì„, êµ¬ë§¤ ë“±ì— ì´ìš©ë˜ëŠ” ì„¤ê³„ í™•ì¸ëœ ì •ë³´. ë¬¸ì„œìƒíƒœ ì‹ë³„ (ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì— ì²´í¬): â–¡ ë¬¸ì„œìƒíƒœ-2 : ...\n",
      "original_rows: 4\n",
      "metadata: {'table_id': 'table_1_1', 'source': 'pdf_table_extraction', 'priority': 4, 'source_file': 'ê¸°ì´ˆê³¼í•™ì—°êµ¬ì›_2025ë…„ë„ ì¤‘ì´ì˜¨ê°€ì†ê¸°ìš© ê·¹ì €ì˜¨ì‹œìŠ¤í…œ ìš´ì „ ìš©ì—­.pdf'}\n",
      "chunk_id: chunk_0001\n",
      "\n",
      "=== Chunk 3 ===\n",
      "type: table\n",
      "subtype: general\n",
      "content: : ì‘ ì„±, ì†Œì† ë° ì§ìœ„: ê·¹ì €ì˜¨íŒ€/ì—°êµ¬ìœ„ì›, ì„± ëª…: ìœ ì •í˜„. : ê²€ í† , ì†Œì† ë° ì§ìœ„: í’ˆì§ˆê´€ë¦¬ì‹¤/ì—°êµ¬ìœ„ì›, ì„± ëª…: ì‹ ì¼ê²½. ì†Œì† ë° ì§ìœ„: ê·¹ì €ì˜¨íŒ€/íŒ€ì¥, ì„± ëª…: ì‹ ì¬...\n",
      "original_rows: 5\n",
      "metadata: {'table_id': 'table_1_2', 'source': 'pdf_table_extraction', 'priority': 4, 'source_file': 'ê¸°ì´ˆê³¼í•™ì—°êµ¬ì›_2025ë…„ë„ ì¤‘ì´ì˜¨ê°€ì†ê¸°ìš© ê·¹ì €ì˜¨ì‹œìŠ¤í…œ ìš´ì „ ìš©ì—­.pdf'}\n",
      "chunk_id: chunk_0002\n"
     ]
    }
   ],
   "source": [
    "# ì²« 3ê°œ í™•ì¸\n",
    "for i, chunk in enumerate(data[:3], 1):\n",
    "    print(f\"\\n=== Chunk {i} ===\")\n",
    "    for k, v in chunk.items():\n",
    "        if isinstance(v, str) and len(v) > 100:\n",
    "            print(f\"{k}: {v[:100]}...\")  # ë„ˆë¬´ ê¸´ ë¬¸ìì—´ì€ ì•ë¶€ë¶„ë§Œ\n",
    "        else:\n",
    "            print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7273f950-28a4-4326-9e96-6ef18ec94906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'my_chroma_db' í´ë”ì™€ ëª¨ë“  ë‚´ìš©ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# ì‚­ì œí•  í´ë” ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”.\n",
    "folder_path = 'my_chroma_db'\n",
    "try:\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(f\"'{folder_path}' í´ë”ì™€ ëª¨ë“  ë‚´ìš©ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "except OSError as e:\n",
    "    print(f\"ì˜¤ë¥˜: {e.strerror} - {e.filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a5168-7e56-40eb-9e53-27cbbbd84ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
