{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdd892f7-ac2f-45f6-a074-2434702b16db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "출력 디렉터리가 없어 생성했습니다: ../data/processed/datapreprocessingbjs(pdfplumber)/text_single_file\n",
      "--- 변환 중: 대전대학교_대전대학교 2024학년도 다층적 융합 학습경험 플랫폼(MILE) 전.pdf ([<Page:1>, <Page:2>, <Page:3>, <Page:4>, <Page:5>, <Page:6>, <Page:7>, <Page:8>, <Page:9>, <Page:10>, <Page:11>, <Page:12>, <Page:13>, <Page:14>, <Page:15>, <Page:16>, <Page:17>, <Page:18>, <Page:19>, <Page:20>, <Page:21>, <Page:22>, <Page:23>, <Page:24>, <Page:25>, <Page:26>, <Page:27>, <Page:28>, <Page:29>, <Page:30>, <Page:31>, <Page:32>, <Page:33>, <Page:34>, <Page:35>, <Page:36>, <Page:37>, <Page:38>, <Page:39>, <Page:40>, <Page:41>, <Page:42>, <Page:43>, <Page:44>, <Page:45>, <Page:46>, <Page:47>, <Page:48>, <Page:49>, <Page:50>, <Page:51>, <Page:52>, <Page:53>, <Page:54>, <Page:55>, <Page:56>, <Page:57>, <Page:58>, <Page:59>, <Page:60>, <Page:61>, <Page:62>, <Page:63>, <Page:64>, <Page:65>, <Page:66>, <Page:67>, <Page:68>, <Page:69>, <Page:70>, <Page:71>, <Page:72>, <Page:73>, <Page:74>, <Page:75>] 페이지) ---\n",
      "✅ 성공: 대전대학교_대전대학교 2024학년도 다층적 융합 학습경험 플랫폼(MILE) 전.pdf -> ../data/processed/datapreprocessingbjs(pdfplumber)/text_single_file/대전대학교_대전대학교 2024학년도 다층적 융합 학습경험 플랫폼(MILE) 전.txt\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "def convert_pdf_single_file(input_path, output_directory):\n",
    "    \"\"\"\n",
    "    pdfplumber를 사용하여 단일 PDF 파일의 모든 텍스트를 추출하고 텍스트 파일로 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): 변환할 PDF 파일의 전체 경로.\n",
    "        output_directory (str): 변환된 텍스트 파일을 저장할 디렉터리.\n",
    "    \"\"\"\n",
    "    # 출력 디렉터리가 없으면 생성\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "        print(f\"출력 디렉터리가 없어 생성했습니다: {output_directory}\")\n",
    "\n",
    "    # 파일명 설정 (확장자를 .txt로 변경)\n",
    "    file_name = os.path.basename(input_path)\n",
    "    base_name, _ = os.path.splitext(file_name)\n",
    "    output_path = os.path.join(output_directory, f'{base_name}.txt')\n",
    "\n",
    "    full_text = \"\"\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(input_path) as pdf:\n",
    "            print(f\"--- 변환 중: {file_name} ({pdf.pages} 페이지) ---\")\n",
    "            for page in pdf.pages:\n",
    "                # 페이지의 텍스트 추출. 페이지 번호를 포함하여 구조화 가능\n",
    "                extracted_text = page.extract_text()\n",
    "                if extracted_text:\n",
    "                    full_text += extracted_text + \"\\n\"  # 페이지 간 줄 바꿈 추가\n",
    "\n",
    "        # 추출된 전체 텍스트를 파일로 저장\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(full_text)\n",
    "\n",
    "        print(f\"✅ 성공: {file_name} -> {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생 ({file_name}): {e}\")\n",
    "\n",
    "# --- 실행 예시 ---\n",
    "# 변환할 PDF 파일 경로\n",
    "input_file = '../data/raw/files/대전대학교_대전대학교 2024학년도 다층적 융합 학습경험 플랫폼(MILE) 전.pdf'\n",
    "# 텍스트 파일을 저장할 디렉터리\n",
    "output_dir = '../data/processed/datapreprocessingbjs(pdfplumber)/text_single_file'\n",
    "\n",
    "# 함수 호출\n",
    "convert_pdf_single_file(input_file, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32edbeda-d350-4d66-bffe-e9822a7006c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "출력 디렉터리가 없어 생성했습니다: ../data/processed/datapreprocessingbjs(pdfplumber)/text_all_files\n",
      "\n",
      "--- PDF 파일 일괄 변환 시작 (총 5개) ---\n",
      "✅ 성공: 기초과학연구원_2025년도 중이온가속기용 극저온시스템 운전 용역.pdf -> ../data/processed/datapreprocessingbjs(pdfplumber)/text_all_files/기초과학연구원_2025년도 중이온가속기용 극저온시스템 운전 용역.txt\n",
      "✅ 성공: 고려대학교_차세대 포털·학사 정보시스템 구축사업.pdf -> ../data/processed/datapreprocessingbjs(pdfplumber)/text_all_files/고려대학교_차세대 포털·학사 정보시스템 구축사업.txt\n",
      "✅ 성공: 서울특별시_2024년 지도정보 플랫폼 및 전문활용 연계 시스템 고도화 용.pdf -> ../data/processed/datapreprocessingbjs(pdfplumber)/text_all_files/서울특별시_2024년 지도정보 플랫폼 및 전문활용 연계 시스템 고도화 용.txt\n",
      "✅ 성공: 서울시립대학교_[사전공개] 학업성취도 다차원 종단분석 통합시스템 1차.pdf -> ../data/processed/datapreprocessingbjs(pdfplumber)/text_all_files/서울시립대학교_[사전공개] 학업성취도 다차원 종단분석 통합시스템 1차.txt\n",
      "✅ 성공: 대전대학교_대전대학교 2024학년도 다층적 융합 학습경험 플랫폼(MILE) 전.pdf -> ../data/processed/datapreprocessingbjs(pdfplumber)/text_all_files/대전대학교_대전대학교 2024학년도 다층적 융합 학습경험 플랫폼(MILE) 전.txt\n",
      "\n",
      "--- 변환 작업 완료 ---\n",
      "성공적으로 변환된 파일 수: 5\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "def convert_all_pdfs_in_folder(input_directory, output_directory):\n",
    "    \"\"\"\n",
    "    지정된 디렉터리 내의 모든 PDF 파일을 변환합니다.\n",
    "\n",
    "    Args:\n",
    "        input_directory (str): PDF 파일들이 있는 디렉터리 경로.\n",
    "        output_directory (str): 변환된 텍스트 파일을 저장할 디렉터리.\n",
    "    \"\"\"\n",
    "    # 출력 디렉터리가 없으면 생성\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "        print(f\"출력 디렉터리가 없어 생성했습니다: {output_directory}\")\n",
    "\n",
    "    # 입력 디렉터리 내 모든 파일을 순회하며 PDF 파일만 찾기\n",
    "    pdf_files = [f for f in os.listdir(input_directory) if f.lower().endswith(\".pdf\")]\n",
    "    \n",
    "    total_files = len(pdf_files)\n",
    "    success_count = 0\n",
    "    failed_files = []\n",
    "\n",
    "    print(f\"\\n--- PDF 파일 일괄 변환 시작 (총 {total_files}개) ---\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        input_path = os.path.join(input_directory, pdf_file)\n",
    "        base_name, _ = os.path.splitext(pdf_file)\n",
    "        output_path = os.path.join(output_directory, f'{base_name}.txt')\n",
    "        \n",
    "        full_text = \"\"\n",
    "\n",
    "        try:\n",
    "            with pdfplumber.open(input_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    extracted_text = page.extract_text()\n",
    "                    if extracted_text:\n",
    "                        full_text += extracted_text + \"\\n\"\n",
    "\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(full_text)\n",
    "\n",
    "            print(f\"✅ 성공: {pdf_file} -> {os.path.relpath(output_path, start='.')}\")\n",
    "            success_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 오류 발생 ({pdf_file}): {e}\")\n",
    "            failed_files.append(pdf_file)\n",
    "\n",
    "    print(\"\\n--- 변환 작업 완료 ---\")\n",
    "    print(f\"성공적으로 변환된 파일 수: {success_count}\")\n",
    "    if failed_files:\n",
    "        print(f\"오류가 발생한 파일 목록: {len(failed_files)}개\")\n",
    "        for f in failed_files:\n",
    "            print(f\"- {f}\")\n",
    "\n",
    "# --- 실행 예시 ---\n",
    "# PDF 파일들이 있는 입력 폴더 경로\n",
    "input_folder = '../data/raw/files'\n",
    "# 변환된 텍스트 파일을 저장할 출력 디렉터리\n",
    "output_folder = '../data/processed/datapreprocessingbjs(pdfplumber)/text_all_files'\n",
    "\n",
    "# 함수 호출\n",
    "convert_all_pdfs_in_folder(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "207af9db-6d94-4184-b991-136e9028a807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 파일 파싱 시작: ../data/raw/files/대전대학교_대전대학교 2024학년도 다층적 융합 학습경험 플랫폼(MILE) 전.pdf\n",
      "총 75 페이지 발견\n",
      "파싱 완료 - 문단: 75, 표: 51\n",
      "청킹 작업 시작...\n",
      "청킹 완료 - 총 254개 청크 생성\n",
      "\n",
      "=== 청크 분석 결과 ===\n",
      "타입별 분포:\n",
      "  table: 51\n",
      "  paragraph: 203\n",
      "\n",
      "세부 타입별 분포:\n",
      "  organization: 1\n",
      "  general: 30\n",
      "  budget: 4\n",
      "  schedule: 16\n",
      "  split_general: 202\n",
      "  header: 1\n",
      "\n",
      "우선순위별 분포:\n",
      "  우선순위 10: 16개\n",
      "  우선순위 8: 4개\n",
      "  우선순위 6: 2개\n",
      "  우선순위 5: 202개\n",
      "  우선순위 4: 30개\n",
      "\n",
      "=== 샘플 청크 ===\n",
      "첫 번째 표 청크:\n",
      "  타입: table-organization\n",
      "  내용: 문의: 입찰, 소속 / 담당자: 총무팀 이병석, 전화: 042-280-2163. 문의: 사업, 소속 / 담당자: 대학교육혁신원 교수학습개발센터\n",
      "홍나래, 전화: 042-280-4036...\n",
      "  우선순위: 6\n",
      "\n",
      "첫 번째 문단 청크:\n",
      "  타입: paragraph-split_general\n",
      "  내용: (재공고)대전대학교                    다층적         융합      학습경험                  \n",
      "                                                                                  \n",
      "                                            ...\n",
      "  문단 수: 1\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def convert_table_to_natural_language(table_rows: List[List[str]]) -> str:\n",
    "    \"\"\"\n",
    "    pdfplumber에서 추출한 표 데이터를 자연어 형태로 변환.\n",
    "    NoneType 오류를 방지하는 로직이 추가되었습니다.\n",
    "    \"\"\"\n",
    "    if not table_rows or len(table_rows) < 2:\n",
    "        return \"\"\n",
    "    \n",
    "    # 첫 번째 행을 헤더로 가정 (NoneType 방지)\n",
    "    headers = [h if h is not None else \"\" for h in table_rows[0]]\n",
    "    content_rows = table_rows[1:]\n",
    "    \n",
    "    natural_text = []\n",
    "    \n",
    "    # '월' 또는 '일정'이 포함된 헤더가 있을 경우 일정표로 간주\n",
    "    if any('월' in h for h in headers) or any('일정' in h for h in headers):\n",
    "        natural_text.append(\"다음은 추진일정표입니다:\")\n",
    "        for row in content_rows:\n",
    "            row = [cell if cell is not None else \"\" for cell in row]\n",
    "            if len(row) > 1 and row[0].strip():\n",
    "                task = row[0].strip()\n",
    "                schedule_info = []\n",
    "                for i, cell in enumerate(row[1:], 1):\n",
    "                    if cell.strip() and i < len(headers):\n",
    "                        header = headers[i]\n",
    "                        schedule_info.append(f\"{header}에 {task}\")\n",
    "                if schedule_info:\n",
    "                    natural_text.append(\". \".join(schedule_info))\n",
    "    else:\n",
    "        # 일반 표 처리\n",
    "        for row in content_rows:\n",
    "            row = [cell if cell is not None else \"\" for cell in row]\n",
    "            if len(row) >= len(headers):\n",
    "                row_text = []\n",
    "                for i, (header, cell) in enumerate(zip(headers, row)):\n",
    "                    if cell and cell.strip():\n",
    "                        row_text.append(f\"{header}: {cell.strip()}\")\n",
    "                if row_text:\n",
    "                    natural_text.append(\", \".join(row_text))\n",
    "    \n",
    "    return \". \".join(natural_text)\n",
    "\n",
    "def extract_text_and_tables(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    pdfplumber를 사용하여 PDF 파일에서 텍스트와 표를 추출하고 구조화합니다.\n",
    "    NoneType 오류를 방지하는 로직이 추가되었습니다.\n",
    "    \"\"\"\n",
    "    print(f\"PDF 파일 파싱 시작: {file_path}\")\n",
    "    \n",
    "    parsed_data = {\n",
    "        'sections': [],\n",
    "        'tables': [],\n",
    "        'metadata': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            parsed_data['metadata']['total_pages'] = len(pdf.pages)\n",
    "            print(f\"총 {len(pdf.pages)} 페이지 발견\")\n",
    "\n",
    "            for page_idx, page in enumerate(pdf.pages):\n",
    "                # 페이지 내 모든 표 추출\n",
    "                page_tables = page.extract_tables()\n",
    "                \n",
    "                # 표 데이터는 별도로 저장\n",
    "                for i, table in enumerate(page_tables):\n",
    "                    table_data = {\n",
    "                        'id': f\"table_{page_idx}_{i}\",\n",
    "                        'type': 'table',\n",
    "                        'rows': table,\n",
    "                        'raw_content': convert_table_to_natural_language(table),\n",
    "                        'metadata': { 'page': page_idx + 1 }\n",
    "                    }\n",
    "                    if table_data['raw_content']:\n",
    "                        parsed_data['tables'].append(table_data)\n",
    "\n",
    "                # 페이지 내 텍스트 추출 (NoneType 방지)\n",
    "                page_text_with_layout = page.extract_text(x_tolerance=2, y_tolerance=2, layout=True)\n",
    "                if not page_text_with_layout:\n",
    "                    continue\n",
    "\n",
    "                # 텍스트를 문단 단위로 분리\n",
    "                paragraphs = [para for para in page_text_with_layout.split('\\n\\n') if para.strip()]\n",
    "                for i, para in enumerate(paragraphs):\n",
    "                    parsed_data['sections'].append({\n",
    "                        'id': f\"para_{page_idx}_{i}\",\n",
    "                        'type': 'paragraph',\n",
    "                        'content': para.strip(),\n",
    "                        'metadata': { 'page': page_idx + 1 }\n",
    "                    })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"PDF 파싱 중 오류 발생: {e}\")\n",
    "    \n",
    "    parsed_data['metadata']['total_paragraphs'] = len(parsed_data['sections'])\n",
    "    parsed_data['metadata']['total_tables'] = len(parsed_data['tables'])\n",
    "    print(f\"파싱 완료 - 문단: {len(parsed_data['sections'])}, 표: {len(parsed_data['tables'])}\")\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "### **청크 생성 및 분석 함수**\n",
    "\n",
    "def create_structured_chunks(parsed_data: Dict[str, Any], max_chunk_size: int = 1000) -> List[Dict[str, Any]]:\n",
    "    chunks = []\n",
    "    \n",
    "    print(\"청킹 작업 시작...\")\n",
    "    \n",
    "    for table in parsed_data['tables']:\n",
    "        table_chunk = create_table_chunk(table)\n",
    "        if table_chunk:\n",
    "            chunks.append(table_chunk)\n",
    "    \n",
    "    paragraph_chunks = create_paragraph_chunks(parsed_data['sections'], max_chunk_size)\n",
    "    chunks.extend(paragraph_chunks)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk['chunk_id'] = f\"chunk_{i:04d}\"\n",
    "    \n",
    "    print(f\"청킹 완료 - 총 {len(chunks)}개 청크 생성\")\n",
    "    return chunks\n",
    "\n",
    "def create_table_chunk(table_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if not table_data.get('raw_content') or not table_data['raw_content'].strip():\n",
    "        return None\n",
    "    \n",
    "    table_type = identify_table_type(table_data)\n",
    "    \n",
    "    chunk = {\n",
    "        'type': 'table',\n",
    "        'subtype': table_type,\n",
    "        'content': table_data['raw_content'],\n",
    "        'original_rows': len(table_data.get('rows', [])),\n",
    "        'metadata': {\n",
    "            'table_id': table_data.get('id'),\n",
    "            'source': 'pdf_table_extraction',\n",
    "            'priority': get_table_priority(table_type)\n",
    "        }\n",
    "    }\n",
    "    return chunk\n",
    "\n",
    "def identify_table_type(table_data: Dict[str, Any]) -> str:\n",
    "    content = table_data.get('raw_content', '').lower()\n",
    "    rows = table_data.get('rows', [])\n",
    "    \n",
    "    if rows and rows[0]:\n",
    "        header = ' '.join(cell for cell in rows[0] if cell).lower()\n",
    "        if any(keyword in header for keyword in ['월', '일정', '추진']):\n",
    "            return 'schedule'\n",
    "        if any(keyword in header for keyword in ['예산', '비용', '금액', '원']):\n",
    "            return 'budget'\n",
    "        if any(keyword in header for keyword in ['담당', '역할', '조직']):\n",
    "            return 'organization'\n",
    "    \n",
    "    if any(keyword in content for keyword in ['일정', '월', '추진']):\n",
    "        return 'schedule'\n",
    "    elif any(keyword in content for keyword in ['예산', '비용', '천원']):\n",
    "        return 'budget'\n",
    "    else:\n",
    "        return 'general'\n",
    "\n",
    "def get_table_priority(table_type: str) -> int:\n",
    "    priority_map = {\n",
    "        'schedule': 10,\n",
    "        'budget': 8,\n",
    "        'organization': 6,\n",
    "        'general': 4\n",
    "    }\n",
    "    return priority_map.get(table_type, 4)\n",
    "\n",
    "def create_paragraph_chunks(sections: List[Dict[str, Any]], max_chunk_size: int) -> List[Dict[str, Any]]:\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for section in sections:\n",
    "        content = section.get('content', '')\n",
    "        if not content.strip():\n",
    "            continue\n",
    "        \n",
    "        estimated_tokens = len(content) * 0.7\n",
    "        \n",
    "        if current_size + estimated_tokens <= max_chunk_size:\n",
    "            current_chunk.append(content)\n",
    "            current_size += estimated_tokens\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(create_paragraph_chunk(current_chunk))\n",
    "            \n",
    "            if estimated_tokens > max_chunk_size:\n",
    "                split_chunks = split_large_paragraph(content, max_chunk_size)\n",
    "                chunks.extend(split_chunks)\n",
    "                current_chunk = []\n",
    "                current_size = 0\n",
    "            else:\n",
    "                current_chunk = [content]\n",
    "                current_size = estimated_tokens\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(create_paragraph_chunk(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def create_paragraph_chunk(content_list: List[str]) -> Dict[str, Any]:\n",
    "    combined_content = ' '.join(content_list)\n",
    "    paragraph_type = identify_paragraph_type(combined_content)\n",
    "    \n",
    "    chunk = {\n",
    "        'type': 'paragraph',\n",
    "        'subtype': paragraph_type,\n",
    "        'content': combined_content,\n",
    "        'paragraph_count': len(content_list),\n",
    "        'metadata': {\n",
    "            'source': 'pdf_paragraph_extraction',\n",
    "            'priority': get_paragraph_priority(paragraph_type)\n",
    "        }\n",
    "    }\n",
    "    return chunk\n",
    "\n",
    "def identify_paragraph_type(content: str) -> str:\n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    if any(keyword in content_lower for keyword in ['제', '장', '절', '항']):\n",
    "        return 'header'\n",
    "    if any(keyword in content_lower for keyword in ['목적', '개요', '배경']):\n",
    "        return 'overview'\n",
    "    if any(keyword in content_lower for keyword in ['결론', '요약', '종합']):\n",
    "        return 'conclusion'\n",
    "    if any(keyword in content_lower for keyword in ['방법', '절차', '과정']):\n",
    "        return 'methodology'\n",
    "    \n",
    "    return 'general'\n",
    "\n",
    "def get_paragraph_priority(paragraph_type: str) -> int:\n",
    "    priority_map = {\n",
    "        'overview': 9,\n",
    "        'conclusion': 8,\n",
    "        'methodology': 7,\n",
    "        'header': 6,\n",
    "        'general': 5\n",
    "    }\n",
    "    return priority_map.get(paragraph_type, 5)\n",
    "\n",
    "def split_large_paragraph(content: str, max_chunk_size: int) -> List[Dict[str, Any]]:\n",
    "    chunks = []\n",
    "    sentences = re.split(r'[.!?]\\s+', content)\n",
    "    \n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        estimated_tokens = len(sentence) * 0.7\n",
    "        \n",
    "        if current_size + estimated_tokens <= max_chunk_size:\n",
    "            current_chunk.append(sentence)\n",
    "            current_size += estimated_tokens\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunk_content = '. '.join(current_chunk) + '.'\n",
    "                chunks.append({\n",
    "                    'type': 'paragraph',\n",
    "                    'subtype': 'split_general',\n",
    "                    'content': chunk_content,\n",
    "                    'paragraph_count': 1,\n",
    "                    'metadata': {\n",
    "                        'source': 'paragraph_split',\n",
    "                        'priority': 5\n",
    "                    }\n",
    "                })\n",
    "            current_chunk = [sentence]\n",
    "            current_size = estimated_tokens\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunk_content = '. '.join(current_chunk) + '.'\n",
    "        chunks.append({\n",
    "            'type': 'paragraph',\n",
    "            'subtype': 'split_general',\n",
    "            'content': chunk_content,\n",
    "            'paragraph_count': 1,\n",
    "            'metadata': {\n",
    "                'source': 'paragraph_split',\n",
    "                'priority': 5\n",
    "            }\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "def analyze_chunks(chunks: List[Dict[str, Any]]) -> None:\n",
    "    print(\"\\n=== 청크 분석 결과 ===\")\n",
    "    \n",
    "    type_counts = {}\n",
    "    subtype_counts = {}\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        chunk_type = chunk['type']\n",
    "        chunk_subtype = chunk['subtype']\n",
    "        \n",
    "        type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1\n",
    "        subtype_counts[chunk_subtype] = subtype_counts.get(chunk_subtype, 0) + 1\n",
    "    \n",
    "    print(\"타입별 분포:\")\n",
    "    for chunk_type, count in type_counts.items():\n",
    "        print(f\"  {chunk_type}: {count}\")\n",
    "    \n",
    "    print(\"\\n세부 타입별 분포:\")\n",
    "    for subtype, count in subtype_counts.items():\n",
    "        print(f\"  {subtype}: {count}\")\n",
    "    \n",
    "    priority_counts = {}\n",
    "    for chunk in chunks:\n",
    "        priority = chunk['metadata']['priority']\n",
    "        priority_counts[priority] = priority_counts.get(priority, 0) + 1\n",
    "    \n",
    "    print(\"\\n우선순위별 분포:\")\n",
    "    for priority in sorted(priority_counts.keys(), reverse=True):\n",
    "        print(f\"  우선순위 {priority}: {priority_counts[priority]}개\")\n",
    "\n",
    "def main_pdf_processing():\n",
    "    pdf_file_path = '../data/raw/files/대전대학교_대전대학교 2024학년도 다층적 융합 학습경험 플랫폼(MILE) 전.pdf'\n",
    "    \n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        print(f\"파일을 찾을 수 없습니다: {pdf_file_path}\")\n",
    "        return\n",
    "        \n",
    "    parsed_pdf_data = extract_text_and_tables(pdf_file_path)\n",
    "    pdf_chunks = create_structured_chunks(parsed_pdf_data)\n",
    "    \n",
    "    analyze_chunks(pdf_chunks)\n",
    "    \n",
    "    print(\"\\n=== 샘플 청크 ===\")\n",
    "    table_chunks = [c for c in pdf_chunks if c.get('type') == 'table']\n",
    "    if table_chunks:\n",
    "        print(\"첫 번째 표 청크:\")\n",
    "        sample_table = table_chunks[0]\n",
    "        print(f\"  타입: {sample_table['type']}-{sample_table['subtype']}\")\n",
    "        print(f\"  내용: {sample_table['content'][:200]}...\")\n",
    "        print(f\"  우선순위: {sample_table['metadata']['priority']}\")\n",
    "    \n",
    "    paragraph_chunks = [c for c in pdf_chunks if c['type'] == 'paragraph']\n",
    "    if paragraph_chunks:\n",
    "        print(\"\\n첫 번째 문단 청크:\")\n",
    "        sample_paragraph = paragraph_chunks[0]\n",
    "        print(f\"  타입: {sample_paragraph['type']}-{sample_paragraph['subtype']}\")\n",
    "        print(f\"  내용: {sample_paragraph['content'][:200]}...\")\n",
    "        print(f\"  문단 수: {sample_paragraph['paragraph_count']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_pdf_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "153da80a-3090-45b8-96aa-fbe17b357ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 총 5개 PDF 파일 일괄 처리 시작 ---\n",
      "\n",
      "[1/5] 파일 처리 중: 기초과학연구원_2025년도 중이온가속기용 극저온시스템 운전 용역.pdf\n",
      "PDF 파일 파싱 시작: ../data/raw/files/기초과학연구원_2025년도 중이온가속기용 극저온시스템 운전 용역.pdf\n",
      "총 49 페이지 발견\n",
      "파싱 완료 - 문단: 49, 표: 52\n",
      "청킹 작업 시작...\n",
      "청킹 완료 - 총 192개 청크 생성\n",
      "✅ 기초과학연구원_2025년도 중이온가속기용 극저온시스템 운전 용역.pdf에서 192개 청크 생성 완료.\n",
      "\n",
      "[2/5] 파일 처리 중: 고려대학교_차세대 포털·학사 정보시스템 구축사업.pdf\n",
      "PDF 파일 파싱 시작: ../data/raw/files/고려대학교_차세대 포털·학사 정보시스템 구축사업.pdf\n",
      "총 297 페이지 발견\n",
      "파싱 완료 - 문단: 297, 표: 377\n",
      "청킹 작업 시작...\n",
      "청킹 완료 - 총 808개 청크 생성\n",
      "✅ 고려대학교_차세대 포털·학사 정보시스템 구축사업.pdf에서 808개 청크 생성 완료.\n",
      "\n",
      "[3/5] 파일 처리 중: 서울특별시_2024년 지도정보 플랫폼 및 전문활용 연계 시스템 고도화 용.pdf\n",
      "PDF 파일 파싱 시작: ../data/raw/files/서울특별시_2024년 지도정보 플랫폼 및 전문활용 연계 시스템 고도화 용.pdf\n",
      "총 75 페이지 발견\n",
      "파싱 완료 - 문단: 75, 표: 151\n",
      "청킹 작업 시작...\n",
      "청킹 완료 - 총 405개 청크 생성\n",
      "✅ 서울특별시_2024년 지도정보 플랫폼 및 전문활용 연계 시스템 고도화 용.pdf에서 405개 청크 생성 완료.\n",
      "\n",
      "[4/5] 파일 처리 중: 서울시립대학교_[사전공개] 학업성취도 다차원 종단분석 통합시스템 1차.pdf\n",
      "PDF 파일 파싱 시작: ../data/raw/files/서울시립대학교_[사전공개] 학업성취도 다차원 종단분석 통합시스템 1차.pdf\n",
      "총 149 페이지 발견\n",
      "파싱 완료 - 문단: 149, 표: 163\n",
      "청킹 작업 시작...\n",
      "청킹 완료 - 총 563개 청크 생성\n",
      "✅ 서울시립대학교_[사전공개] 학업성취도 다차원 종단분석 통합시스템 1차.pdf에서 563개 청크 생성 완료.\n",
      "\n",
      "[5/5] 파일 처리 중: 대전대학교_대전대학교 2024학년도 다층적 융합 학습경험 플랫폼(MILE) 전.pdf\n",
      "PDF 파일 파싱 시작: ../data/raw/files/대전대학교_대전대학교 2024학년도 다층적 융합 학습경험 플랫폼(MILE) 전.pdf\n",
      "총 75 페이지 발견\n",
      "파싱 완료 - 문단: 75, 표: 51\n",
      "청킹 작업 시작...\n",
      "청킹 완료 - 총 254개 청크 생성\n",
      "✅ 대전대학교_대전대학교 2024학년도 다층적 융합 학습경험 플랫폼(MILE) 전.pdf에서 254개 청크 생성 완료.\n",
      "✅ 모든 청크 데이터를 ../data/processed/datapreprocessingbjs(pdfplumber)/all_pdf_chunks.json에 성공적으로 저장했습니다.\n",
      "\n",
      "--- 모든 파일 처리 및 JSON 저장 완료 ---\n",
      "총 2222개 청크가 생성되어 ../data/processed/datapreprocessingbjs(pdfplumber)/all_pdf_chunks.json에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 최종 청크 데이터를 JSON 파일로 저장하는 함수\n",
    "def save_chunks_to_json(chunks_data: List[Dict], output_file_path: str):\n",
    "    \"\"\"\n",
    "    모든 청크 데이터를 하나의 JSON 파일로 저장합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"✅ 모든 청크 데이터를 {output_file_path}에 성공적으로 저장했습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ JSON 파일 저장 중 오류가 발생했습니다: {e}\")\n",
    "\n",
    "# 최종 실행 함수: 모든 PDF 파일을 처리하고 JSON으로 저장\n",
    "def process_all_pdfs_and_save_json(input_dir, output_dir, json_file_name=\"all_pdf_chunks.json\"):\n",
    "    \"\"\"\n",
    "    지정된 입력 디렉터리의 모든 PDF 파일을 파싱하고 청킹한 후,\n",
    "    결과를 단일 JSON 파일로 저장합니다.\n",
    "    \"\"\"\n",
    "    # 출력 디렉터리 생성\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"출력 디렉터리가 없어 생성했습니다: {output_dir}\")\n",
    "\n",
    "    all_chunks = []\n",
    "    file_list = [f for f in os.listdir(input_dir) if f.lower().endswith('.pdf')]\n",
    "    total_files = len(file_list)\n",
    "    \n",
    "    print(f\"\\n--- 총 {total_files}개 PDF 파일 일괄 처리 시작 ---\")\n",
    "\n",
    "    for i, file_name in enumerate(file_list):\n",
    "        input_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        print(f\"\\n[{i+1}/{total_files}] 파일 처리 중: {file_name}\")\n",
    "        \n",
    "        # PDF 파일 파싱 (오류 방지 로직 적용)\n",
    "        parsed_data = extract_text_and_tables(input_path)\n",
    "        \n",
    "        # 파싱된 데이터를 청크로 변환\n",
    "        if parsed_data:\n",
    "            chunks_for_file = create_structured_chunks(parsed_data)\n",
    "            \n",
    "            # 각 청크에 파일명 메타데이터 추가\n",
    "            for chunk in chunks_for_file:\n",
    "                chunk['metadata']['source_file'] = file_name\n",
    "            \n",
    "            all_chunks.extend(chunks_for_file)\n",
    "            print(f\"✅ {file_name}에서 {len(chunks_for_file)}개 청크 생성 완료.\")\n",
    "    \n",
    "    final_output_path = os.path.join(output_dir, json_file_name)\n",
    "    save_chunks_to_json(all_chunks, final_output_path)\n",
    "    print(f\"\\n--- 모든 파일 처리 및 JSON 저장 완료 ---\")\n",
    "    print(f\"총 {len(all_chunks)}개 청크가 생성되어 {final_output_path}에 저장되었습니다.\")\n",
    "\n",
    "# --- 실행 예시 ---\n",
    "input_folder = '../data/raw/files'\n",
    "output_folder = '../data/processed/datapreprocessingbjs(pdfplumber)'\n",
    "\n",
    "process_all_pdfs_and_save_json(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cfbe713b-b863-494e-818d-bb766718c999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ../data/processed/datapreprocessingbjs(pdfplumber)/all_pdf_chunks.json에서 2222개의 청크를 성공적으로 불러왔습니다.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_chunks_from_json(file_path):\n",
    "    \"\"\"JSON 파일에서 청크 데이터를 불러옵니다.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"✅ {file_path}에서 {len(data)}개의 청크를 성공적으로 불러왔습니다.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 오류: 파일을 찾을 수 없습니다: {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"❌ 오류: JSON 디코딩에 실패했습니다. 파일 형식을 확인해주세요.\")\n",
    "        return None\n",
    "\n",
    "# JSON 파일 경로 설정 및 데이터 로드\n",
    "json_file_path = '../data/processed/datapreprocessingbjs(pdfplumber)/all_pdf_chunks.json'\n",
    "chunks = load_chunks_from_json(json_file_path)\n",
    "\n",
    "if chunks is None:\n",
    "    # 데이터 로드 실패 시, 스크립트 종료\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f64e1ad9-e007-44a7-8f18-c5ee6d999b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 임베딩 모델 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'을 성공적으로 로드했습니다.\n",
      "✨ 새로운 컬렉션 'pdf_rag_collection_snunlp'을 생성했습니다.\n"
     ]
    }
   ],
   "source": [
    "# chunks 리스트의 각 청크를 순회하며 청크 ID 고유화\n",
    "for chunk in chunks:\n",
    "    # 메타데이터에서 원본 파일 이름 가져오기, 없으면 \"unknown\" 사용\n",
    "    file_name = chunk['metadata'].get(\"source_file\", \"unknown\")\n",
    "    \n",
    "    # 기존 chunk_id에 파일 이름을 붙여서 고유 ID 생성\n",
    "    chunk['chunk_id'] = f\"{file_name}_{chunk['chunk_id']}\"\n",
    "\n",
    "# ChromaDB 클라이언트 초기화\n",
    "client = chromadb.Client()\n",
    "\n",
    "# PersistentClient 적용\n",
    "db_path = \"../data/processed/datapreprocessingbjs(pdfplumber)/chromaDB\"  # 🔹 DB 저장 경로\n",
    "client = chromadb.PersistentClient(path=db_path)  # 🔹 메모리 대신 디스크 기반 DB 사용\n",
    "\n",
    "# 임베딩 모델 지정\n",
    "model_name = \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\"\n",
    "try:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(f\"✅ 임베딩 모델 '{model_name}'을 성공적으로 로드했습니다.\")\n",
    "except OSError as e:\n",
    "    print(f\"❌ 모델 로드 중 오류 발생: {e}\")\n",
    "    exit()\n",
    "\n",
    "def get_or_create_collection(collection_name):\n",
    "    \"\"\"컬렉션을 가져오거나 새로 생성합니다.\"\"\"\n",
    "    try:\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "        print(f\"✅ 기존 컬렉션 '{collection_name}'을 불러왔습니다.\")\n",
    "        return collection\n",
    "    except Exception:\n",
    "        collection = client.create_collection(name=collection_name)\n",
    "        print(f\"✨ 새로운 컬렉션 '{collection_name}'을 생성했습니다.\")\n",
    "        return collection\n",
    "\n",
    "# ChromaDB 컬렉션 설정\n",
    "collection_name = \"pdf_rag_collection_snunlp\"\n",
    "collection = get_or_create_collection(collection_name)\n",
    "\n",
    "# 기존 데이터가 있을 경우 삭제 (테스트용)\n",
    "if collection.count() > 0:\n",
    "    print(f\"⚠️ 기존 데이터 {collection.count()}개를 삭제합니다.\")\n",
    "    client.delete_collection(collection_name)\n",
    "    collection = client.create_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "66068fec-7aa6-4786-8587-0fc6b7f5b629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 청크 임베딩 및 ChromaDB 저장 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ChromaDB 저장 중: 100%|██████████| 45/45 [00:18<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 총 2222개의 청크가 ChromaDB에 성공적으로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def embed_and_save_to_chroma(chunks, collection, model):\n",
    "    \"\"\"청크 데이터를 임베딩하여 ChromaDB에 저장합니다.\"\"\"\n",
    "    print(\"🚀 청크 임베딩 및 ChromaDB 저장 시작...\")\n",
    "    \n",
    "    # 데이터를 ChromaDB에 맞게 변환\n",
    "    documents = [chunk['content'] for chunk in chunks]\n",
    "    metadatas = [chunk['metadata'] for chunk in chunks]\n",
    "    ids = [chunk['chunk_id'] for chunk in chunks]\n",
    "    \n",
    "    # 50개씩 일괄 처리\n",
    "    batch_size = 50\n",
    "    total_chunks = len(documents)\n",
    "    \n",
    "    for i in tqdm(range(0, total_chunks, batch_size), desc=\"ChromaDB 저장 중\"):\n",
    "        batch_documents = documents[i:i + batch_size]\n",
    "        batch_metadatas = metadatas[i:i + batch_size]\n",
    "        batch_ids = ids[i:i + batch_size]\n",
    "        \n",
    "        # 모델을 사용하여 임베딩 벡터 생성\n",
    "        embeddings = model.encode(batch_documents, show_progress_bar=False).tolist()\n",
    "        \n",
    "        # 임베딩과 함께 데이터 저장\n",
    "        collection.add(\n",
    "            embeddings=embeddings,\n",
    "            documents=batch_documents,\n",
    "            metadatas=batch_metadatas,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "    \n",
    "    print(f\"✅ 총 {collection.count()}개의 청크가 ChromaDB에 성공적으로 저장되었습니다.\")\n",
    "\n",
    "# 최종 실행 함수 호출\n",
    "embed_and_save_to_chroma(chunks, collection, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1aaacf54-4e21-4398-a3f4-03a6c2ed59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromaDB 컬렉션 로드\n",
    "collection = client.get_collection(name=collection_name)\n",
    "\n",
    "# 임베딩 모델 로드 (저장 시 사용한 모델과 동일해야 함)\n",
    "model = SentenceTransformer(\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "985fe44b-d1f2-4c45-b61d-239e76223aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI LLM 클라이언트 초기화 완료\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI API 키 설정 (또는 다른 LLM API 키)\n",
    "openai_api_key ='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'\n",
    "client_llm = OpenAI(api_key=openai_api_key)\n",
    "print(\"✅ OpenAI LLM 클라이언트 초기화 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "16b20cf0-92ab-4da3-8019-c82bb4510662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_augmented_generation(query: str):\n",
    "    \"\"\"\n",
    "    사용자의 질문에 RAG 방식으로 답변하는 함수입니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자의 질문\n",
    "    \n",
    "    Returns:\n",
    "        str: LLM이 생성한 답변\n",
    "    \"\"\"\n",
    "    print(\"1. 검색(Retrieval) 시작...\")\n",
    "    # 1.1. 질문을 임베딩\n",
    "    query_embedding = model.encode([query]).tolist()\n",
    "    \n",
    "    # 1.2. ChromaDB에서 가장 유사한 청크 검색\n",
    "    # n_results를 늘려 더 많은 문서를 가져올 수 있습니다.\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=10,  # 상위 10개 결과를 가져옴\n",
    "    )\n",
    "    \n",
    "    # 1.3. 검색된 문서(청크) 추출\n",
    "    retrieved_chunks = results['documents'][0]\n",
    "    retrieved_metas = results['metadatas'][0]\n",
    "    print(f\"✅ 검색 완료: {len(retrieved_chunks)}개의 관련 문서 발견\")\n",
    "    \n",
    "    print(\"2. 보강(Augmentation) 시작...\")\n",
    "    # 2.1. 검색된 청크를 프롬프트에 포함\n",
    "    context = \"\"\n",
    "    for doc, meta in zip(retrieved_chunks, retrieved_metas):\n",
    "        file_name = meta.get(\"source_file\", \"unknown\")\n",
    "        context += f\"[출처: {file_name}]\\n{doc}\\n\\n\"\n",
    "    \n",
    "    # 2.2. 시스템 프롬프트 구성\n",
    "    system_prompt = f\"\"\"\n",
    "    당신은 주어진 문서를 기반으로 질문에 답변하는 AI 비서입니다.\n",
    "    문서 내용을 요약하고, 반드시 문서에서 제공하는 정보만 바탕으로 답변하세요.\n",
    "    문서에 관련된 내용이 확실히 없는 경우에만 '문서에 관련 정보가 없습니다.'라고 답변하세요.\n",
    "\n",
    "    [검색된 문서]\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"3. 생성(Generation) 시작...\")\n",
    "    # 3.1. LLM에 질문 전달 및 답변 생성\n",
    "    response = client_llm.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",  # 또는 \"gpt-3.5-turbo\" 등\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=700\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e008f3d3-3a5f-4bb4-846b-964c2a8b69f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 검색(Retrieval) 시작...\n",
      "✅ 검색 완료: 10개의 관련 문서 발견\n",
      "2. 보강(Augmentation) 시작...\n",
      "3. 생성(Generation) 시작...\n",
      "\n",
      "--- 질문 ---\n",
      "대전대학교의 다층적 융합 학습경험 플랫폼에 대해 자세히 알려줘.\n",
      "\n",
      "--- 답변 ---\n",
      "대전대학교의 다층적 융합 학습경험 플랫폼(MILE)은 융합 교육 혁신과 학생 맞춤형 학습 지원을 목표로 구축되는 시스템입니다. 주요 내용은 다음과 같습니다.\n",
      "\n",
      "1. 사업 목표  \n",
      "- 융합 교육 학습 성과를 실시간으로 추적하고 피드백을 강화  \n",
      "- 경계 없는 학생 맞춤형 학습경로 제공을 통한 융합 교육 확산  \n",
      "- 유연한 학사 운영과 융합 교육 프로그램 지원  \n",
      "\n",
      "2. 시스템 및 사업 범위  \n",
      "- ‘나노 마이크로디그리 전산시스템’과 ‘적응형 교수·학습 시스템’과 연계  \n",
      "- 교수 및 융합교육 담당 코디네이터와 학생이 활용할 수 있는 융합 교육 모니터링 시스템 신규 구축  \n",
      "- 나노·마이크로디그리 출력 화면 조정 및 교수 융합교육 역량 진단 기능 포함  \n",
      "- 학생 중심 교육 기반인 ‘ACT 교과인증 확대’를 대비한 학사 강좌개설 연계 보완 및 추가 개선  \n",
      "\n",
      "3. 필요성 및 운영 방향  \n",
      "- 학생들이 자율적으로 학습 경로를 선택할 수 있도록 개방적 융합 교육 운영과 성과 관리 체계 마련  \n",
      "- 무경계 교육과정 참여 학생 지원 및 융합 교육 활성화를 위한 교수-학생 활용 플랫폼 구축  \n",
      "- 전주기적 학생 지원을 위한 제반 환경 조성  \n",
      "\n",
      "요약하면, MILE 플랫폼은 대전대학교가 융합 교육을 체계적이고 실시간으로 지원하며 학생 개개인 맞춤형 학습을 가능하게 하는 다층적 통합 학습관리 시스템으로, 기존의 나노·마이크로디그리 시스템과 적응형 학습 시스템을 연계 보완하여 융합 교육의 혁신과 확산을 도모하는 사업입니다.\n"
     ]
    }
   ],
   "source": [
    "# --- 사용 예시 ---\n",
    "user_query = \"대전대학교의 다층적 융합 학습경험 플랫폼에 대해 자세히 알려줘.\"\n",
    "answer = retrieval_augmented_generation(user_query)\n",
    "\n",
    "print(\"\\n--- 질문 ---\")\n",
    "print(user_query)\n",
    "print(\"\\n--- 답변 ---\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "788ef98e-645b-4c04-a65a-f2e7b4b0dcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4901.6904296875 KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_path = \"../data/processed/datapreprocessingbjs(pdfplumber)/all_pdf_chunks.json\"\n",
    "print(os.path.getsize(file_path) / 1024, \"KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "447f0c48-fe4d-47cd-af04-b86d8d330235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "\n",
      "    {\n",
      "\n",
      "        \"type\": \"table\",\n",
      "\n",
      "        \"subtype\": \"general\",\n",
      "\n",
      "        \"content\": \"문서번호: 개정번호, -: 0. 문서번호: 발 행 일, -: 2024. 10. 30\",\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(5):  # 처음 5줄만\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5a9b1361-01b8-4d89-8eb1-4a66d4ccc9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 개수: 2222\n",
      "data 타입: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"총 개수:\", len(data))\n",
    "print(\"data 타입:\", type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "557e4910-f743-4cc3-8ac0-a03218914046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 항목: dict_keys(['type', 'subtype', 'content', 'original_rows', 'metadata', 'chunk_id'])\n"
     ]
    }
   ],
   "source": [
    "if isinstance(data, list):\n",
    "    print(\"첫 번째 항목:\", data[0].keys())\n",
    "elif isinstance(data, dict):\n",
    "    print(\"keys:\", list(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "816afd37-ceb1-46ee-89dd-7d6bd1953720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Chunk 1 ===\n",
      "type: table\n",
      "subtype: general\n",
      "content: 문서번호: 개정번호, -: 0. 문서번호: 발 행 일, -: 2024. 10. 30\n",
      "original_rows: 3\n",
      "metadata: {'table_id': 'table_1_0', 'source': 'pdf_table_extraction', 'priority': 4, 'source_file': '기초과학연구원_2025년도 중이온가속기용 극저온시스템 운전 용역.pdf'}\n",
      "chunk_id: chunk_0000\n",
      "\n",
      "=== Chunk 2 ===\n",
      "type: table\n",
      "subtype: general\n",
      "content: 문서상태 식별 (다음 중 하나에 체크): □ 문서상태-1 : 최종설계, 해석, 구매 등에 이용되는 설계 확인된 정보. 문서상태 식별 (다음 중 하나에 체크): □ 문서상태-2 : ...\n",
      "original_rows: 4\n",
      "metadata: {'table_id': 'table_1_1', 'source': 'pdf_table_extraction', 'priority': 4, 'source_file': '기초과학연구원_2025년도 중이온가속기용 극저온시스템 운전 용역.pdf'}\n",
      "chunk_id: chunk_0001\n",
      "\n",
      "=== Chunk 3 ===\n",
      "type: table\n",
      "subtype: general\n",
      "content: : 작 성, 소속 및 직위: 극저온팀/연구위원, 성 명: 유정현. : 검 토, 소속 및 직위: 품질관리실/연구위원, 성 명: 신일경. 소속 및 직위: 극저온팀/팀장, 성 명: 신재...\n",
      "original_rows: 5\n",
      "metadata: {'table_id': 'table_1_2', 'source': 'pdf_table_extraction', 'priority': 4, 'source_file': '기초과학연구원_2025년도 중이온가속기용 극저온시스템 운전 용역.pdf'}\n",
      "chunk_id: chunk_0002\n"
     ]
    }
   ],
   "source": [
    "# 첫 3개 확인\n",
    "for i, chunk in enumerate(data[:3], 1):\n",
    "    print(f\"\\n=== Chunk {i} ===\")\n",
    "    for k, v in chunk.items():\n",
    "        if isinstance(v, str) and len(v) > 100:\n",
    "            print(f\"{k}: {v[:100]}...\")  # 너무 긴 문자열은 앞부분만\n",
    "        else:\n",
    "            print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7273f950-28a4-4326-9e96-6ef18ec94906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'my_chroma_db' 폴더와 모든 내용이 성공적으로 삭제되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# 삭제할 폴더 경로를 지정하세요.\n",
    "folder_path = 'my_chroma_db'\n",
    "try:\n",
    "    shutil.rmtree(folder_path)\n",
    "    print(f\"'{folder_path}' 폴더와 모든 내용이 성공적으로 삭제되었습니다.\")\n",
    "except OSError as e:\n",
    "    print(f\"오류: {e.strerror} - {e.filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a5168-7e56-40eb-9e53-27cbbbd84ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
