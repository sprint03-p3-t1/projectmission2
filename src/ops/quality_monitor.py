"""
RFP RAG ì‹œìŠ¤í…œ - í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
ì‹¤ì‹œê°„ í’ˆì§ˆ ì¶”ì  ë° ì•Œë¦¼ ê¸°ëŠ¥
"""

import time
import logging
from typing import Dict, List, Any, Optional, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass
from threading import Thread, Event
import json

from .quality_metrics import QualityMetrics

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class QualityAlert:
    """í’ˆì§ˆ ì•Œë¦¼ ë°ì´í„° í´ë˜ìŠ¤"""
    alert_type: str
    message: str
    severity: str  # 'low', 'medium', 'high', 'critical'
    timestamp: str
    metrics: Dict[str, Any]
    recommendations: List[str]

class QualityMonitor:
    """í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, 
                 quality_metrics: QualityMetrics = None,
                 alert_thresholds: Dict[str, float] = None,
                 monitoring_interval: int = 300):  # 5ë¶„ë§ˆë‹¤ ëª¨ë‹ˆí„°ë§
        
        self.quality_metrics = quality_metrics or QualityMetrics()
        self.monitoring_interval = monitoring_interval
        self.is_monitoring = False
        self.monitor_thread = None
        self.stop_event = Event()
        
        # ì•Œë¦¼ ì„ê³„ê°’ ì„¤ì •
        self.alert_thresholds = alert_thresholds or {
            "overall_score_low": 0.6,
            "overall_score_critical": 0.4,
            "accuracy_score_low": 0.5,
            "relevance_score_low": 0.5,
            "completeness_score_low": 0.5,
            "clarity_score_low": 0.5,
            "structure_score_low": 0.5,
            "trend_decline_days": 3,
            "consecutive_low_scores": 5
        }
        
        # ì•Œë¦¼ ì½œë°± í•¨ìˆ˜ë“¤
        self.alert_callbacks: List[Callable[[QualityAlert], None]] = []
        
        # ëª¨ë‹ˆí„°ë§ íˆìŠ¤í† ë¦¬
        self.monitoring_history: List[Dict[str, Any]] = []
        
        logger.info("Quality monitor initialized")
    
    def add_alert_callback(self, callback: Callable[[QualityAlert], None]):
        """ì•Œë¦¼ ì½œë°± í•¨ìˆ˜ ì¶”ê°€"""
        self.alert_callbacks.append(callback)
        logger.info(f"Alert callback added: {callback.__name__}")
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.is_monitoring:
            logger.warning("Monitoring is already running")
            return
        
        self.is_monitoring = True
        self.stop_event.clear()
        self.monitor_thread = Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()
        
        logger.info("Quality monitoring started")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        if not self.is_monitoring:
            logger.warning("Monitoring is not running")
            return
        
        self.is_monitoring = False
        self.stop_event.set()
        
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        
        logger.info("Quality monitoring stopped")
    
    def _monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while not self.stop_event.is_set():
            try:
                self._check_quality_metrics()
                self._check_quality_trends()
                self._check_consecutive_low_scores()
                
                # ëª¨ë‹ˆí„°ë§ ê²°ê³¼ ì €ì¥
                self._save_monitoring_result()
                
            except Exception as e:
                logger.error(f"Error in monitoring loop: {e}")
            
            # ë‹¤ìŒ ëª¨ë‹ˆí„°ë§ê¹Œì§€ ëŒ€ê¸°
            self.stop_event.wait(self.monitoring_interval)
    
    def _check_quality_metrics(self):
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ìŠ¤ í™•ì¸"""
        try:
            # ìµœê·¼ 1ì‹œê°„ í†µê³„ ì¡°íšŒ
            stats = self.quality_metrics.get_quality_statistics(days=1)
            
            if stats["total_evaluations"] == 0:
                return
            
            # ì „ì²´ ì ìˆ˜ í™•ì¸
            overall_score = stats["avg_overall_score"]
            if overall_score < self.alert_thresholds["overall_score_critical"]:
                self._trigger_alert(
                    alert_type="overall_score_critical",
                    message=f"ì „ì²´ í’ˆì§ˆ ì ìˆ˜ê°€ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤: {overall_score:.3f}",
                    severity="critical",
                    metrics=stats,
                    recommendations=[
                        "ì‹œìŠ¤í…œ ì ê²€ í•„ìš”",
                        "í”„ë¡¬í”„íŠ¸ ê°œì„  ê²€í† ",
                        "ëª¨ë¸ ì„±ëŠ¥ í™•ì¸"
                    ]
                )
            elif overall_score < self.alert_thresholds["overall_score_low"]:
                self._trigger_alert(
                    alert_type="overall_score_low",
                    message=f"ì „ì²´ í’ˆì§ˆ ì ìˆ˜ê°€ ë‚®ìŠµë‹ˆë‹¤: {overall_score:.3f}",
                    severity="high",
                    metrics=stats,
                    recommendations=[
                        "í’ˆì§ˆ ê°œì„  ë°©ì•ˆ ê²€í† ",
                        "ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘"
                    ]
                )
            
            # ê°œë³„ ì§€í‘œ í™•ì¸
            for metric in ["accuracy", "relevance", "completeness", "clarity", "structure"]:
                score_key = f"avg_{metric}_score"
                if score_key in stats:
                    score = stats[score_key]
                    threshold_key = f"{metric}_score_low"
                    if threshold_key in self.alert_thresholds and score < self.alert_thresholds[threshold_key]:
                        self._trigger_alert(
                            alert_type=f"{metric}_score_low",
                            message=f"{metric} ì ìˆ˜ê°€ ë‚®ìŠµë‹ˆë‹¤: {score:.3f}",
                            severity="medium",
                            metrics=stats,
                            recommendations=[
                                f"{metric} ê°œì„  ë°©ì•ˆ ê²€í† ",
                                "ê´€ë ¨ í”„ë¡¬í”„íŠ¸ ìµœì í™”"
                            ]
                        )
        
        except Exception as e:
            logger.error(f"Error checking quality metrics: {e}")
    
    def _check_quality_trends(self):
        """í’ˆì§ˆ íŠ¸ë Œë“œ í™•ì¸"""
        try:
            # ìµœê·¼ 7ì¼ íŠ¸ë Œë“œ ì¡°íšŒ
            trends_df = self.quality_metrics.get_quality_trends(days=7)
            
            if len(trends_df) < 3:
                return
            
            # ìµœê·¼ 3ì¼ í‰ê·  ì ìˆ˜ ê³„ì‚°
            recent_scores = trends_df['avg_overall_score'].tail(3).values
            if len(recent_scores) >= 3:
                # í•˜í–¥ íŠ¸ë Œë“œ í™•ì¸
                if all(recent_scores[i] > recent_scores[i+1] for i in range(len(recent_scores)-1)):
                    decline_days = len(recent_scores)
                    if decline_days >= self.alert_thresholds["trend_decline_days"]:
                        self._trigger_alert(
                            alert_type="quality_trend_decline",
                            message=f"í’ˆì§ˆì´ {decline_days}ì¼ ì—°ì† í•˜ë½í•˜ê³  ìˆìŠµë‹ˆë‹¤",
                            severity="high",
                            metrics={"recent_scores": recent_scores.tolist()},
                            recommendations=[
                                "í’ˆì§ˆ í•˜ë½ ì›ì¸ ë¶„ì„",
                                "ì‹œìŠ¤í…œ ì„±ëŠ¥ ì ê²€",
                                "ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘"
                            ]
                        )
        
        except Exception as e:
            logger.error(f"Error checking quality trends: {e}")
    
    def _check_consecutive_low_scores(self):
        """ì—°ì† ë‚®ì€ ì ìˆ˜ í™•ì¸"""
        try:
            # ìµœê·¼ í‰ê°€ ê²°ê³¼ ì¡°íšŒ
            recent_evaluations = self.quality_metrics.get_recent_evaluations(limit=20)
            
            if len(recent_evaluations) == 0:
                return
            
            # ì—°ì† ë‚®ì€ ì ìˆ˜ í™•ì¸
            low_score_threshold = self.alert_thresholds["overall_score_low"]
            consecutive_low_count = 0
            
            for _, row in recent_evaluations.iterrows():
                if row['overall_score'] < low_score_threshold:
                    consecutive_low_count += 1
                else:
                    break
            
            if consecutive_low_count >= self.alert_thresholds["consecutive_low_scores"]:
                self._trigger_alert(
                    alert_type="consecutive_low_scores",
                    message=f"ì—°ì† {consecutive_low_count}ê°œì˜ ë‚®ì€ í’ˆì§ˆ ì ìˆ˜ ë°œìƒ",
                    severity="high",
                    metrics={"consecutive_low_count": consecutive_low_count},
                    recommendations=[
                        "ì¦‰ì‹œ ì‹œìŠ¤í…œ ì ê²€",
                        "í”„ë¡¬í”„íŠ¸ ê°œì„ ",
                        "ëª¨ë¸ ì„±ëŠ¥ í™•ì¸"
                    ]
                )
        
        except Exception as e:
            logger.error(f"Error checking consecutive low scores: {e}")
    
    def _trigger_alert(self, 
                      alert_type: str, 
                      message: str, 
                      severity: str,
                      metrics: Dict[str, Any],
                      recommendations: List[str]):
        """ì•Œë¦¼ íŠ¸ë¦¬ê±°"""
        
        alert = QualityAlert(
            alert_type=alert_type,
            message=message,
            severity=severity,
            timestamp=datetime.now().isoformat(),
            metrics=metrics,
            recommendations=recommendations
        )
        
        # ì•Œë¦¼ ì½œë°± í•¨ìˆ˜ë“¤ ì‹¤í–‰
        for callback in self.alert_callbacks:
            try:
                callback(alert)
            except Exception as e:
                logger.error(f"Error in alert callback {callback.__name__}: {e}")
        
        logger.warning(f"Quality alert triggered: {alert_type} - {message}")
    
    def _save_monitoring_result(self):
        """ëª¨ë‹ˆí„°ë§ ê²°ê³¼ ì €ì¥"""
        try:
            stats = self.quality_metrics.get_quality_statistics(days=1)
            
            monitoring_result = {
                "timestamp": datetime.now().isoformat(),
                "total_evaluations": stats["total_evaluations"],
                "avg_overall_score": stats["avg_overall_score"],
                "quality_distribution": stats["quality_distribution"],
                "alert_count": len([h for h in self.monitoring_history if h.get("has_alert", False)])
            }
            
            self.monitoring_history.append(monitoring_result)
            
            # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ (ìµœê·¼ 100ê°œë§Œ ìœ ì§€)
            if len(self.monitoring_history) > 100:
                self.monitoring_history = self.monitoring_history[-100:]
        
        except Exception as e:
            logger.error(f"Error saving monitoring result: {e}")
    
    def get_monitoring_status(self) -> Dict[str, Any]:
        """ëª¨ë‹ˆí„°ë§ ìƒíƒœ ì¡°íšŒ"""
        try:
            stats = self.quality_metrics.get_quality_statistics(days=1)
            
            return {
                "is_monitoring": self.is_monitoring,
                "monitoring_interval": self.monitoring_interval,
                "last_check": datetime.now().isoformat(),
                "current_metrics": stats,
                "alert_thresholds": self.alert_thresholds,
                "monitoring_history_count": len(self.monitoring_history),
                "alert_callbacks_count": len(self.alert_callbacks)
            }
        
        except Exception as e:
            logger.error(f"Error getting monitoring status: {e}")
            return {
                "is_monitoring": self.is_monitoring,
                "error": str(e)
            }
    
    def get_quality_insights(self) -> Dict[str, Any]:
        """í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            # ìµœê·¼ 7ì¼ í†µê³„
            stats_7d = self.quality_metrics.get_quality_statistics(days=7)
            stats_1d = self.quality_metrics.get_quality_statistics(days=1)
            
            # íŠ¸ë Œë“œ ë¶„ì„
            trends_df = self.quality_metrics.get_quality_trends(days=7)
            
            # ê°œì„  ì œì•ˆ
            improvement_suggestions = self.quality_metrics.get_improvement_suggestions(days=7)
            
            # í’ˆì§ˆ ë¶„í¬ ë¶„ì„
            quality_dist = stats_7d["quality_distribution"]
            total = sum(quality_dist.values())
            if total > 0:
                high_quality_ratio = quality_dist["high_quality"] / total
                low_quality_ratio = quality_dist["low_quality"] / total
            else:
                high_quality_ratio = 0
                low_quality_ratio = 0
            
            return {
                "overall_quality": {
                    "7day_avg": stats_7d["avg_overall_score"],
                    "1day_avg": stats_1d["avg_overall_score"],
                    "trend": "improving" if stats_1d["avg_overall_score"] > stats_7d["avg_overall_score"] else "declining"
                },
                "quality_distribution": {
                    "high_quality_ratio": round(high_quality_ratio, 3),
                    "low_quality_ratio": round(low_quality_ratio, 3),
                    "total_evaluations": stats_7d["total_evaluations"]
                },
                "improvement_suggestions": improvement_suggestions,
                "trend_data": trends_df.to_dict('records') if not trends_df.empty else [],
                "insights": self._generate_insights(stats_7d, stats_1d, trends_df)
            }
        
        except Exception as e:
            logger.error(f"Error generating quality insights: {e}")
            return {"error": str(e)}
    
    def _generate_insights(self, stats_7d: Dict, stats_1d: Dict, trends_df) -> List[str]:
        """ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ì „ì²´ í’ˆì§ˆ íŠ¸ë Œë“œ
        if stats_1d["avg_overall_score"] > stats_7d["avg_overall_score"]:
            insights.append("ìµœê·¼ í’ˆì§ˆì´ ê°œì„ ë˜ê³  ìˆìŠµë‹ˆë‹¤")
        elif stats_1d["avg_overall_score"] < stats_7d["avg_overall_score"]:
            insights.append("ìµœê·¼ í’ˆì§ˆì´ í•˜ë½í•˜ê³  ìˆìŠµë‹ˆë‹¤")
        
        # ê°œë³„ ì§€í‘œ ë¶„ì„
        for metric in ["relevance", "completeness", "accuracy", "clarity", "structure"]:
            score_key = f"avg_{metric}_score"
            if score_key in stats_7d and stats_7d[score_key] < 0.7:
                insights.append(f"{metric} ì ìˆ˜ê°€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        # í’ˆì§ˆ ë¶„í¬ ë¶„ì„
        quality_dist = stats_7d["quality_distribution"]
        total = sum(quality_dist.values())
        if total > 0:
            high_ratio = quality_dist["high_quality"] / total
            if high_ratio < 0.3:
                insights.append("ê³ í’ˆì§ˆ ë‹µë³€ ë¹„ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤")
            elif high_ratio > 0.7:
                insights.append("ê³ í’ˆì§ˆ ë‹µë³€ ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤")
        
        return insights

# ê¸°ë³¸ ì•Œë¦¼ ì½œë°± í•¨ìˆ˜ë“¤
def console_alert_callback(alert: QualityAlert):
    """ì½˜ì†” ì•Œë¦¼ ì½œë°±"""
    severity_emoji = {
        "low": "â„¹ï¸",
        "medium": "âš ï¸", 
        "high": "ğŸš¨",
        "critical": "ğŸ”¥"
    }
    
    emoji = severity_emoji.get(alert.severity, "â“")
    print(f"{emoji} [QUALITY ALERT] {alert.alert_type}: {alert.message}")
    print(f"   Recommendations: {', '.join(alert.recommendations)}")

def file_alert_callback(alert: QualityAlert, log_file: str = "quality_alerts.log"):
    """íŒŒì¼ ì•Œë¦¼ ì½œë°±"""
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(f"{alert.timestamp} [{alert.severity.upper()}] {alert.alert_type}: {alert.message}\n")
        f.write(f"  Recommendations: {', '.join(alert.recommendations)}\n")
        f.write(f"  Metrics: {json.dumps(alert.metrics, ensure_ascii=False)}\n\n")

# í’ˆì§ˆ ëª¨ë‹ˆí„° ì‹±ê¸€í†¤
_quality_monitor_instance = None

def get_quality_monitor() -> QualityMonitor:
    """í’ˆì§ˆ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _quality_monitor_instance
    if _quality_monitor_instance is None:
        _quality_monitor_instance = QualityMonitor()
        # ê¸°ë³¸ ì½œë°± ì¶”ê°€
        _quality_monitor_instance.add_alert_callback(console_alert_callback)
        _quality_monitor_instance.add_alert_callback(file_alert_callback)
    return _quality_monitor_instance
