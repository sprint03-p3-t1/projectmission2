import os
import re
import json
import pickle
import logging
from typing import List, Dict, Set, Tuple

from tqdm import tqdm
from tqdm.asyncio import tqdm_asyncio
from langchain_chroma import Chroma
from langchain.schema import Document
from rank_bm25 import BM25Okapi 
from more_itertools import chunked

import aiofiles

# ë¡œì»¬ ì„í¬íŠ¸
from ..utils.exceptions import RetrieverError, ChunkLoadingError
from ..utils.filtering import extract_filters, check_filter_match, normalize_keywords
from ..utils.scaling import minmax_scale
from .tokenizer_wrapper import TokenizerWrapper
from .rerank import RerankModel

# ë¡œì»¬ ì„í¬íŠ¸
# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')



class Retriever:
    def __init__(self,
                 meta_df=None,
                 embedder=None,
                 reranker: RerankModel =None,
                 tokenizer=None,
                 persist_directory=None,
                 rerank_max_length=512,
                 bm25_weight=0.5,
                 rerank_weight=0.5,
                 bm25_path="bm25_index.pkl",
                 debug_mode=False
                 ):
        self.meta_df = meta_df
        self.embedder = embedder
        self.reranker = reranker
        self.tokenizer = tokenizer or TokenizerWrapper("kiwi")
        self.persist_directory = persist_directory
        self.rerank_max_length = rerank_max_length

        self.bm25_weight = bm25_weight
        self.rerank_weight = rerank_weight

        self.db = None
        self.bm25 = None
        self.bm25_ready = False
        self.bm25_path = bm25_path
        self.documents = []
        self.debug_mode = debug_mode

        self.last_scores = {}

    def set_weights(self, bm25_weight: float, rerank_weight: float):
        self.bm25_weight = bm25_weight
        self.rerank_weight = rerank_weight
        logging.info(f"ğŸ”§ ê°€ì¤‘ì¹˜ ì„¤ì •ë¨ | BM25: {bm25_weight} | Rerank: {rerank_weight}")

    def get_doc_key(self, doc: Document) -> str:
        chunk_id = doc.metadata.get("chunk_id")
        if chunk_id:
            return chunk_id
        return str(hash(doc.page_content.strip()))

    def save_bm25_index(self):
        os.makedirs(os.path.dirname(self.bm25_path), exist_ok=True)
        with open(self.bm25_path, "wb") as f:
            pickle.dump(self.bm25, f)
        logging.info(f"âœ… BM25 ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {self.bm25_path}")

    def load_bm25_index(self):
        path = self.bm25_path
        if os.path.exists(path):
            try:
                with open(path, "rb") as f:
                    self.bm25 = pickle.load(f)
                self.bm25_ready = True
                logging.info(f"âœ… BM25 ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {path}")
            except Exception as e:
                self.bm25_ready = False
                logging.warning(f"âŒ BM25 ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            self.bm25_ready = False
            logging.warning(f"âŒ BM25 ì¸ë±ìŠ¤ íŒŒì¼ ì—†ìŒ: {path}")

    def deduplicate_documents(self, documents: List[Document]) -> List[Document]:
        seen = set()
        unique_docs = []
        for doc in documents:
            chunk_id = doc.metadata.get("chunk_id")
            if chunk_id and chunk_id not in seen:
                seen.add(chunk_id)
                unique_docs.append(doc)
        removed = len(documents) - len(unique_docs)
        logging.info(f"ğŸ§¹ ì¤‘ë³µ ì œê±°: {removed}ê°œ ì œê±°ë¨")
        return unique_docs

    
    async def load_or_cache_json_docs(self, folder_path: str, cache_path: str) -> List[Document]:
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìë™ ìƒì„±
        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
    
        if os.path.exists(cache_path):
            with open(cache_path, "rb") as f:
                logging.info("ğŸ“¦ ìºì‹œëœ JSON ë¬¸ì„œ ë¡œë“œ ì¤‘...")
                return pickle.load(f)
        else:
            logging.info("ğŸ“‚ JSON í´ë”ì—ì„œ ë¬¸ì„œ ë¡œë”© ì¤‘...")
            docs = await self.async_load_chunks_from_folder(folder_path)
            with open(cache_path, "wb") as f:
                pickle.dump(docs, f)
            logging.info("âœ… JSON ìºì‹œ ì €ì¥ ì™„ë£Œ")
            return docs

    async def async_load_chunks_from_folder(self, folder_path: str) -> List[Document]:
        if not os.path.isdir(folder_path):
            raise ChunkLoadingError(f"âŒ í´ë” ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {folder_path}")

        file_list = sorted([f for f in os.listdir(folder_path) if f.endswith(".json")])
        existing_sources = self.get_existing_chunk_ids()
        logging.info(f"ğŸ“ ê¸°ì¡´ DBì— ì €ì¥ëœ source ìˆ˜: {len(existing_sources)}")

        tasks = []
        for filename in file_list:
            if filename in existing_sources:
                logging.info(f"â© ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ê±´ë„ˆëœ€: {filename}")
                continue
            file_path = os.path.join(folder_path, filename)
            tasks.append(self._load_single_file(file_path, filename))

        # âœ… ê³ ê¸‰ tqdm ì ìš©: ì‹¤ì œ ì™„ë£Œ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
        all_chunks = []
        for coro in tqdm_asyncio.as_completed(tasks, desc="ğŸ“‚ íŒŒì¼ ì²˜ë¦¬ ì¤‘", total=len(tasks)):
            result = await coro
            all_chunks.append(result)

        documents = [doc for sublist in all_chunks for doc in sublist]
        logging.info(f"âœ… ìƒˆë¡œ ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")

        documents = self.deduplicate_documents(documents)
        logging.info(f"ğŸ§¹ ì¤‘ë³µ ì œê±° í›„ ë¬¸ì„œ ìˆ˜: {len(documents)}")
        return documents

    
    async def _load_single_file(self, file_path: str, filename: str) -> list[Document]:
        try:
            async with aiofiles.open(file_path, "r", encoding="utf-8") as f:
                content = await f.read()
                data = json.loads(content)
    
            metadata_base = data.get("csv_metadata", {})
            docs = []
    
            for idx, page in enumerate(data.get("pdf_data", [])):
                page_num = page.get("page", idx)
                text = page.get("text", "").strip()
    
                if text:
                    metadata = metadata_base.copy()
                    metadata["chunk_id"] = f"{filename}::page::{page_num}::type::text"
                    metadata["page"] = page_num
    
                    docs.append(Document(page_content=text, metadata=metadata))
    
            return docs
    
        except Exception as e:
            logging.warning(f"âš ï¸ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {filename} | ì˜¤ë¥˜: {e}")
            return []

    def get_all_documents_from_db(self) -> List[Document]:
        if self.db is None:
            raise ValueError("Vector DBê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        all_docs = []
        collection = self.db._collection
        count = collection.count()
        offset = 0
        limit = 1000

        while offset < count:
            results = collection.get(
                limit=limit,
                offset=offset,
                include=["metadatas", "documents"]
            )
            for doc, meta in zip(results["documents"], results["metadatas"]):
                if not meta.get("chunk_id"):
                    logging.warning("âš ï¸ chunk_id ëˆ„ë½ëœ ë¬¸ì„œ ë°œê²¬")
                all_docs.append(Document(page_content=doc, metadata=meta))
            offset += limit

        return all_docs


    def load_or_build_vector_db(self, documents: List[Document], force_rebuild: bool = False):
        """Vector DBì™€ BM25 ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤."""
        
        # 1ë‹¨ê³„: Vector DB ì´ˆê¸°í™” (ë¡œë“œ ë˜ëŠ” ìƒì„±)
        self._initialize_vector_db(documents, force_rebuild)
        
        # 2ë‹¨ê³„: ìƒˆ ë¬¸ì„œê°€ ìˆë‹¤ë©´ DBì— ì¶”ê°€
        new_docs = self._update_db_with_new_docs(documents)
        
        # 3ë‹¨ê³„: BM25 ì¸ë±ìŠ¤ ë™ê¸°í™” (ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì—¬ë¶€ì— ë”°ë¼ ì²˜ë¦¬)
        self.documents = self.get_all_documents_from_db()
        self._synchronize_bm25_index(was_db_updated=(len(new_docs) > 0))
        
        # 4ë‹¨ê³„: ë¦¬ë­ì»¤ ì„ë² ë”© ìºì‹±
        self._cache_reranker_embeddings()
        logging.info("ğŸš€ ëª¨ë“  DB ë° ì¸ë±ìŠ¤ ì¤€ë¹„ ì™„ë£Œ.")
    

    def _initialize_vector_db(self, documents: List[Document], force_rebuild: bool):
        """Vector DBë¥¼ ìƒì„±í•˜ê±°ë‚˜ ë¡œë“œí•©ë‹ˆë‹¤."""
        if force_rebuild or not self._db_exists():
            logging.info("ğŸ†• ë²¡í„° DB ìƒì„± ì¤‘...")
            wrapped_docs = list(tqdm(documents, desc="ğŸ”„ ë¬¸ì„œ ì„ë² ë”© ì¤‘"))
            self.db = Chroma.from_documents(wrapped_docs, self.embedder, persist_directory=self.persist_directory)
            logging.info("âœ… ìƒˆ DB êµ¬ì¶• ì™„ë£Œ.")
        else:
            logging.info("âœ… ê¸°ì¡´ ë²¡í„° DB ë¡œë“œ ì¤‘...")
            self.load_vector_db()
    
    def _update_db_with_new_docs(self, documents: List[Document]) -> List[Document]:
        """ìƒˆë¡œìš´ ë¬¸ì„œë¥¼ í•„í„°ë§í•˜ì—¬ DBì— ì¶”ê°€í•©ë‹ˆë‹¤."""
        new_docs = self._filter_new_documents(documents)
        if not new_docs:
            logging.info("â© ìƒˆ ë¬¸ì„œ ì—†ìŒ, DB ì¶”ê°€ ìƒëµ")
            return []
    
        logging.info(f"â• ìƒˆ ë¬¸ì„œ {len(new_docs)}ê°œ ì¶”ê°€ ì¤‘...")
        batch_size = 100
        total_batches = (len(new_docs) + batch_size - 1) // batch_size
        
        for batch in tqdm(chunked(new_docs, batch_size), desc="ğŸ“¦ ë°°ì¹˜ ì¶”ê°€ ì¤‘", total=total_batches):
            self.db.add_documents(batch)
            
        return new_docs
    
    def _synchronize_bm25_index(self, was_db_updated: bool):
        """DB ìƒíƒœì— ë”°ë¼ BM25 ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ë¡œë“œí•©ë‹ˆë‹¤."""
        # ìƒˆ ë¬¸ì„œê°€ ì¶”ê°€ëë‹¤ë©´ BM25 ì¸ë±ìŠ¤ëŠ” ë¬´ì¡°ê±´ ìƒˆë¡œ ë§Œë“¤ì–´ì•¼ í•¨
        if was_db_updated:
            logging.info("ğŸ”§ ìƒˆ ë¬¸ì„œ ì¶”ê°€ë¨, BM25 ì¸ë±ìŠ¤ ì¬ìƒì„±...")
            self.build_bm25_index()
            self.save_bm25_index()
            logging.info("âœ… BM25 ì¸ë±ì‹± ì™„ë£Œ.")
            return
    
        # ìƒˆ ë¬¸ì„œê°€ ì—†ë‹¤ë©´, ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•´ë³´ê³  ì—†ìœ¼ë©´ ìƒì„±
        if not hasattr(self, "bm25_ready") or not self.bm25_ready:
            self.load_bm25_index()
            if not self.bm25_ready:
                logging.info("ğŸ“š ê¸°ì¡´ BM25 ì¸ë±ìŠ¤ ì—†ìŒ, ìƒˆë¡œ êµ¬ì¶• ì‹œì‘...")
                self.build_bm25_index()
                self.save_bm25_index()
                logging.info("âœ… BM25 ì¸ë±ì‹± ì™„ë£Œ.")
    
    def _cache_reranker_embeddings(self):
        """ë¦¬ë­ì»¤ ëª¨ë¸ì„ ìœ„í•œ ì„ë² ë”©ì„ ìºì‹±í•©ë‹ˆë‹¤."""
        if not self.documents:
            logging.warning("âš ï¸ ìºì‹±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        logging.info("ğŸ’¡ ë¦¬ë­ì»¤ ì„ë² ë”© ìºì‹± ì¤‘...")
        texts = [doc.page_content[:self.rerank_max_length] for doc in self.documents]
        self.reranker.cache_embeddings(texts, max_length=self.rerank_max_length)
        logging.info("âœ… ë¦¬ë­ì»¤ ìºì‹± ì™„ë£Œ.")

    def load_vector_db(self):
        if not self.persist_directory or not os.path.exists(self.persist_directory):
            raise ValueError("ì €ì¥ëœ DBê°€ ì—†ê±°ë‚˜ persist_directoryê°€ ì˜ëª» ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        self.db = Chroma(persist_directory=self.persist_directory, embedding_function=self.embedder)

    def _db_exists(self) -> bool:
        if not self.persist_directory:
            return False
        required_files = ["chroma.sqlite3"]
        return all(os.path.exists(os.path.join(self.persist_directory, f)) for f in required_files)

    def get_existing_chunk_ids(self) -> Set[str]:
        if self.db is None:
            try:
                self.load_vector_db()
            except Exception as e:
                logging.warning(f"âš ï¸ DB ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.db = None
                return set()

        try:
            result = self.db.get(include=["metadatas"])
            chunk_ids = set()
            for i, meta in enumerate(result.get("metadatas", [])):
                cid = meta.get("chunk_id")
                if not cid:
                    logging.warning(f"âš ï¸ chunk_id ëˆ„ë½ëœ ë¬¸ì„œ ë°œê²¬ (index={i})")
                    continue
                chunk_ids.add(cid)
            return chunk_ids
        except Exception as e:
            logging.warning(f"âš ï¸ chunk_id ëª©ë¡ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return set()


    def _filter_new_documents(self, documents: List[Document]) -> List[Document]:
        existing_chunk_ids  = self.get_existing_chunk_ids()
        new_docs = []
        for doc in documents:
            chunk_id  = doc.metadata.get("chunk_id")
            if chunk_id  and chunk_id  not in existing_chunk_ids :
                new_docs.append(doc)
        return new_docs

    # âœ… BM25 ê´€ë ¨ í•¨ìˆ˜ ì¶”ê°€
    def build_bm25_index(self):
        if self.bm25 is not None:
            logging.info("â© BM25 ì¸ë±ìŠ¤ ì´ë¯¸ ì¡´ì¬í•¨, ì¬ìƒì„± ìƒëµ")
            return
        if not self.documents:
            raise ValueError("BM25 ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

        tokenized_corpus = [self.tokenizer.tokenize_korean(doc.page_content) 
                            for doc in tqdm(self.documents, desc="ğŸ§  ë¬¸ì„œ í† í°í™” ì¤‘")]
        self.bm25 = BM25Okapi(tokenized_corpus)
        self.bm25_ready = True
        logging.info("âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")

    def _debug_print_bm25_scores(self, top_docs: List[Tuple[float, Document]]):
        """BM25 ì ìˆ˜ ë””ë²„ê·¸ ì¶œë ¥ ì „ìš© í•¨ìˆ˜"""
        print("\nğŸ“ˆ BM25 ìƒìœ„ ë¬¸ì„œ ë° ì ìˆ˜:")
        for i, (score, doc) in enumerate(top_docs):
            print(f"BM25 ë¬¸ì„œ {i+1} | ì ìˆ˜: {score:.4f} | ì¶œì²˜: {doc.metadata.get('íŒŒì¼ëª…')}")
            print(doc.page_content[:300])
            print("-" * 40)
        
    def bm25_search(self, query: str, k: int = 5, filter: Dict = None, debug: bool = False) -> List[Tuple[float, Document]]:
        if not self.bm25_ready:
            raise ValueError("BM25 ì¸ë±ìŠ¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        tokenized_query = self.tokenizer.tokenize_korean(query)
        doc_scores = self.bm25.get_scores(tokenized_query)
           
        # âœ… í•„í„°ë§ ì ìš©
        filtered_docs = []
        for score, doc in zip(doc_scores, self.documents):
            if filter:
                match = all(doc.metadata.get(k) == v for k, v in filter.items())
                if not match:
                    continue
            filtered_docs.append((score, doc))

        top_docs = sorted(filtered_docs, key=lambda x: x[0], reverse=True)[:k]

        if self.debug_mode:
            self._debug_print_bm25_scores(top_docs)
            
        return top_docs

    def rerank_documents(self, query: str, documents: List[Document]) -> Dict[str, float]:
        """ì¿¼ë¦¬ì™€ ë¬¸ì„œë“¤ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ì¬ìˆœìœ„í™” ì ìˆ˜ ë°˜í™˜"""
        if not self.reranker:
            logging.warning("âš ï¸ ì¬ìˆœìœ„í™” ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ìˆ˜ê°€ 0ìœ¼ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.")
            return {self.get_doc_key(doc): 0.0 for doc in documents}

        if not documents:
            logging.info("â„¹ï¸ ì¬ìˆœìœ„í™”í•  ë¬¸ì„œê°€ ì—†ì–´ ë¹ˆ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return {}
            
        texts_to_rerank = [doc.page_content[:self.rerank_max_length] for doc in documents]
        base_scores = self.reranker.rerank(query, texts_to_rerank)
    
        query_tokens = self.tokenizer.tokenize_korean(query)
        bonus_weight = 1.0
    
        final_scores = {}
        for doc, base_score in zip(documents, base_scores.values()):
            bonus = 0
            if query in doc.page_content:
                bonus += 2.0
            bonus += sum(1 for token in query_tokens if token in doc.page_content) * bonus_weight
    
            key = self.get_doc_key(doc)
            final_scores[key] = base_score + bonus
    
        logging.info("ğŸ§  ì¬ìˆœìœ„í™” ì ìˆ˜ ê³„ì‚° ì™„ë£Œ (ë³´ë„ˆìŠ¤ í¬í•¨)")
        return final_scores
    
    def _calculate_combined_scores(self, documents: List[Document], query: str, 
                                 bm25_scores: Dict[str, float], rerank_scores: Dict[str, float]) -> List[Tuple[float, Document]]:
        """ë¬¸ì„œë“¤ì— ëŒ€í•œ BM25 + ì¬ìˆœìœ„í™” ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜"""
        final_scored = []
        self.last_scores= {}

        # 1. ì ìˆ˜ ì •ê·œí™”
        scaled_bm25 = minmax_scale(bm25_scores)
        scaled_rerank = minmax_scale(rerank_scores)

        for doc in documents:
            key = self.get_doc_key(doc)
            bm25 = scaled_bm25.get(key, 0.0)
            rerank = scaled_rerank.get(key, 0.0)
            combined = self.bm25_weight * bm25 + self.rerank_weight * rerank
            
            self.last_scores[key] = {
                "bm25": bm25,
                "rerank": rerank,
                "combined": combined
            }
            final_scored.append((combined, doc))
    
        return final_scored

    def _debug_print_scores(self, documents: List[Document], search_type: str):
        """ë””ë²„ê·¸ ëª¨ë“œì¼ ë•Œ ì ìˆ˜ ì •ë³´ ì¶œë ¥"""
        if not self.debug_mode:
            return
            
        logging.info(f"{search_type} ì ìˆ˜ ì •ë³´")
        for doc in documents:
            key = self.get_doc_key(doc)
            scores = self.last_scores.get(key, {})
            bm25 = scores.get("bm25", 0.0)
            rerank = scores.get("rerank", 0.0)
            combined = scores.get("combined", 0.0)

            source = doc.metadata.get("íŒŒì¼ëª…", "â“")
            chunk_index = doc.metadata.get("chunk_id", "â“")
            print(f"ğŸ” {source} | Chunk {chunk_index} | BM25: {bm25:.2f} | Rerank: {rerank:.2f} | Combined: {combined:.2f}")

        
    def _merge_search_results(self, vector_results: List[Document], bm25_results: List[Tuple[float, Document]]) -> Tuple[List[Document], Dict[str, float]]:
        """ë²¡í„°ì™€ BM25 ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³‘í•©í•˜ê³  ì ìˆ˜ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        merged = {}
        bm25_scores = {}
        
        # BM25 ê²°ê³¼ ìš°ì„  ì¶”ê°€
        for score, doc in bm25_results:
            key = self.get_doc_key(doc)
            merged[key] = doc
            bm25_scores[key] = score
        
        # ë²¡í„° ê²°ê³¼ì—ì„œ ìƒˆë¡œìš´ ë¬¸ì„œë§Œ ì¶”ê°€
        for doc in vector_results:
            key = self.get_doc_key(doc)
            if key not in merged:
                merged[key] = doc
                bm25_scores[key] = 0.0
        
        return list(merged.values()), bm25_scores
    
    def hybrid_search(self, query: str, top_k: int = 3, candidate_size: int = 10,
                     candidate_filenames: List[str] = None) -> List[Document]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: BM25 + ë²¡í„° + rerank (í›„ë³´êµ°ì´ ìˆìœ¼ë©´ ê·¸ ì•ˆì—ì„œë§Œ ì‹¤í–‰)"""
        if self.db is None:
            raise ValueError("Vector DBê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if not self.bm25_ready:
            raise ValueError("BM25 ì¸ë±ìŠ¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
        # 1. ê¸°ë³¸ ê²€ìƒ‰
        vector_results = self.db.similarity_search(query, k=candidate_size, filter=None)
        bm25_results = self.bm25_search(query, k=candidate_size, filter=None)

        logging.info(f"ğŸ“ ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ íŒŒì¼ëª…: {[doc.metadata.get('íŒŒì¼ëª…') for doc in vector_results]}")

         # 2. í›„ë³´êµ° í•„í„°ë§
        if candidate_filenames:
            logging.info(f"ğŸ“ í›„ë³´êµ° ì œí•œ ì ìš©: {len(candidate_filenames)}ê°œ")
            vector_results = [doc for doc in vector_results if doc.metadata.get("íŒŒì¼ëª…") in candidate_filenames]
            bm25_results = [(score, doc) for score, doc in bm25_results if doc.metadata.get("íŒŒì¼ëª…") in candidate_filenames]
    
            # âœ… fallback: í›„ë³´êµ°ì´ ê²€ìƒ‰ ê²°ê³¼ì— ì•„ì˜ˆ ì—†ìœ¼ë©´ meta_df ê¸°ë°˜ dummy ë¬¸ì„œë¼ë„ ì¶”ê°€
            if not vector_results and not bm25_results:
                logging.warning("âš ï¸ í›„ë³´êµ°ì´ ê²€ìƒ‰ ê²°ê³¼ì— ì—†ìŒ â†’ meta_df í›„ë³´êµ° ê°•ì œ ì¶”ê°€")
                candidate_chunks = []
                for fname in candidate_filenames:
                    try:
                        # meta_dfì—ì„œ row ì°¾ì•„ dummy Document ìƒì„±
                        row = self.meta_df[self.meta_df["íŒŒì¼ëª…"] == fname].iloc[0].to_dict()
                        
                        # page_contentëŠ” 'ì‚¬ì—… ìš”ì•½'ë§Œ ì“°ê³ , metadataì—ì„œëŠ” ì œê±°
                        page_content = row.get("ì‚¬ì—… ìš”ì•½", "")
                        
                        # metadataì—ì„œ 'ì‚¬ì—… ìš”ì•½' í‚¤ ì œê±° (ì¤‘ë³µ ë°©ì§€)
                        metadata = {k: v for k, v in row.items() if k != "ì‚¬ì—… ìš”ì•½"}
                        
                        doc = Document(page_content=page_content, metadata=metadata)
                        candidate_chunks.append(doc)
                    except Exception as e:
                        logging.error(f"âŒ í›„ë³´êµ° ê°•ì œ ì¶”ê°€ ì‹¤íŒ¨: {fname}, {e}")
    
                if candidate_chunks:
                    vector_results = candidate_chunks  # ê°•ì œ íˆ¬ì…
        
        # 3. ê²°ê³¼ ë³‘í•©
        merged_docs, bm25_scores = self._merge_search_results(vector_results, bm25_results)

        if not merged_docs:
            logging.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ â†’ ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            return []
    
        # 4. rerank
        rerank_scores = self.rerank_documents(query, merged_docs)
    
        # 5. ì ìˆ˜ ê³„ì‚° + ì •ë ¬
        scored_docs = self._calculate_combined_scores(merged_docs, query, bm25_scores, rerank_scores)
        final_results = sorted(scored_docs, key=lambda x: x[0], reverse=True)
        
        if self.debug_mode:
            self._debug_print_scores([doc for _, doc in final_results], "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰")
    
        logging.info(f"ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)} â†’ {top_k}ê°œ ë°˜í™˜")
        return [doc for _, doc in final_results[:top_k]]


    def detect_query_type(self, query: str, filters: Dict[str, Dict]) -> str:
        normalized_query = query.replace(" ", "").lower()
        
        explicit_summary_keywords = normalize_keywords([
            "ì‚¬ì—…ìš”ì•½", "ê³µê³ ìš”ì•½", "ì‚¬ì—…ê°œìš”", "ê³µê³ ê°œìš”"
        ])
        
        metadata_keywords = normalize_keywords([
            "ì‚¬ì—…ê¸ˆì•¡",  "ì…ì°°ì¼", "ì…ì°°ì‹œì‘ì¼", "ì°¸ì—¬ì‹œì‘ì¼",
            "ì…ì°°ë§ˆê°ì¼", "ì°¸ì—¬ë§ˆê°ì¼", "ê³µê³ ë²ˆí˜¸", "ê³µê°œì¼ì", "ì…ì°°ê³µê³ ì¼"
        ])

        if any(k in normalized_query for k in explicit_summary_keywords):
            return "metadata"
        if any(filters for field in metadata_keywords):
            return "metadata"
        return "semantic"

    
    def smart_search(self, query: str, top_k: int = 5, candidate_size: int = 10) -> List[Document]:
        """ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰: í•„í„° ì¶”ì¶œ + ì¿¼ë¦¬ ìœ í˜• íŒë‹¨ + í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        if self.db is None or not self.bm25_ready:
            raise ValueError("âŒ Vector DB ë˜ëŠ” BM25 ì¸ë±ìŠ¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
        # 1. í•„í„° ì¶”ì¶œ
        filters = extract_filters(query, self.meta_df, self.tokenizer)
        logging.info(f"ğŸ§  ì¶”ì¶œëœ í•„í„°: {filters}")
    
        # 2. ì¿¼ë¦¬ ìœ í˜• íŒë‹¨
        query_type = self.detect_query_type(query, filters)
    
        # 3. ë°œì£¼ê¸°ê´€ ì œê±° (ì¿¼ë¦¬ì—ì„œ ì§ì ‘ ë¹¼ì¤Œ)
        if filters.get("ë°œì£¼ ê¸°ê´€"):
            agency_name = filters["ë°œì£¼ ê¸°ê´€"]["value"]
            query = re.sub(rf"\b{re.escape(agency_name)}\b", "", query).strip()
            logging.info(f"ğŸ§¹ ì¿¼ë¦¬ì—ì„œ ë°œì£¼ê¸°ê´€ í‚¤ì›Œë“œ ì œê±°ë¨: '{agency_name}'")
    
        # 4. ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í›„ë³´êµ° ë½‘ê¸°
        matched_records = []
        candidate_filenames = None
        if query_type == "metadata" and self.meta_df is not None:
            matched_df = self.meta_df[
                self.meta_df.apply(lambda row: check_filter_match(row, filters), axis=1)
            ]
            logging.info(f"ğŸ“Š ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì™„ë£Œ: {len(matched_df)}ê°œ")
    
            if not matched_df.empty:
                matched_records = matched_df.head(10).to_dict(orient="records")
                candidate_filenames = matched_df["íŒŒì¼ëª…"].dropna().unique().tolist()
                logging.info(f"ğŸ“ ì˜ë¯¸ ê²€ìƒ‰ ëŒ€ìƒ ì œí•œë¨ (íŒŒì¼ëª… ê¸°ì¤€): {len(candidate_filenames)}ê°œ")
            else:
                logging.warning("âš ï¸ ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê²°ê³¼ ì—†ìŒ â†’ ì „ì²´ ë¬¸ì„œ ëŒ€ìƒìœ¼ë¡œ ê²€ìƒ‰")
    
        # 5. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
        logging.info("ğŸ” ì˜ë¯¸ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰")
        semantic_docs = self.hybrid_search(
            query=query,
            top_k=top_k,
            candidate_size=candidate_size,
            candidate_filenames=candidate_filenames
        )
        
        return matched_records, semantic_docs
        
        