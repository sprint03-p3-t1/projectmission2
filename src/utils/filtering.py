import re
import logging
import pandas as pd
from datetime import datetime
from typing import List, Dict, Union, Any, Optional

FILTER_MAPPER = {
    "ì‚¬ì—…ê¸ˆì•¡": {
        "field": "ì‚¬ì—… ê¸ˆì•¡", 
        "type": int,
        "pattern": r"(ì‚¬ì—…\s?ê¸ˆì•¡)?\s*(\d+[ì–µë§Œì²œë°±ì¡°]+)\s*(ì´ìƒ|ì´í•˜|ì´ˆê³¼|ë¯¸ë§Œ)?"
    },
    "ì…ì°°ì‹œì‘ì¼": {
        "field": "ì…ì°° ì°¸ì—¬ ì‹œì‘ì¼",
        "type": "date",
        "pattern": r"(ì…ì°°\s?ì‹œì‘ì¼|ì°¸ì—¬\s?ì‹œì‘ì¼)[^\d]*(\d{4})[ë…„\s]*(\d{1,2})?[ì›”]?"
    },
    "ì…ì°°ë§ˆê°ì¼": {
        "field": "ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼",
        "type": "date",
        "pattern": r"(ì…ì°°\s?ë§ˆê°ì¼|ì°¸ì—¬\s?ë§ˆê°ì¼)[^\d]*(\d{4})[ë…„\s]*(\d{1,2})?[ì›”]?"
    },
    "ì…ì°°ê³µê³ ì¼": {
        "field": "ê³µê°œ ì¼ì",
        "type": "date",
        "pattern": r"(ì…ì°°\s?ê³µê³ ì¼)[^\d]*(\d{4})[ë…„\s]*(\d{1,2})?[ì›”]?"
    },
    "ë°œì£¼ê¸°ê´€": {
        "field": "ë°œì£¼ ê¸°ê´€",  
        "type": str,
        "pattern": r"(í•œêµ­ë†ì–´ì´Œê³µì‚¬|ì¡°ë‹¬ì²­|ë„ë¡œê³µì‚¬|[ê°€-í£]{2,})"
    },
    "ê³µê³ ë²ˆí˜¸": {
        "field": "ê³µê³  ë²ˆí˜¸", 
        "type": str,
        "pattern": r"(ê³µê³ ë²ˆí˜¸\s?\d{4}-?\d{3,})"
    },
}
def normalize_keywords(keywords: list[str]) -> set[str]:
    """í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ê·œí™”í•˜ì—¬ ë¹„êµ ê°€ëŠ¥í•˜ê²Œ ë³€í™˜"""
    return {k.replace(" ", "").lower() for k in keywords}

def safe_parse_date(value: str) -> Optional[datetime]:
    if not isinstance(value, str):
        return None
    try:
        parts = [int(p) for p in re.findall(r"\d+", value)]
        if len(parts) >= 2:
            year, month = parts[0], parts[1]
            day = parts[2] if len(parts) > 2 else 1
            return datetime(year, month, day)
    except Exception as e:
        logging.warning(f"âŒ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {value} â†’ {e}")
        return None

def parse_korean_number(text: str) -> int:
    unit_values = {
        "ì‹­": 10,
        "ë°±": 100,
        "ì²œ": 1000,
        "ë§Œ": 10_000,
        "ì–µ": 100_000_000,
        "ì¡°": 1_000_000_000_000
    }

    # ì •ê·œí™”
    text = text.replace(",", "").replace("ì–µì›", "ì–µ").replace("ë°±ë§Œì›", "ë°±ë§Œ") \
               .replace("ì²œë§Œì›", "ì²œë§Œ").replace("ë§Œì›", "ë§Œ").replace("ì›", "").strip()
    print("ğŸŒ¸ ì²˜ë¦¬ì „ text:", text)

    # ë‹¨ìœ„ë³„ ë¸”ë¡ ì¶”ì¶œ
    blocks = re.findall(r"(\d+)([ì‹­ë°±ì²œë§Œì–µì¡°]+)", text)

    total = 0
    current_block = 0
    last_big_unit = 1

    for num_str, unit_str in blocks:
        num = int(num_str)
        small_unit = 1
        big_unit = 1

        for char in unit_str:
            if char in ["ì‹­", "ë°±", "ì²œ"]:
                small_unit *= unit_values[char]
            elif char in ["ë§Œ", "ì–µ", "ì¡°"]:
                big_unit = unit_values[char]

        current_block += num * small_unit

        # í° ë‹¨ìœ„ê°€ ë¶™ì—ˆìœ¼ë©´ ì „ì²´ ë¸”ë¡ì— ê³±í•´ì„œ totalì— ë”í•¨
        if big_unit > 1:
            total += current_block * big_unit
            current_block = 0

    total += current_block
    print("ğŸŒ¸ ì²˜ë¦¬ì™„ë£Œí›„:", total)
    return total


# ì˜ˆì‹œ:
# parse_korean_number("5ì²œë§Œì›")        # 50000000
# parse_korean_number("1ì–µ 2ì²œë§Œì›")    # 120000000
# parse_korean_number("2ì²œë§Œ")         # 20000000
# parse_korean_number("3ë°±ì–µ")         # 30000000000
# parse_korean_number("456ë°±ë§Œ")       # 456000000


def convert_value(raw: str, value_type):
    """í•„í„° ì¶”ì¶œìš© ê°’ ë³€í™˜"""
    if value_type in ("int", int):
        return parse_korean_number(raw)
    elif value_type in ("date", datetime):
        return safe_parse_date(raw)
    elif value_type in ("float", float):
        try:
            return float(str(raw).replace(",", "").strip())
        except ValueError:
            return None
    else:
        return raw.strip()


OPERATOR_FUNC = {
    ">=": lambda v, t: v >= t,
    "<=": lambda v, t: v <= t,
    ">":  lambda v, t: v > t,
    "<":  lambda v, t: v < t,
    "=":  lambda v, t: v == t,
    "~":  lambda v, t: abs(v - t) <= t * 0.1
}

def extract_operator(text: str, context: str = "") -> str:
    full_text = text + " " + context
    if "ì´í›„" in full_text or "ë¶€í„°" in full_text or "ìµœì†Œ" in full_text or "ì´ìƒ" in full_text:
        return ">="
    elif "ì´ì „" in full_text or "ê¹Œì§€" in full_text or "ìµœëŒ€" in full_text or "ì´í•˜" in full_text:
        return "<="
    elif "ì´ˆê³¼" in full_text:
        return ">"
    elif "ë¯¸ë§Œ" in full_text:
        return "<"
    elif "ì•½" in full_text or "ì •ë„" in full_text:
        return "~"
    return "="


# ğŸš¨ ê¸°ê´€ëª…/íŒŒì¼ëª… í•„í„°ë§ ì‹œ ì œê±°í•  ì¡ìŒ ë‹¨ì–´ ëª©ë¡
NOISE_WORDS = {
     # ë‚ ì§œ/ì‹œì  ê´€ë ¨
    "ë…„", "ì›”", "ì¼", "ë…„ë„", "2024", "2025",

    # ì…ì°°/ê³µê³  ê´€ë ¨
    "ì…ì°°", "ê³µê³ ", "ì¬ê³µê³ ", "ê¸´ê¸‰", "í˜‘ìƒ", "ì‚¬ì „ê³µê°œ",

    # ê¸ˆì•¡ ê´€ë ¨
    "ì›", "ì˜ˆì‚°",

}

def extract_field_filter_by_tokens(
    query: str,
    field_values: List[str],
    tokenizer,
    field_name: str,
    use_exact_match: bool = True,
    threshold: float = 0.5
) -> Optional[Dict[str, Dict]]:
    
    query_tokens = set(tokenizer.tokenize_korean(query, use_bigrams=False)) - NOISE_WORDS
    print("â¤ï¸query_tokens", query_tokens)
    logging.debug(f"ğŸ§¹ í•„í„°ë§ìš© í† í°ì…‹: {query_tokens}")

    # 1ï¸âƒ£ ì •í™• ë§¤ì¹­ ìš°ì„ 
    if use_exact_match:
        for value in field_values:
            if value and value in query:
                return {field_name: {"value": value, "operator": "="}}

    # ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­
    best_match = None
    best_score = 0
    for value in field_values:
        match_count = sum(1 for token in query_tokens if token in value)
        score = match_count / len(query_tokens) if query_tokens else 0
        if score > best_score and score > threshold:
            best_match = value
            best_score = score
           
    if best_match:
        print("â¤ï¸best_match", best_match, best_score)
        return {field_name: {"value": best_match, "operator": "="}}

    return None


def extract_filters(query: str, meta_df: pd.DataFrame, tokenizer) -> Dict[str, Dict]:
    filters = {}
    
    # 3ï¸âƒ£ ì •ê·œì‹ ê¸°ë°˜ í•„í„° ì¶”ì¶œ
    for keyword, filter_info in FILTER_MAPPER.items():
        field_name = filter_info.get("field")
    
        # âœ… ê¸°ê´€ëª…/íŒŒì¼ëª…ì€ ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì§€ ì•ŠìŒ
        if field_name in filters or field_name in ["ë°œì£¼ ê¸°ê´€", "íŒŒì¼ëª…"]:
            continue
            
        match = re.search(filter_info['pattern'], query)
        if match:
            value_type = filter_info.get('type')
            if value_type == "date":
                year = match.group(2)
                month = match.group(3) if match.lastindex and match.lastindex >= 3 and match.group(3) else "1"
                raw_value = f"{year}ë…„ {month}ì›”"
            elif value_type == int:
                raw_value = match.group(2)
                condition = match.group(3) or ""
                operator = extract_operator(condition, query)
            else:
                raw_value = match.group(1)
                operator = "="
    
            value = convert_value(raw_value, value_type)
            operator = extract_operator(raw_value, query) if value_type in ["date", int] else "="
            if value is not None:
                filters[field_name] = {"value": value, "operator": operator}
                logging.info(f"ğŸ“Œ {field_name} í•„í„° ì ìš©ë¨: {value} ({operator})")

    # ë°œì£¼ ê¸°ê´€ í•„í„°ë§
    agency_filter_applied = False
    if "ë°œì£¼ ê¸°ê´€" in meta_df.columns:
        agency_list = meta_df["ë°œì£¼ ê¸°ê´€"].dropna().unique().tolist()
        agency_filter = extract_field_filter_by_tokens(
            query=query,
            field_values=agency_list,
            tokenizer=tokenizer,
            field_name="ë°œì£¼ ê¸°ê´€",
            use_exact_match=True,
            threshold=0.5
        )
        
        print("â¤ï¸agency_filter : ", agency_filter)
        if agency_filter:
            filters.update(agency_filter)
            agency_filter_applied = True
            logging.info(f"ğŸ¢ ë°œì£¼ ê¸°ê´€ í•„í„° ì ìš©ë¨: {agency_filter['ë°œì£¼ ê¸°ê´€']['value']}")
   
    # íŒŒì¼ëª… ë³´ì¡° í•„í„°ë§ (ê¸°ê´€ í•„í„° ì—†ì„ ë•Œë§Œ)
    if not agency_filter_applied and "íŒŒì¼ëª…" in meta_df.columns:
        filename_list = meta_df["íŒŒì¼ëª…"].dropna().unique().tolist()
        print("â¤ï¸íŒŒì¼í•„í„°ì‘ë™ ")
        filename_filter = extract_field_filter_by_tokens(
            query=query,
            field_values=filename_list,
            tokenizer=tokenizer,
            field_name="íŒŒì¼ëª…",
            use_exact_match=True,
            threshold=0.5  # âœ… ë” ìœ ì—°í•˜ê²Œ
        )
        print("â¤ï¸file_filter : ", filename_filter)
        if filename_filter:
            filters.update(filename_filter)
            logging.info(f"ğŸ“ íŒŒì¼ëª… í•„í„° ì ìš©ë¨: {filename_filter['íŒŒì¼ëª…']['value']}")

    return filters


def is_valid_value(value):
    """ê°’ì´ ìœ íš¨í•œì§€ í™•ì¸í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    if value is None or str(value).strip() in ["", "ë¯¸ì •", "nan"]:
        return False
    return True

def check_filter_match(data: Union[Dict, Any], filters: Dict[str, Dict]) -> bool:
    for field, condition in filters.items():
        raw_value = data.get(field)

        if not is_valid_value(raw_value):
            return False

        target = condition["value"]
        operator = condition["operator"]

        # âœ… raw_valueê°€ targetê³¼ ê°™ì€ íƒ€ì…ì¸ì§€ í™•ì¸
        try:
            if isinstance(target, int):
                value = int(str(raw_value).replace(",", "").strip())
            elif isinstance(target, float):
                value = float(str(raw_value).replace(",", "").strip())
            elif isinstance(target, datetime):
                value = safe_parse_date(str(raw_value))
            else:
                value = str(raw_value).strip()
        except Exception:
            return False

        compare_func = OPERATOR_FUNC.get(operator)
        if not compare_func:
            return False

        try:
            return compare_func(value, target)
        except TypeError:
            return False

    return True

# ì˜ˆì‹œ: 'ì‚¬ì—…ê¸ˆì•¡ 5ì²œë§Œì› ì´ìƒì¸ ê³µê³  ì°¾ì•„ì¤˜'