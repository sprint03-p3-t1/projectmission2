"""
RFP RAG ì‹œìŠ¤í…œ - ì œë„¤ë ˆì´ì…˜ ëª¨ë“ˆ
ê²€ìƒ‰ëœ ì²­í¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ëª¨ë“ˆ
ë³¸ì¸ ì‘ì—…ìš© ëª¨ë“ˆ
"""

import os
import logging
import time
from typing import List, Dict, Any, Optional
from datetime import datetime

# LLM
from openai import OpenAI

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from data_processing import RetrievalResult, RAGResponse, RAGSystemInterface
from ops import get_quality_metrics, get_quality_monitor, get_conversation_tracker
from prompts.prompt_manager import get_prompt_manager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RFPGenerator(RAGSystemInterface):
    """RFP ì‘ë‹µ ìƒì„±ê¸° - ë³¸ì¸ ì‘ì—… ì˜ì—­"""
    
    def __init__(self, api_key: str = None, model: str = None, temperature: float = None, max_tokens: int = None):
        # YAML ì„¤ì •ì—ì„œ ê¸°ë³¸ê°’ ê°€ì ¸ì˜¤ê¸°
        try:
            from src.config.yaml_config import yaml_config
            config = yaml_config.get_generation_config()
            
            # íŒŒë¼ë¯¸í„°ê°€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° YAML ì„¤ì • ì‚¬ìš©
            self.api_key = api_key or os.getenv(config.get('api_key_env', 'OPENAI_API_KEY'))
            self.model = model or config.get('model', 'gpt-4.1-mini')
            self.temperature = temperature or config.get('temperature', 0.1)
            self.max_tokens = max_tokens or config.get('max_tokens', 2000)
            
            # MLOps ì„¤ì •
            self.enable_quality_evaluation = config.get('enable_quality_evaluation', True)
            self.enable_conversation_logging = config.get('enable_conversation_logging', True)
            self.conversation_history_limit = config.get('conversation_history_limit', 6)
            
            # í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì„¤ì •
            self.prompt_manager_config = config.get('prompt_manager_config', {})
            self.legacy_prompts = config.get('legacy_prompts', {})
            
        except ImportError:
            # í´ë°±: í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°
            self.api_key = api_key or os.getenv("OPENAI_API_KEY")
            self.model = model or os.getenv("MODEL_NAME", "gpt-4.1-mini")
            self.temperature = temperature or float(os.getenv("TEMPERATURE", "0.1"))
            self.max_tokens = max_tokens or int(os.getenv("MAX_TOKENS", "2000"))
            self.enable_quality_evaluation = True
            self.enable_conversation_logging = True
            self.conversation_history_limit = 6
            self.prompt_manager_config = {}
            self.legacy_prompts = {}
        
        self.client = None
        self.conversation_history: List[Dict[str, str]] = []
        self._is_ready = False
        self.current_session_id = None
        
        # MLOps êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self.quality_metrics = get_quality_metrics()
        self.quality_monitor = get_quality_monitor()
        self.conversation_tracker = get_conversation_tracker()
        
        # í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        try:
            self.prompt_manager = get_prompt_manager()
            # YAML ì„¤ì •ì—ì„œ í˜„ì¬ ë²„ì „ ì„¤ì •
            if self.prompt_manager_config.get('current_version'):
                self.prompt_manager.set_current_version(self.prompt_manager_config['current_version'])
        except Exception as e:
            logger.warning(f"Failed to initialize prompt manager: {e}")
            self.prompt_manager = None
        
        # ì§ˆë¬¸ ìœ í˜• ì„¤ì • (ê¸°ë³¸ê°’: general)
        self.question_type = "general"
    
    def initialize(self):
        """ì œë„¤ë ˆì´í„° ì´ˆê¸°í™”"""
        if not self.api_key:
            raise ValueError("OpenAI API key is required")
        
        self.client = OpenAI(api_key=self.api_key)
        self._is_ready = True
        logger.info(f"Initialized RFP Generator with model: {self.model}")
    
    def is_ready(self) -> bool:
        """ì œë„¤ë ˆì´í„° ì¤€ë¹„ ìƒíƒœ í™•ì¸"""
        return self._is_ready and self.client is not None
    
    def generate_response(self, 
                         question: str, 
                         retrieved_results: List[RetrievalResult],
                         use_conversation_history: bool = True) -> RAGResponse:
        """ê²€ìƒ‰ëœ ì²­í¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        if not self.is_ready():
            raise RuntimeError("Generator is not initialized. Call initialize() first.")
        
        start_time = time.time()
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._build_context(retrieved_results)
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©)
        system_prompt = self._get_system_prompt()
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]
        
        if use_conversation_history and self.conversation_history:
            # YAML ì„¤ì •ì—ì„œ ì§€ì •ëœ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì œí•œ ì‚¬ìš©
            messages.extend(self.conversation_history[-self.conversation_history_limit:])
        
        # ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ì»¨í…ìŠ¤íŠ¸
        # ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„± (í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©)
        user_message = self._create_user_message(question, context)
        messages.append({"role": "user", "content": user_message})
        
        try:
            # OpenAI API í˜¸ì¶œ - Pydantic ì—ëŸ¬ ìš°íšŒ
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=self.temperature,
                    max_tokens=self.max_tokens
                )
                answer = response.choices[0].message.content
                
                # ìƒì„± ë©”íƒ€ë°ì´í„°
                generation_time = time.time() - start_time
                generation_metadata = {
                    "model": self.model,
                    "temperature": self.temperature,
                    "max_tokens": self.max_tokens,
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens,
                    "generation_time": generation_time,
                    "timestamp": datetime.now().isoformat()
                }
            except Exception as pydantic_error:
                # Pydantic ì—ëŸ¬ ë°œìƒ ì‹œ ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±
                logger.warning(f"Pydantic error occurred, using fallback: {pydantic_error}")
                answer = f"ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n{context[:1000]}..."
                
                # ìƒì„± ë©”íƒ€ë°ì´í„° (ì—ëŸ¬ ì‹œ)
                generation_time = time.time() - start_time
                generation_metadata = {
                    "model": self.model,
                    "temperature": self.temperature,
                    "max_tokens": self.max_tokens,
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0,
                    "generation_time": generation_time,
                    "timestamp": datetime.now().isoformat(),
                    "error": str(pydantic_error)
                }
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            if use_conversation_history:
                self.conversation_history.append({"role": "user", "content": question})
                self.conversation_history.append({"role": "assistant", "content": answer})
            
            # RetrievalResult ê°ì²´ë“¤ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            chunks_dict = []
            for result in retrieved_results:
                chunk_dict = {
                    "chunk": {
                        "chunk_id": result.chunk.chunk_id,
                        "doc_id": result.chunk.doc_id,
                        "content": result.chunk.content,
                        "chunk_type": result.chunk.chunk_type,
                        "page_number": result.chunk.page_number,
                        "metadata": result.chunk.metadata
                    },
                    "score": result.score,
                    "rank": result.rank
                }
                chunks_dict.append(chunk_dict)
            
            # í’ˆì§ˆ í‰ê°€ ìˆ˜í–‰ (ì˜µì…˜)
            quality_evaluation = None
            if self.enable_quality_evaluation:
                try:
                    quality_evaluation = self.evaluate_response_quality(question, answer, context)
                    
                    # í’ˆì§ˆ í‰ê°€ ê²°ê³¼ ì €ì¥
                    evaluation_id = self.quality_metrics.store_evaluation(
                        question=question,
                        answer=answer,
                        context=context,
                        scores=quality_evaluation["scores"],
                        overall_score=quality_evaluation["overall_score"],
                        suggestions=quality_evaluation["suggestions"],
                        evaluation_text=quality_evaluation["evaluation_text"],
                        model_name=self.model,
                        user_id=None,  # TODO: ì‚¬ìš©ì ID ì¶”ê°€
                        session_id=None  # TODO: ì„¸ì…˜ ID ì¶”ê°€
                    )
                    
                    # ìƒì„± ë©”íƒ€ë°ì´í„°ì— í’ˆì§ˆ í‰ê°€ ì •ë³´ ì¶”ê°€
                    generation_metadata["quality_evaluation"] = quality_evaluation
                    generation_metadata["evaluation_id"] = evaluation_id
                    
                    logger.info(f"Quality evaluation completed: {quality_evaluation['overall_score']:.3f}")
                    
                except Exception as e:
                    logger.error(f"Quality evaluation failed: {e}")
                    quality_evaluation = {"error": str(e)}
            
            # ëŒ€í™” ë¡œê¹… (ì˜µì…˜)
            if self.enable_conversation_logging:
                try:
                    logger.info(f"ğŸ” ëŒ€í™” ë¡œê¹… ì‹œì‘ - retrieved_results ê°œìˆ˜: {len(retrieved_results)}")
                    
                    # ê²€ìƒ‰ ë‹¨ê³„ë³„ ë¡œê·¸ ìƒì„±
                    search_steps = []
                    if retrieved_results:
                        logger.info(f"ğŸ” ì²« ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼ íƒ€ì…: {type(retrieved_results[0])}")
                        logger.info(f"ğŸ” ì²« ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼ ì†ì„±: {dir(retrieved_results[0])}")
                        
                        # ì„ë² ë”© ë‹¨ê³„ - RetrievalResultì—ëŠ” embeddingì´ ì—†ìœ¼ë¯€ë¡œ ë‹¤ë¥¸ ë°©ë²• ì‚¬ìš©
                        search_steps.append({
                            'type': 'embedding',
                            'input': {'query': question},
                            'output': {'embedding_dim': 'N/A'},  # RetrievalResultì— embedding ì •ë³´ ì—†ìŒ
                            'execution_time_ms': 0,  # TODO: ì‹¤ì œ ì„ë² ë”© ì‹œê°„ ì¸¡ì •
                            'metadata': {'note': 'embedding_dim not available in RetrievalResult'}
                        })
                        
                        # ë²¡í„° ê²€ìƒ‰ ë‹¨ê³„
                        search_steps.append({
                            'type': 'vector_search',
                            'input': {'query': question, 'top_k': len(retrieved_results)},
                            'output': {'retrieved_count': len(retrieved_results)},
                            'execution_time_ms': 0,  # TODO: ì‹¤ì œ ê²€ìƒ‰ ì‹œê°„ ì¸¡ì •
                            'metadata': {'search_method': 'vector'}
                        })
                    
                    # ëŒ€í™” ë¡œê·¸ ì €ì¥
                    logger.info(f"ğŸ” ëŒ€í™” ë¡œê·¸ ì €ì¥ ì‹œì‘ - session_id: {self.current_session_id}")
                    logger.info(f"ğŸ” question: {question[:100]}...")
                    logger.info(f"ğŸ” answer: {answer[:100]}...")
                    logger.info(f"ğŸ” chunks_dict length: {len(chunks_dict) if chunks_dict else 0}")
                    
                    log_id = self.conversation_tracker.log_conversation(
                        session_id=self.current_session_id or "default_session",
                        question=question,
                        answer=answer,
                        system_type="faiss",  # TODO: ì‹¤ì œ ì‹œìŠ¤í…œ íƒ€ì… ì „ë‹¬
                        model_name=self.model,
                        temperature=self.temperature,
                        max_tokens=self.max_tokens,
                        search_method="vector",
                        retrieved_chunks=chunks_dict,
                        generation_metadata=generation_metadata,
                        quality_evaluation=quality_evaluation,
                        conversation_history=self.conversation_history,
                        search_steps=search_steps
                    )
                    
                    generation_metadata["conversation_log_id"] = log_id
                    logger.info(f"âœ… Conversation logged with ID: {log_id}")
                    
                except Exception as e:
                    logger.error(f"âŒ Conversation logging failed: {e}")
                    logger.error(f"âŒ Error type: {type(e)}")
                    import traceback
                    logger.error(f"âŒ Traceback: {traceback.format_exc()}")
        
            return RAGResponse(
                question=question,
                answer=answer,
                retrieved_chunks=chunks_dict,
                generation_metadata=generation_metadata
            )
        
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            # ì—ëŸ¬ ì‹œì—ë„ retrieved_chunksë¥¼ Dictë¡œ ë³€í™˜
            chunks_dict = []
            for result in retrieved_results:
                chunks_dict.append({
                    "chunk_id": result.chunk.chunk_id,
                    "content": result.chunk.content[:200] + "..." if len(result.chunk.content) > 200 else result.chunk.content,
                    "score": result.score,
                    "rank": result.rank
                })
            
            return RAGResponse(
                question=question,
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                retrieved_chunks=retrieved_results,
                generation_metadata={"error": str(e), "generation_time": time.time() - start_time}
            )
    
    def _build_context(self, retrieved_results: List[RetrievalResult]) -> str:
        """ê²€ìƒ‰ëœ ì²­í¬ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±"""
        if not retrieved_results:
            return "ê´€ë ¨ëœ ë¬¸ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        
        for result in retrieved_results:
            chunk = result.chunk
            context_part = f"""
[ë¬¸ì„œ {result.rank}] (ìœ ì‚¬ë„: {result.score:.3f})
ê³µê³ ë²ˆí˜¸: {chunk.metadata.get('ê³µê³ ë²ˆí˜¸', 'N/A')}
ì‚¬ì—…ëª…: {chunk.metadata.get('ì‚¬ì—…ëª…', 'N/A')}
ì²­í¬ ìœ í˜•: {chunk.chunk_type}
{f"í˜ì´ì§€: {chunk.page_number}" if chunk.page_number else ""}

ë‚´ìš©:
{chunk.content}
"""
            context_parts.append(context_part)
        
        return "\n" + "="*80 + "\n".join(context_parts)
    
    def _create_user_message(self, question: str, context: str) -> str:
        """ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„± (ì§ˆë¬¸ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)"""
        if self.prompt_manager:
            try:
                # ì§ˆë¬¸ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
                if hasattr(self, 'question_type') and self.question_type != "general":
                    template = self.prompt_manager.get_user_template_by_type(self.question_type)
                    if template:
                        logger.info(f"ğŸ¯ ì§ˆë¬¸ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {self.question_type}")
                        return template.format(question=question, context=context)
                
                # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©
                return self.prompt_manager.format_user_message(question, context)
            except Exception as e:
                logger.warning(f"Failed to use prompt manager for user message: {e}")
        
        # í´ë°±: ë ˆê±°ì‹œ í…œí”Œë¦¿ ì‚¬ìš©
        return f"""
ì§ˆë¬¸: {question}

ê´€ë ¨ RFP ë¬¸ì„œ ì •ë³´:
{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
ë¬¸ì„œì— ì •ë³´ê°€ ìˆìœ¼ë©´ ì‚¬ì—…ëª…, ë°œì£¼ê¸°ê´€, ì‚¬ì—…ê¸ˆì•¡, ê¸°ê°„ ë“± í•µì‹¬ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•íˆ ë§ì”€í•´ ì£¼ì„¸ìš”.
"""
    
    def _get_system_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜ (í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©)"""
        if self.prompt_manager:
            try:
                return self.prompt_manager.get_system_prompt()
            except Exception as e:
                logger.warning(f"Failed to use prompt manager for system prompt: {e}")
        
        # í´ë°±: ë ˆê±°ì‹œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        if self.legacy_prompts.get('system_prompt'):
            return self.legacy_prompts['system_prompt']
        
        # ìµœì¢… í´ë°±: í•˜ë“œì½”ë”©ëœ í”„ë¡¬í”„íŠ¸
        return """
ë‹¹ì‹ ì€ RFP(ì œì•ˆìš”ì²­ì„œ) ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì •ë¶€ê¸°ê´€ê³¼ ê¸°ì—…ì˜ ì…ì°° ê³µê³  ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì»¨ì„¤í„´íŠ¸ë“¤ì´ í•„ìš”í•œ ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ë‹¤ìŒ ì›ì¹™ì„ ì§€ì¼œì£¼ì„¸ìš”:
1. ì œê³µëœ ë¬¸ì„œ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ê°„ë‹¨í•˜ê³  ëª…í™•íˆ ë§í•˜ì„¸ìš”.
3. ë¬¸ì„œì— ì •ë³´ê°€ ìˆìœ¼ë©´ ì‚¬ì—…ëª…, ë°œì£¼ê¸°ê´€, ì‚¬ì—…ê¸ˆì•¡, ê¸°ê°„ ë“± í•µì‹¬ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
4. í‘œë‚˜ ëª©ë¡ì´ ìˆëŠ” ê²½ìš° êµ¬ì¡°í™”í•˜ì—¬ ë³´ê¸° ì‰½ê²Œ ì •ë¦¬í•˜ì„¸ìš”.
5. ì…ì°° ì°¸ê°€ ìê²©, í‰ê°€ ê¸°ì¤€, ì œì¶œ ì„œë¥˜ ë“± ì¤‘ìš”í•œ ìš”êµ¬ì‚¬í•­ì€ ë†“ì¹˜ì§€ ë§ê³  í¬í•¨í•˜ì„¸ìš”.
6. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
7. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ì¶”ì¸¡í•˜ê±°ë‚˜ ê´€ë ¨ ì—†ëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ë§ˆì„¸ìš”.
"""
    
    def update_system_prompt(self, new_prompt: str):
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸ (ë ˆê±°ì‹œ í˜¸í™˜ì„±)"""
        self._system_prompt = new_prompt
        logger.info("System prompt updated")
    
    def set_prompt_version(self, version: str) -> bool:
        """í”„ë¡¬í”„íŠ¸ ë²„ì „ ë³€ê²½"""
        if self.prompt_manager:
            return self.prompt_manager.set_current_version(version)
        return False
    
    def get_available_prompt_versions(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ ë²„ì „ ëª©ë¡ ë°˜í™˜"""
        if self.prompt_manager:
            return self.prompt_manager.get_available_versions()
        return []
    
    def get_current_prompt_version(self) -> str:
        """í˜„ì¬ í”„ë¡¬í”„íŠ¸ ë²„ì „ ë°˜í™˜"""
        if self.prompt_manager:
            return self.prompt_manager.get_current_version()
        return "legacy"
    
    def _get_default_evaluation_prompt(self, question: str, answer: str, context: str) -> str:
        """ê¸°ë³¸ í‰ê°€ í”„ë¡¬í”„íŠ¸ (í´ë°±ìš©)"""
        return f"""
ë‹¤ìŒ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í‰ê°€í•´ì£¼ì„¸ìš”. ê° í•­ëª©ì„ 0-1 ì ìˆ˜ë¡œ í‰ê°€í•˜ê³ , ê°œì„  ì œì•ˆì„ í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€: {answer}

ì°¸ê³  ë¬¸ì„œ: {context[:2000]}...

í‰ê°€ ê¸°ì¤€:
1. ê´€ë ¨ì„± (Relevance): ë‹µë³€ì´ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆëŠ”ê°€? (0-1)
2. ì™„ì„±ë„ (Completeness): ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ì™„ì „í•œê°€? (0-1)
3. ì •í™•ì„± (Accuracy): ë‹µë³€ ë‚´ìš©ì´ ì–¼ë§ˆë‚˜ ì •í™•í•œê°€? (0-1)
4. ëª…í™•ì„± (Clarity): ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ì´í•´í•˜ê¸° ì‰¬ìš´ê°€? (0-1)
5. êµ¬ì¡°í™” (Structure): ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ì²´ê³„ì ìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆëŠ”ê°€? (0-1)

ì‘ë‹µ í˜•ì‹:
ê´€ë ¨ì„±: 0.85
ì™„ì„±ë„: 0.78
ì •í™•ì„±: 0.92
ëª…í™•ì„±: 0.80
êµ¬ì¡°í™”: 0.75
ì¢…í•©ì ìˆ˜: 0.82
ê°œì„ ì œì•ˆ: [êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆ 3ê°€ì§€]
"""
    
    def clear_conversation_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history = []
        logger.info("Conversation history cleared")
    
    def get_conversation_history(self) -> List[Dict[str, str]]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.conversation_history.copy()
    
    def set_generation_parameters(self, temperature: float = None, max_tokens: int = None):
        """ìƒì„± íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸"""
        if temperature is not None:
            self.temperature = temperature
        if max_tokens is not None:
            self.max_tokens = max_tokens
        logger.info(f"Generation parameters updated: temperature={self.temperature}, max_tokens={self.max_tokens}")
    
    def generate_summary(self, retrieved_results: List[RetrievalResult], summary_type: str = "general") -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ìš”ì•½ ìƒì„±"""
        if not retrieved_results:
            return "ìš”ì•½í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
        
        context = self._build_context(retrieved_results)
        
        summary_prompts = {
            "general": "ìœ„ ë¬¸ì„œë“¤ì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            "requirements": "ìœ„ ë¬¸ì„œë“¤ì—ì„œ ì…ì°° ìš”êµ¬ì‚¬í•­ê³¼ ì¡°ê±´ë“¤ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
            "evaluation": "ìœ„ ë¬¸ì„œë“¤ì˜ í‰ê°€ ê¸°ì¤€ê³¼ ë°©ë²•ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
            "timeline": "ìœ„ ë¬¸ì„œë“¤ì˜ ì¼ì •ê³¼ ë§ˆê°ì¼ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”."
        }
        
        prompt = summary_prompts.get(summary_type, summary_prompts["general"])
        
        messages = [
            {"role": "system", "content": self._get_system_prompt()},
            {"role": "user", "content": f"{context}\n\n{prompt}"}
        ]
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            return response.choices[0].message.content
        
        except Exception as e:
            logger.error(f"Error generating summary: {e}")
            return "ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def generate_comparison(self, results_list: List[List[RetrievalResult]], comparison_aspects: List[str]) -> str:
        """ì—¬ëŸ¬ ë¬¸ì„œ ì„¸íŠ¸ ë¹„êµ ë¶„ì„"""
        if not results_list or len(results_list) < 2:
            return "ë¹„êµí•  ë¬¸ì„œê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        
        contexts = []
        for i, results in enumerate(results_list):
            context = self._build_context(results)
            contexts.append(f"[ë¬¸ì„œ ê·¸ë£¹ {i+1}]\n{context}")
        
        comparison_context = "\n\n".join(contexts)
        aspects_str = ", ".join(comparison_aspects)
        
        messages = [
            {"role": "system", "content": self._get_system_prompt()},
            {"role": "user", "content": f"""
ë‹¤ìŒ ë¬¸ì„œ ê·¸ë£¹ë“¤ì„ {aspects_str} ì¸¡ë©´ì—ì„œ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”:

{comparison_context}

ê° ê·¸ë£¹ì˜ íŠ¹ì§•ê³¼ ì°¨ì´ì ì„ ëª…í™•íˆ ì •ë¦¬í•´ì£¼ì„¸ìš”.
"""}
        ]
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            return response.choices[0].message.content
        
        except Exception as e:
            logger.error(f"Error generating comparison: {e}")
            return "ë¹„êµ ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def evaluate_response_quality(self, question: str, answer: str, context: str) -> Dict[str, Any]:
        """LLM ê¸°ë°˜ ë‹µë³€ í’ˆì§ˆ í‰ê°€"""
        if not self.is_ready():
            raise RuntimeError("Generator is not initialized. Call initialize() first.")
        
        # í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„± (í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©)
        if self.prompt_manager:
            try:
                evaluation_prompt = self.prompt_manager.format_evaluation_prompt(question, answer, context)
            except Exception as e:
                logger.warning(f"Failed to use prompt manager for evaluation prompt: {e}")
                evaluation_prompt = self._get_default_evaluation_prompt(question, answer, context)
        else:
            evaluation_prompt = self._get_default_evaluation_prompt(question, answer, context)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°ê´€ì ì´ê³  ì •í™•í•œ í‰ê°€ë¥¼ í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": evaluation_prompt}
                ],
                temperature=0.1,  # ì¼ê´€ëœ í‰ê°€ë¥¼ ìœ„í•´ ë‚®ì€ temperature
                max_tokens=500
            )
            
            evaluation_text = response.choices[0].message.content
            
            # í‰ê°€ ê²°ê³¼ íŒŒì‹±
            scores = self._parse_evaluation_scores(evaluation_text)
            suggestions = self._parse_improvement_suggestions(evaluation_text)
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            overall_score = sum(scores.values()) / len(scores) if scores else 0.0
            
            return {
                "scores": scores,
                "overall_score": overall_score,
                "suggestions": suggestions,
                "evaluation_text": evaluation_text,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error evaluating response quality: {e}")
            return {
                "scores": {"relevance": 0.0, "completeness": 0.0, "accuracy": 0.0, "clarity": 0.0, "structure": 0.0},
                "overall_score": 0.0,
                "suggestions": ["í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."],
                "evaluation_text": "í‰ê°€ ì‹¤íŒ¨",
                "timestamp": datetime.now().isoformat(),
                "error": str(e)
            }
    
    def _parse_evaluation_scores(self, evaluation_text: str) -> Dict[str, float]:
        """í‰ê°€ í…ìŠ¤íŠ¸ì—ì„œ ì ìˆ˜ ì¶”ì¶œ"""
        scores = {}
        lines = evaluation_text.split('\n')
        
        # ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ (í•œê¸€ í‚¤ì›Œë“œë¥¼ ì˜ë¬¸ í‚¤ë¡œ ë³€í™˜)
        key_mapping = {
            'ê´€ë ¨ì„±': 'relevance',
            'ì™„ì„±ë„': 'completeness', 
            'ì •í™•ì„±': 'accuracy',
            'ëª…í™•ì„±': 'clarity',
            'êµ¬ì¡°í™”': 'structure'
        }
        
        for line in lines:
            line = line.strip()
            if ':' in line:
                try:
                    # ì½œë¡ ìœ¼ë¡œ ë¶„ë¦¬
                    parts = line.split(':', 1)
                    if len(parts) == 2:
                        key = parts[0].strip()
                        value_str = parts[1].strip()
                        
                        # ìˆ«ì ì¶”ì¶œ (ê³µë°±ì´ë‚˜ ë‹¤ë¥¸ ë¬¸ìê°€ í¬í•¨ë  ìˆ˜ ìˆìŒ)
                        import re
                        number_match = re.search(r'(\d+\.?\d*)', value_str)
                        if number_match:
                            value = float(number_match.group(1))
                            
                            # í•œê¸€ í‚¤ì›Œë“œ í™•ì¸ ë° ì˜ë¬¸ í‚¤ë¡œ ë³€í™˜
                            for korean_key, english_key in key_mapping.items():
                                if korean_key in key:
                                    scores[english_key] = value
                                    break
                except (ValueError, IndexError):
                    continue
        
        return scores
    
    def _parse_improvement_suggestions(self, evaluation_text: str) -> List[str]:
        """í‰ê°€ í…ìŠ¤íŠ¸ì—ì„œ ê°œì„  ì œì•ˆ ì¶”ì¶œ"""
        suggestions = []
        lines = evaluation_text.split('\n')
        
        in_suggestions = False
        for line in lines:
            line = line.strip()
            if 'ê°œì„ ì œì•ˆ' in line or 'ê°œì„  ì œì•ˆ' in line:
                in_suggestions = True
                continue
            if in_suggestions and line:
                if line.startswith('-') or line.startswith('â€¢') or line.startswith('*'):
                    suggestions.append(line[1:].strip())
                elif line and not any(keyword in line.lower() for keyword in ['ê´€ë ¨ì„±', 'ì™„ì„±ë„', 'ì •í™•ì„±', 'ëª…í™•ì„±', 'êµ¬ì¡°í™”', 'ì¢…í•©ì ìˆ˜']):
                    suggestions.append(line)
        
        return suggestions[:3] if suggestions else ["êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
    
    def enable_quality_evaluation(self, enable: bool = True):
        """í’ˆì§ˆ í‰ê°€ í™œì„±í™”/ë¹„í™œì„±í™”"""
        self.enable_quality_evaluation = enable
        logger.info(f"Quality evaluation {'enabled' if enable else 'disabled'}")
    
    def get_quality_statistics(self, days: int = 7) -> Dict[str, Any]:
        """í’ˆì§ˆ í†µê³„ ì¡°íšŒ"""
        return self.quality_metrics.get_quality_statistics(days)
    
    def get_quality_trends(self, days: int = 30) -> Dict[str, Any]:
        """í’ˆì§ˆ íŠ¸ë Œë“œ ì¡°íšŒ"""
        trends_df = self.quality_metrics.get_quality_trends(days)
        return trends_df.to_dict('records') if not trends_df.empty else []
    
    def get_quality_insights(self) -> Dict[str, Any]:
        """í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ"""
        return self.quality_monitor.get_quality_insights()
    
    def start_quality_monitoring(self):
        """í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.quality_monitor.start_monitoring()
        logger.info("Quality monitoring started")
    
    def stop_quality_monitoring(self):
        """í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.quality_monitor.stop_monitoring()
        logger.info("Quality monitoring stopped")
    
    def get_monitoring_status(self) -> Dict[str, Any]:
        """ëª¨ë‹ˆí„°ë§ ìƒíƒœ ì¡°íšŒ"""
        return self.quality_monitor.get_monitoring_status()
    
    # ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬ ë©”ì„œë“œë“¤
    def start_conversation_session(self, user_id: str = None, session_metadata: Dict[str, Any] = None) -> str:
        """ìƒˆ ëŒ€í™” ì„¸ì…˜ ì‹œì‘"""
        self.current_session_id = self.conversation_tracker.start_session(user_id, session_metadata)
        logger.info(f"Started conversation session: {self.current_session_id}")
        return self.current_session_id
    
    def end_conversation_session(self, end_metadata: Dict[str, Any] = None):
        """í˜„ì¬ ëŒ€í™” ì„¸ì…˜ ì¢…ë£Œ"""
        if self.current_session_id:
            self.conversation_tracker.end_session(self.current_session_id, end_metadata)
            logger.info(f"Ended conversation session: {self.current_session_id}")
            self.current_session_id = None
    
    def enable_conversation_logging(self, enable: bool = True):
        """ëŒ€í™” ë¡œê¹… í™œì„±í™”/ë¹„í™œì„±í™”"""
        self.enable_conversation_logging = enable
        logger.info(f"Conversation logging {'enabled' if enable else 'disabled'}")
    
    def get_conversation_analytics(self, days: int = 7) -> Dict[str, Any]:
        """ëŒ€í™” ë¶„ì„ í†µê³„ ì¡°íšŒ"""
        return self.conversation_tracker.get_conversation_analytics(days)
    
    def search_conversations(
        self,
        query: str = None,
        system_type: str = None,
        min_quality_score: float = None,
        date_from: str = None,
        date_to: str = None,
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        """ëŒ€í™” ë¡œê·¸ ê²€ìƒ‰"""
        return self.conversation_tracker.search_conversations(
            query, system_type, min_quality_score, date_from, date_to, limit
        )
    
    def get_conversation_details(self, log_id: str) -> Dict[str, Any]:
        """íŠ¹ì • ëŒ€í™”ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        # ëŒ€í™” ë¡œê·¸ ì¡°íšŒ
        conversations = self.conversation_tracker.search_conversations(limit=1000)
        conversation = next((c for c in conversations if c['log_id'] == log_id), None)
        
        if not conversation:
            return None
        
        # ê²€ìƒ‰ ë‹¨ê³„ë³„ ìƒì„¸ ì •ë³´ ì¡°íšŒ
        search_steps = self.conversation_tracker.get_search_step_details(log_id)
        conversation['search_steps'] = search_steps
        
        return conversation

# ì œë„¤ë ˆì´ì…˜ ëª¨ë“ˆ ë‹¨ë… í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_generator_standalone():
    """ì œë„¤ë ˆì´ì…˜ ëª¨ë“ˆ ë‹¨ë… í…ŒìŠ¤íŠ¸"""
    from data_processing import DocumentChunk, RetrievalResult
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    # ì œë„¤ë ˆì´í„° ì´ˆê¸°í™”
    generator = RFPGenerator(api_key)
    generator.initialize()
    
    # í…ŒìŠ¤íŠ¸ìš© ê°€ì§œ ê²€ìƒ‰ ê²°ê³¼ ìƒì„±
    test_chunk = DocumentChunk(
        chunk_id="test_chunk",
        doc_id="test_doc",
        content="í•œêµ­ì‚¬í•™ì§„í¥ì¬ë‹¨ì—ì„œ ëŒ€í•™ì¬ì •ì •ë³´ì‹œìŠ¤í…œ ê³ ë„í™” ì‚¬ì—…ì„ ë°œì£¼í–ˆìŠµë‹ˆë‹¤. ì‚¬ì—…ê¸ˆì•¡ì€ 2ì–µ 1ì²œë§Œì›ì´ë©°, ê³„ì•½ê¸°ê°„ì€ 6ê°œì›”ì…ë‹ˆë‹¤.",
        chunk_type="metadata",
        metadata={
            "ê³µê³ ë²ˆí˜¸": "20240815487",
            "ì‚¬ì—…ëª…": "ëŒ€í•™ì¬ì •ì •ë³´ì‹œìŠ¤í…œ ê³ ë„í™”",
            "ë°œì£¼ê¸°ê´€": "í•œêµ­ì‚¬í•™ì§„í¥ì¬ë‹¨",
            "ì‚¬ì—…ê¸ˆì•¡": 211000000
        }
    )
    
    test_results = [
        RetrievalResult(chunk=test_chunk, score=0.95, rank=1)
    ]
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_question = "í•œêµ­ì‚¬í•™ì§„í¥ì¬ë‹¨ ì‚¬ì—…ì˜ ì£¼ìš” ë‚´ìš©ì„ ì•Œë ¤ì£¼ì„¸ìš”."
    
    # ì‘ë‹µ ìƒì„±
    response = generator.generate_response(test_question, test_results)
    
    print(f"ì§ˆë¬¸: {response.question}")
    print(f"ë‹µë³€: {response.answer}")
    print(f"ë©”íƒ€ë°ì´í„°: {response.generation_metadata}")
    
    # í’ˆì§ˆ í‰ê°€ ê²°ê³¼ í™•ì¸
    if "quality_evaluation" in response.generation_metadata:
        quality_eval = response.generation_metadata["quality_evaluation"]
        print(f"\n=== í’ˆì§ˆ í‰ê°€ ê²°ê³¼ ===")
        print(f"ì¢…í•© ì ìˆ˜: {quality_eval['overall_score']:.3f}")
        print(f"ì„¸ë¶€ ì ìˆ˜: {quality_eval['scores']}")
        print(f"ê°œì„  ì œì•ˆ: {quality_eval['suggestions']}")
    
    # í’ˆì§ˆ í†µê³„ ì¡°íšŒ
    print(f"\n=== í’ˆì§ˆ í†µê³„ (ìµœê·¼ 7ì¼) ===")
    stats = generator.get_quality_statistics(days=7)
    print(f"í‰ê·  í’ˆì§ˆ ì ìˆ˜: {stats['avg_overall_score']:.3f}")
    print(f"ì´ í‰ê°€ ìˆ˜: {stats['total_evaluations']}")
    
    # í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ
    print(f"\n=== í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸ ===")
    insights = generator.get_quality_insights()
    print(f"ì¸ì‚¬ì´íŠ¸: {insights.get('insights', [])}")

if __name__ == "__main__":
    test_generator_standalone()
